---
title: "major_project_3"
output: html_document
---

```{r library}
library(tidyverse)
#library(here)      # directory referencing
#library(readxl)    # reading Excel files

#library(janitor)   # data cleaning 
#library(stringr)   # string manipulation
library(tidyr)     # new tidy functions
library(knitr) # kable
#library(modi) # ok for multivariate outlier detection
library(caret)# low variance filter
# missing values
#library(naniar)
#library(knitr)
#library(ggpubr) # ggplot arrangement
#ploting 
library(gridExtra)
library(kableExtra)
#outlier
#library(univOutl)
# tree methods
#library(tourr)
#library(RColorBrewer)
#library(plotly)
#library(htmltools)
library(performanceEstimation)# for SMOTE
library(rpart)
library(rpart.plot)
library(rattle) #fancyRpartPlot
library(Rtsne)
library(randomForest)
library(neuralnet)
library(e1071)# SVM regression
library(mltools)
library(data.table)
library(skimr)
library(smotefamily)
library(broom)
library(jtools)
```


```{r load data}
load("tech_data.Rdata") # load cleaned data from John's code, make sure you have the Rdata file within the working directory
```

```{r dataset fliter}
#dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64, SMKSTAT==5)  # filter age and smoke status
dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64)  # filter age and smoke status
var_list<-c("BMISC","SEX","AGEC","DIABBC","HCHOLBC","HSUGBC","HYPBC","PHDCMWBC","EXLWMBC","EXLWVBC",
 "SYSTOL","DIASTOL","TRIGRESB","CHOLRESB","LDLRESB","HBA1PREB","GLUCFREB","HDLCHREB","CVDMEDST","APOBRESB") # add/remove variables that are interested

dat2<-dat %>% select (var_list) # select columns that we are interested
str(dat2) # 3488 obs x 13 variables

skim(dat2$DIABBC)

dat2
dat2$EXLWMBC<-as.numeric(as.character(dat2$EXLWMBC))
dat2$EXLWVBC<-as.numeric(as.character(dat2$EXLWVBC))

dat2
```

```{r create factor scores}
dat20<-dat2 %>% mutate(
 Sys_score=ifelse(SYSTOL<120,0,ifelse(SYSTOL<130,1,ifelse(SYSTOL<140,2,ifelse(SYSTOL>=998,NA,3)))),
 Dis_score=ifelse(DIASTOL<80,0,ifelse(DIASTOL<90,1,ifelse(DIASTOL<100,2,ifelse(DIASTOL>=998,NA,3)))),  
Tri_score=as.factor(ifelse((TRIGRESB==1|TRIGRESB==2|TRIGRESB==3),0,ifelse((TRIGRESB==4|TRIGRESB==5),1,ifelse(TRIGRESB==97|TRIGRESB==98,NA,2)))),
Chol_score=as.factor(ifelse((CHOLRESB==1|CHOLRESB==2|CHOLRESB==3),0,ifelse((CHOLRESB==4|CHOLRESB==5|CHOLRESB==6),1,ifelse(CHOLRESB==97|CHOLRESB==98,NA,2)))),
LDL_score=as.factor(ifelse((LDLRESB==1|LDLRESB==2|LDLRESB==3|LDLRESB==4),0,ifelse((LDLRESB==5|LDLRESB==6|LDLRESB==7),1,ifelse(LDLRESB==97|LDLRESB==98,NA,2)))),
Glu_score=as.factor(ifelse((GLUCFREB==4|GLUCFREB==5),0,ifelse((GLUCFREB==6|GLUCFREB==7),1,ifelse(GLUCFREB==97|GLUCFREB==98,NA,2)))),
HDL_score=as.factor(ifelse((HDLCHREB==7|HDLCHREB==8),NA,ifelse((HDLCHREB==5|HDLCHREB==6),0,ifelse(HDLCHREB==1,2,1)))),
Waist_score=ifelse(SEX==1,ifelse(PHDCMWBC<102,0,ifelse(PHDCMWBC>=998,NA,1)),ifelse(PHDCMWBC<88,0,ifelse(PHDCMWBC>=998,NA,1))),
MBC_score=ifelse(EXLWMBC<150,1,0),
VBC_score=ifelse(EXLWVBC<75,1,0),
ApoB_score=as.factor(ifelse((APOBRESB==1|APOBRESB==2|APOBRESB==3|APOBRESB==4),0,ifelse((APOBRESB==5),1,ifelse(APOBRESB==97|APOBRESB==98,NA,2)))),
HbA1c_score=as.factor(ifelse((HBA1PREB==1|HBA1PREB==2),0,ifelse((HBA1PREB==3|HBA1PREB==4),1,ifelse(HBA1PREB==7|HBA1PREB==8,NA,2)))),

)

str(dat20)
```

```{r create obesity scores}
# dat3 contains obesity scores that are manually created for 4 diseases
# the higher score the more likely to be obesity

dat3<-dat20 %>% mutate(
  dia_score= ifelse((DIABBC==5), 0, ifelse(DIABBC==3|DIABBC==2|DIABBC==1,1, NA)),
  cho_score= ifelse((HCHOLBC==5), 0, ifelse(HCHOLBC==3|HCHOLBC==2|HCHOLBC==1,1, NA)),
  sug_score= ifelse((HSUGBC==5),0, ifelse(HSUGBC==3|HSUGBC==2|HSUGBC==1,1,NA)),
  hyp_score= ifelse((HYPBC==5), 0, ifelse(HYPBC==3|HYPBC==2|HYPBC==1,1, NA)),
  cvd_score= ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),1,ifelse(CVDMEDST==4,0,NA)),
  final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
)

hist(dat3$cvd_score) # the final score is highly unbalanced
nrow(dat3[which(dat3$CVDMEDST==5),]) # the final score of 2790 observations are 0

skim(dat3)

dat4<-dat20 %>% mutate(
  dia_score= ifelse((DIABBC==5), 0, ifelse(DIABBC==3|DIABBC==2|DIABBC==1,1, NA)),
  cho_score= ifelse((HCHOLBC==5), 0, ifelse(HCHOLBC==3|HCHOLBC==2|HCHOLBC==1,1, NA)),
  sug_score= ifelse((HSUGBC==5),0, ifelse(HSUGBC==3|HSUGBC==2|HSUGBC==1,1,NA)),
  hyp_score= ifelse((HYPBC==5), 0, ifelse(HYPBC==3|HYPBC==2|HYPBC==1,1, NA)),
  cvd_score= ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),1,ifelse(CVDMEDST==4,0,NA)),
  final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
)

hist(dat4$cho_score) # the final score is highly unbalanced
nrow(dat4[which(dat4$HCHOLBC==5),]) # the final score of 2790 observations are 0

skim(dat4)

dat5<-dat20 %>% mutate(
  dia_score= ifelse((DIABBC==5), 0, ifelse(DIABBC==3|DIABBC==2|DIABBC==1,1, NA)),
  cho_score= ifelse((HCHOLBC==5), 0, ifelse(HCHOLBC==3|HCHOLBC==2|HCHOLBC==1,1, NA)),
  sug_score= ifelse((HSUGBC==5),0, ifelse(HSUGBC==3|HSUGBC==2|HSUGBC==1,1,NA)),
  hyp_score= ifelse((HYPBC==5), 0, ifelse(HYPBC==3|HYPBC==2|HYPBC==1,1, NA)),
  cvd_score= ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),1,ifelse(CVDMEDST==4,0,NA)),
  final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
)

hist(dat5$sug_score) # the final score is highly unbalanced
nrow(dat5[which(dat5$HSUGBC==5),]) # the final score of 2790 observations are 0

skim(dat5)

dat6<-dat20 %>% mutate(
  dia_score= ifelse((DIABBC==5), 0, ifelse(DIABBC==3|DIABBC==2|DIABBC==1,1, NA)),
  cho_score= ifelse((HCHOLBC==5), 0, ifelse(HCHOLBC==3|HCHOLBC==2|HCHOLBC==1,1, NA)),
  sug_score= ifelse((HSUGBC==5),0, ifelse(HSUGBC==3|HSUGBC==2|HSUGBC==1,1,NA)),
  hyp_score= ifelse((HYPBC==5), 0, ifelse(HYPBC==3|HYPBC==2|HYPBC==1,1, NA)),
  cvd_score= ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),1,ifelse(CVDMEDST==4,0,NA)),
  final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
)

hist(dat6$hyp_score) # the final score is highly unbalanced
nrow(dat6[which(dat5$HYPBC==5),]) # the final score of 2790 observations are 0

skim(dat6)

#dat7
dat7<-dat20 %>% mutate(
  dia_score= ifelse((DIABBC==5), 0, ifelse(DIABBC==3|DIABBC==2|DIABBC==1,1, NA)),
  cho_score= ifelse((HCHOLBC==5), 0, ifelse(HCHOLBC==3|HCHOLBC==2|HCHOLBC==1,1, NA)),
  sug_score= ifelse((HSUGBC==5),0, ifelse(HSUGBC==3|HSUGBC==2|HSUGBC==1,1,NA)),
  hyp_score= ifelse((HYPBC==5), 0, ifelse(HYPBC==3|HYPBC==2|HYPBC==1,1, NA)),
  cvd_score= ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),1,ifelse(CVDMEDST==4,0,NA)),
  final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
)

hist(dat7$dia_score) # the final score is highly unbalanced
nrow(dat6[which(dat5$DIABBC==5),]) # the final score of 2790 observations are 0
```


```{r smote-k-fold cross-valdation for linear regression }
set.seed(2021)
# Predict cho_score, drop other y and y-related variables
dat4<-subset(dat4, select=-c(DIABBC,HCHOLBC, HSUGBC, HYPBC,CVDMEDST, cvd_score, sug_score, hyp_score, final_score)) 
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation 
k_fold=10
# create k folds
folds<-createFolds(y=dat4$cho_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c() 

#table(dat44.balanced$HYPBC)

# K-fold cross-validation:
for (i in 1:k_fold){
  fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat4[-folds[[i]],] # remaining is training set
  # linear regression using AIC
  M1<-lm(cho_score~., data=fold_train) # full model
  M0<-lm(cho_score~1, data=fold_train) # null model
  lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
  fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
  test_error_RMSE[i] = RMSE(fold_predict, fold_test$cho_score) # calculate and record RMSE
}

summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE

#Predict sug_score
set.seed(2021)
# Predict sug_score, drop other y and y-related variables
dat5<-subset(dat5, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC,CVDMEDST, cho_score,dia_score,cvd_score, hyp_score, final_score)) 
# str(dat5)
# dat55<-dat5[dat5$sug_score>0,]
# str(dat55)
# remove NA values
dat5<-na.omit(dat5)
# set up k value for k-fold cross validation 
k_fold=10
# create k folds
folds<-createFolds(y=dat5$sug_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c() 

#table(dat55.balanced$HYPBC)

# K-fold cross-validation:
for (i in 1:k_fold){
  fold_test<-dat5[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat5[-folds[[i]],] # remaining is training set
  # linear regression using AIC
  M1<-lm(sug_score~., data=fold_train) # full model
  M0<-lm(sug_score~1, data=fold_train) # null model
  lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
  fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
  test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}

summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)

```

```{r logistic model}

```

```{r linear model}

```

```{r k-fold cross-validation for linear regression }
set.seed(2021)
# Predict dia_score, drop other y and y-related variables
#dat4<-subset(dat4, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cvd_score, sug_score,CVDMEDST, hyp_score, final_score)) 
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation 
k_fold=10
# create k folds
folds<-createFolds(y=dat4$cho_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c() 

# K-fold cross-validation:
for (i in 1:k_fold){
  fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat4[-folds[[i]],] # remaining is training set
  # linear regression using AIC
  M1<-lm(cho_score~., data=fold_train) # full model
  M0<-lm(cho_score~1, data=fold_train) # null model
  lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
  fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
  test_error_RMSE[i] = RMSE(fold_predict, fold_test$cho_score) # calculate and record RMSE
}

summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE

# dat5 sug_score
set.seed(2021)
# Predict dia_score, drop other y and y-related variables
#dat5<-subset(dat5, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score,CVDMEDST, hyp_score, final_score,cvd_score)) 
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
#dat5<-na.omit(dat5)
# set up k value for k-fold cross validation 
k_fold=10
# create k folds
folds<-createFolds(y=dat5$sug_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c() 

# K-fold cross-validation:
for (i in 1:k_fold){
  fold_test<-dat5[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat5[-folds[[i]],] # remaining is training set
  # linear regression using AIC
  M1<-lm(sug_score~., data=fold_train) # full model
  M0<-lm(sug_score~1, data=fold_train) # null model
  lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
  fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
  test_error_RMSE[i] = RMSE(fold_predict, fold_test$sug_score) # calculate and record RMSE
}

summary(lm_model)
#model<-lm(dia_score~., data=dat5)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
```

```{r logistic regression}
null = glm(cvd_score~1,data=dat33.balanced,family=binomial("logit"),maxit = 100)
full = glm(cvd_score~.,data=dat33.balanced,family=binomial("logit"),maxit = 100)
n= nrow(dat33.balanced)
# stepwise from full model using BIC
stepBICfull <- step(full,k=log(n))
# stepwise from full model using AIC 
stepAICfull <- step(full,k=2)
# stepwise from null model using BIC 
stepBICnull <- step(null,scope=list(lower=null,upper=full),k=log(n))
# stepwise from null model using AIC 
stepAICnull <- step(null,scope=list(lower=null,upper=full),k=2)
```

```{r k-fold validation}
cv_penLogistic <- function(y,X,method=c("vanilla",
                                        "fowardAIC",
                                        "forwardBIC",
                                        "stepwiseAIC",
                                        "stepwiseBIC",
                                        "ridge",
                                        "lasso",
                                        "firth"),
                           folds=10,
                           repeats=20,
                           seed=1)
{
  set.seed(seed)
  n <- nrow(X)

  error_mat <- matrix(NA,n,repeats)
  dat <- data.frame(y=y,X=X)
  # repeats是最大的循环, 每个repeat都要执行k-fold CV
  for (r in 1:repeats) 
  {
    sets <- sample(rep(1:folds,n)[1:n],n)
	# 划分 训练集和 测试集
    for(i in 1:folds){
      testSet  <- which(sets==i)
      trainSet <- (1:n)[-testSet] 
      testData    <- dat[testSet, ]
      trainData   <- dat[trainSet, ]
      # 针对各种方法设置相应的训练模型:
      if (method=="vanilla") {
        model       <- glm(y~.,family=binomial,data=trainData)
      }
      
      if (method%in%c("fowardAIC","forwardBIC","stepwiseAIC","stepwiseBIC")) 
      {
        null = glm(y~1,data=trainData,family=binomial)
        full = glm(y~.,data=trainData,family=binomial)
      }
  
      if (method=="fowardAIC") {
        model <- step(null,scope=list(lower=null,upper=full),k=2,trace=0)
      }
      
      if (method=="forwardBIC") {
        model <- step(null,scope=list(lower=null,upper=full),k=log(n),trace=0)
      }     
      
      if (method=="stepwiseAIC") {
        model <- step(full,k=2,trace=0)
      }           
      
      if (method=="stepwiseBIC") {
        model <- step(full,k=log(n),trace=0)
      }
      
      if (method=="ridge") {
        
        model <- cv.glmnet(data.matrix(trainData[,-1]), trainData$y, 
                           alpha = 0, family = "binomial")
      }
      
      if (method=="lasso") {
        
        model <- cv.glmnet(data.matrix(trainData[,-1]), trainData$y, 
                           alpha = 1, family = "binomial")
      }
      
      if (method=="firth") {
        model <- brglm(y~., data=trainData)
      }      
      # 针对各个方法进行test
      if (!(method%in%c("ridge","lasso"))) {
        testProb <- predict(model, testData, type="response")
      }
    
      if (method%in%c("ridge","lasso") ) {
        res <- predict(model, newx=data.matrix(testData[,-1]), s="lambda.1se")
        testProb <- 1/(1 + exp(-res))
      }
      # 记录test的误差
      testPred <- round(testProb)
      errs <- as.numeric(testData$y!=testPred)
      error_mat[testSet,r] <- errs
    }
  }
  
  return(error_mat)
}
```

```{r}
repeats <-100
res1<-cv_penLogistic(CVD_score, Glu_score, method="ridge", repeats=repeats)
res2<-cv_penLogistic(y, X, method="lasso", repeats=repeats)
```


