---
title: "major_project_3"
output: html_document
---

```{r library}
library(tidyverse)
#library(here)      # directory referencing
#library(readxl)    # reading Excel files

#library(janitor)   # data cleaning 
#library(stringr)   # string manipulation
library(tidyr)     # new tidy functions
library(knitr) # kable
#library(modi) # ok for multivariate outlier detection
library(caret)# low variance filter
# missing values
#library(naniar)
#library(knitr)
#library(ggpubr) # ggplot arrangement
#ploting 
library(gridExtra)
library(kableExtra)
#outlier
#library(univOutl)
# tree methods
#library(tourr)
#library(RColorBrewer)
#library(plotly)
#library(htmltools)
library(performanceEstimation)# for SMOTE
library(rpart)
library(rpart.plot)
library(rattle) #fancyRpartPlot
library(Rtsne)
library(randomForest)
library(neuralnet)
library(e1071)# SVM regression
library(mltools)
library(data.table)
library(skimr)
library(smotefamily)
library(broom)
library(jtools)
```

```{r load data}
load("tech_data.Rdata") # load cleaned data from John's code, make sure you have the Rdata file within the working directory
```

```{r dataset fliter}
#dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64, SMKSTAT==5)  # filter age and smoke status
dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64)  # filter age and smoke status
var_list<-c("BMISC","SEX","AGEC","DIABBC","HCHOLBC","HSUGBC","HYPBC","PHDCMWBC","EXLWMBC","EXLWVBC",
 "SYSTOL","DIASTOL","TRIGRESB","CHOLRESB","LDLRESB","HBA1PREB","GLUCFREB","HDLCHREB","CVDMEDST","APOBRESB") # add/remove variables that are interested

dat2<-dat %>% select (var_list) # select columns that we are interested
str(dat2) # 3488 obs x 13 variables

skim(dat2$DIABBC)

dat2
dat2$EXLWMBC<-as.numeric(as.character(dat2$EXLWMBC))
dat2$EXLWVBC<-as.numeric(as.character(dat2$EXLWVBC))

dat2
```

```{r create factor scores 0,1,2,3}
dat20<-dat2 %>% mutate(
 Sys_score=ifelse(SYSTOL<120,0,ifelse(SYSTOL<130,1,ifelse(SYSTOL<140,2,ifelse(SYSTOL>=998,NA,3)))),
 Dis_score=ifelse(DIASTOL<80,0,ifelse(DIASTOL<90,1,ifelse(DIASTOL<100,2,ifelse(DIASTOL>=998,NA,3)))),  
Tri_score=as.factor(ifelse((TRIGRESB==1|TRIGRESB==2|TRIGRESB==3),0,ifelse((TRIGRESB==4|TRIGRESB==5),1,ifelse(TRIGRESB==97|TRIGRESB==98,NA,2)))),
Chol_score=as.factor(ifelse((CHOLRESB==1|CHOLRESB==2|CHOLRESB==3),0,ifelse((CHOLRESB==4|CHOLRESB==5|CHOLRESB==6),1,ifelse(CHOLRESB==97|CHOLRESB==98,NA,2)))),
LDL_score=as.factor(ifelse((LDLRESB==1|LDLRESB==2|LDLRESB==3|LDLRESB==4),0,ifelse((LDLRESB==5|LDLRESB==6|LDLRESB==7),1,ifelse(LDLRESB==97|LDLRESB==98,NA,2)))),
Glu_score=as.factor(ifelse((GLUCFREB==4|GLUCFREB==5),0,ifelse((GLUCFREB==6|GLUCFREB==7),1,ifelse(GLUCFREB==97|GLUCFREB==98,NA,2)))),
HDL_score=as.factor(ifelse((HDLCHREB==7|HDLCHREB==8),NA,ifelse((HDLCHREB==5|HDLCHREB==6),0,ifelse(HDLCHREB==1,2,1)))),
Waist_score=ifelse(SEX==1,ifelse(PHDCMWBC<102,0,ifelse(PHDCMWBC>=998,NA,1)),ifelse(PHDCMWBC<88,0,ifelse(PHDCMWBC>=998,NA,1))),
MBC_score=ifelse(EXLWMBC<150,1,0),
VBC_score=ifelse(EXLWVBC<75,1,0),
ApoB_score=as.factor(ifelse((APOBRESB==1|APOBRESB==2|APOBRESB==3|APOBRESB==4),0,ifelse((APOBRESB==5),1,ifelse(APOBRESB==97|APOBRESB==98,NA,2)))),
HbA1c_score=as.factor(ifelse((HBA1PREB==1|HBA1PREB==2),0,ifelse((HBA1PREB==3|HBA1PREB==4),1,ifelse(HBA1PREB==7|HBA1PREB==8,NA,2)))),

)

str(dat20)
```

```{r create obesity scores 0,1}
# dat3 contains obesity scores that are manually created for 4 diseases
# the higher score the more likely to be obesity

dat3<-dat20 %>% mutate(
  dia_score= ifelse((DIABBC==5), 0, ifelse(DIABBC==3|DIABBC==2|DIABBC==1,1, NA)),
  cho_score= ifelse((HCHOLBC==5), 0, ifelse(HCHOLBC==3|HCHOLBC==2|HCHOLBC==1,1, NA)),
  sug_score= ifelse((HSUGBC==5),0, ifelse(HSUGBC==3|HSUGBC==2|HSUGBC==1,1,NA)),
  hyp_score= ifelse((HYPBC==5), 0, ifelse(HYPBC==3|HYPBC==2|HYPBC==1,1, NA)),
  cvd_score= ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),1,ifelse(CVDMEDST==4,0,NA)),
  final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
)

hist(dat3$cvd_score) # the final score is highly unbalanced
nrow(dat3[which(dat3$CVDMEDST==5),]) # the final score of 2790 observations are 0

skim(dat3)

dat4<-dat20 %>% mutate(
  dia_score= ifelse((DIABBC==5), 0, ifelse(DIABBC==3|DIABBC==2|DIABBC==1,1, NA)),
  cho_score= ifelse((HCHOLBC==5), 0, ifelse(HCHOLBC==3|HCHOLBC==2|HCHOLBC==1,1, NA)),
  sug_score= ifelse((HSUGBC==5),0, ifelse(HSUGBC==3|HSUGBC==2|HSUGBC==1,1,NA)),
  hyp_score= ifelse((HYPBC==5), 0, ifelse(HYPBC==3|HYPBC==2|HYPBC==1,1, NA)),
  cvd_score= ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),1,ifelse(CVDMEDST==4,0,NA)),
  final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
)

hist(dat4$cho_score) # the final score is highly unbalanced
nrow(dat4[which(dat4$HCHOLBC==5),]) # the final score of 2790 observations are 0

skim(dat4)

dat5<-dat20 %>% mutate(
  dia_score= ifelse((DIABBC==5), 0, ifelse(DIABBC==3|DIABBC==2|DIABBC==1,1, NA)),
  cho_score= ifelse((HCHOLBC==5), 0, ifelse(HCHOLBC==3|HCHOLBC==2|HCHOLBC==1,1, NA)),
  sug_score= ifelse((HSUGBC==5),0, ifelse(HSUGBC==3|HSUGBC==2|HSUGBC==1,1,NA)),
  hyp_score= ifelse((HYPBC==5), 0, ifelse(HYPBC==3|HYPBC==2|HYPBC==1,1, NA)),
  cvd_score= ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),1,ifelse(CVDMEDST==4,0,NA)),
  final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
)

hist(dat5$sug_score) # the final score is highly unbalanced
nrow(dat5[which(dat5$HSUGBC==5),]) # the final score of 2790 observations are 0

skim(dat5)

dat6<-dat20 %>% mutate(
  dia_score= ifelse((DIABBC==5), 0, ifelse(DIABBC==3|DIABBC==2|DIABBC==1,1, NA)),
  cho_score= ifelse((HCHOLBC==5), 0, ifelse(HCHOLBC==3|HCHOLBC==2|HCHOLBC==1,1, NA)),
  sug_score= ifelse((HSUGBC==5),0, ifelse(HSUGBC==3|HSUGBC==2|HSUGBC==1,1,NA)),
  hyp_score= ifelse((HYPBC==5), 0, ifelse(HYPBC==3|HYPBC==2|HYPBC==1,1, NA)),
  cvd_score= ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),1,ifelse(CVDMEDST==4,0,NA)),
  final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
)

hist(dat6$hyp_score) # the final score is highly unbalanced
nrow(dat6[which(dat5$HYPBC==5),]) # the final score of 2790 observations are 0

skim(dat6)

#dat7
dat7<-dat20 %>% mutate(
  dia_score= ifelse((DIABBC==5), 0, ifelse(DIABBC==3|DIABBC==2|DIABBC==1,1, NA)),
  cho_score= ifelse((HCHOLBC==5), 0, ifelse(HCHOLBC==3|HCHOLBC==2|HCHOLBC==1,1, NA)),
  sug_score= ifelse((HSUGBC==5),0, ifelse(HSUGBC==3|HSUGBC==2|HSUGBC==1,1,NA)),
  hyp_score= ifelse((HYPBC==5), 0, ifelse(HYPBC==3|HYPBC==2|HYPBC==1,1, NA)),
  cvd_score= ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),1,ifelse(CVDMEDST==4,0,NA)),
  final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
)

hist(dat7$dia_score) # the final score is highly unbalanced
nrow(dat6[which(dat5$DIABBC==5),]) # the final score of 2790 observations are 0
```

```{r SMOTE function from DWmR package}
SMOTE <- function(form,data,
                  perc.over=200,k=5,
                  perc.under=200,
                  learner=NULL,...
                  )
  
  # INPUTS:
  # form a model formula
  # data the original training set (with the unbalanced distribution)
  # minCl  the minority class label
  # per.over/100 is the number of new cases (smoted cases) generated
  #              for each rare case. If perc.over < 100 a single case
  #              is generated uniquely for a randomly selected perc.over
  #              of the rare cases
  # k is the number of neighbours to consider as the pool from where
  #   the new examples are generated
  # perc.under/100 is the number of "normal" cases that are randomly
  #                selected for each smoted case
  # learner the learning system to use.
  # ...  any learning parameters to pass to learner
{

  # the column where the target variable is
  tgt <- which(names(data) == as.character(form[[2]]))
  minCl <- levels(data[,tgt])[which.min(table(data[,tgt]))]
  
  # get the cases of the minority class
  minExs <- which(data[,tgt] == minCl)

  # generate synthetic cases from these minExs
  if (tgt < ncol(data)) {
      cols <- 1:ncol(data)
      cols[c(tgt,ncol(data))] <- cols[c(ncol(data),tgt)]
      data <-  data[,cols]
  }
  newExs <- smote.exs(data[minExs,],ncol(data),perc.over,k)
  if (tgt < ncol(data)) {
      newExs <- newExs[,cols]
      data <- data[,cols]
  }
  
  # get the undersample of the "majority class" examples
  selMaj <- sample((1:NROW(data))[-minExs],
                   as.integer((perc.under/100)*nrow(newExs)),
                   replace=T)

  # the final data set (the undersample+the rare cases+the smoted exs)
  newdataset <- rbind(data[selMaj,],data[minExs,],newExs)

  # learn a model if required
  if (is.null(learner)) return(newdataset)
  else do.call(learner,list(form,newdataset,...))
}



# ===================================================
# Obtain a set of smoted examples for a set of rare cases.
# L. Torgo, Feb 2010
# ---------------------------------------------------
smote.exs <- function(data,tgt,N,k)
  # INPUTS:
  # data are the rare cases (the minority "class" cases)
  # tgt is the name of the target variable
  # N is the percentage of over-sampling to carry out;
  # and k is the number of nearest neighours to use for the generation
  # OUTPUTS:
  # The result of the function is a (N/100)*T set of generated
  # examples with rare values on the target
{
  nomatr <- c()
  T <- matrix(nrow=dim(data)[1],ncol=dim(data)[2]-1)
  for(col in seq.int(dim(T)[2]))
    if (class(data[,col]) %in% c('factor','character')) {
      T[,col] <- as.integer(data[,col])
      nomatr <- c(nomatr,col)
    } else T[,col] <- data[,col]
  
  if (N < 100) { # only a percentage of the T cases will be SMOTEd
    nT <- NROW(T)
    idx <- sample(1:nT,as.integer((N/100)*nT))
    T <- T[idx,]
    N <- 100
  }

  p <- dim(T)[2]
  nT <- dim(T)[1]

  ranges <- apply(T,2,max)-apply(T,2,min)
  
  nexs <-  as.integer(N/100) # this is the number of artificial exs generated
                                        # for each member of T
  new <- matrix(nrow=nexs*nT,ncol=p)    # the new cases

  for(i in 1:nT) {

    # the k NNs of case T[i,]
    xd <- scale(T,T[i,],ranges)
    for(a in nomatr) xd[,a] <- xd[,a]==0
    dd <- drop(xd^2 %*% rep(1, ncol(xd)))
    kNNs <- order(dd)[2:(k+1)]

    for(n in 1:nexs) {
      # select randomly one of the k NNs
      neig <- sample(1:k,1)

      ex <- vector(length=ncol(T))

      # the attribute values of the generated case
      difs <- T[kNNs[neig],]-T[i,]
      new[(i-1)*nexs+n,] <- T[i,]+runif(1)*difs
      for(a in nomatr)
        new[(i-1)*nexs+n,a] <- c(T[kNNs[neig],a],T[i,a])[1+round(runif(1),0)]

    }
  }
  newCases <- data.frame(new)
  for(a in nomatr)
    newCases[,a] <- factor(newCases[,a],levels=1:nlevels(data[,a]),labels=levels(data[,a]))

  newCases[,tgt] <- factor(rep(data[1,tgt],nrow(newCases)),levels=levels(data[,tgt]))
  colnames(newCases) <- colnames(data)
  newCases
}
```

```{r smote balancing dat 3 to dat33 balancing}
dat33<-dat3[,1:26] # do NOT include DIABBC and sores other than dia_score
dat33$CVDMEDST<-droplevels(dat33$CVDMEDST)
dat33<-na.omit(dat33)
table(dat33$CVDMEDST)


dat33.balanced <-smote(CVDMEDST~., dat33, perc.over=600, perc.under=2)

table(dat33.balanced$CVDMEDST)


dat33.balanced<-dat33.balanced %>% mutate(
  cvd_score= ifelse(CVDMEDST==4,0,1),
)

dat33.balanced$cvd_score<-as.factor(dat33.balanced$cvd_score)


# 
# 
# dat33.balanced<-dat33.balanced[,-4]
# dat33.balanced
# 
# model<-lm(dia_score~., data=dat33.balanced)


set.seed(2021)
training.samples <- dat33.balanced$cvd_score %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat33.balanced[training.samples, ]
test.data <- dat33.balanced[-training.samples, ]

dat33.balanced

#model<-nnet::multinom(dia_score~.-DIABBC, data=train.data)
model<-nnet::multinom(cvd_score~.-CVDMEDST, data=train.data)
predict_class<-predict(model,newdata=test.data)



length(predict_class)
length(test.data$DIABBC)

summary(model)
data.frame(predict_class)

table(predict_class, test.data$cvd_score)

mean(predict_class==test.data$cvd_score)





training.samples <- dat33$CVDMEDST %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat33[training.samples, ]
test.data <- dat33[-training.samples, ]

model<-nnet::multinom(CVDMEDST~., data=train.data)


predict_class<-predict(model, newdata=test.data)
table(predict_class, test.data$CVDMEDST)



mean(predict_class==test.data$CVDMEDST)

# 
# summary(cvd_model)

```

```{r smote balancing dat 4 to dat44 balancing}
dat44<-dat4[,1:26] # do NOT include DIABBC and sores other than dia_score
dat44$HCHOLBC<-droplevels(dat44$HCHOLBC)
dat44<-na.omit(dat44)
table(dat44$HCHOLBC)
dat44.balanced <-smote(HCHOLBC~., dat44, perc.over=600, perc.under=2)
table(dat44.balanced$HCHOLBC)
dat44.balanced<-dat44.balanced %>% mutate(
  cho_score= ifelse(HCHOLBC==5, 0, 1),
)

dat44.balanced$cho_score<-as.factor(dat44.balanced$cho_score)


# 
# 
# dat33.balanced<-dat33.balanced[,-4]
# dat33.balanced
# 
# model<-lm(dia_score~., data=dat33.balanced)


set.seed(2021)
training.samples <- dat44.balanced$cho_score %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat44.balanced[training.samples, ]
test.data <- dat44.balanced[-training.samples, ]

dat44.balanced

#model<-nnet::multinom(cho_score~.-HCHOLBC, data=train.data)
model<-nnet::multinom(cho_score~.-HCHOLBC, data=train.data)
predict_class<-predict(model,newdata=test.data)



length(predict_class)
length(test.data$HCHOLBC)

summary(model)
data.frame(predict_class)

table(predict_class, test.data$cho_score)

mean(predict_class==test.data$cho_score)





training.samples <- dat44$HCHOLBC %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat44[training.samples, ]
test.data <- dat44[-training.samples, ]

model<-nnet::multinom(HCHOLBC~., data=train.data)


predict_class<-predict(model, newdata=test.data)
table(predict_class, test.data$HCHOLBC)



mean(predict_class==test.data$HCHOLBC)

# 
# summary(model)


```

```{r smote balancing dat 5 to dat55 balancing}
dat55<-dat5[,1:26] # do NOT include DIABBC and sores other than dia_score
dat55$HSUGBC<-droplevels(dat55$HSUGBC)
dat55<-na.omit(dat55)
table(dat55$HSUGBC)
dat55.balanced <-smote(HSUGBC~., dat55, perc.over=600, perc.under=2)
table(dat55.balanced$HSUGBC)
dat55.balanced<-dat55.balanced %>% mutate(
  hyp_score= ifelse(HSUGBC==5, 0, 1),
)

dat55.balanced$sug_score<-as.factor(dat55.balanced$sug_score)

# 
# 
# dat33.balanced<-dat33.balanced[,-4]
# dat33.balanced
# 
# model<-lm(dia_score~., data=dat33.balanced)


set.seed(2021)
training.samples <- dat55.balanced$sug_score %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat55.balanced[training.samples, ]
test.data <- dat55.balanced[-training.samples, ]

dat55.balanced

#model<-nnet::multinom(cho_score~.-HCHOLBC, data=train.data)
model<-nnet::multinom(sug_score~.-HSUGBC, data=train.data)
predict_class<-predict(model,newdata=test.data)



length(predict_class)
length(test.data$HSUGBC)

summary(model)
data.frame(predict_class)

table(predict_class, test.data$sug_score)

mean(predict_class==test.data$sug_score)





training.samples <- dat55$HYPBC %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat55[training.samples, ]
test.data <- dat55[-training.samples, ]

model<-nnet::multinom(HSUGBC~., data=train.data)


predict_class<-predict(model, newdata=test.data)
table(predict_class, test.data$HSUGBC)



mean(predict_class==test.data$HSUGBC)

# 
# summary(model)

```

```{r smote balancing dat 6 to dat66 balancing}
dat66<-dat5[,1:26] # do NOT include DIABBC and sores other than dia_score
dat66$HYPBC<-droplevels(dat66$HSUGBC)
dat66<-na.omit(dat66)
table(dat66$HSUGBC)
dat66.balanced <-smote(HSUGBC~., dat66, perc.over=600, perc.under=2)
table(dat66.balanced$HSUGBC)
dat66.balanced<-dat66.balanced %>% mutate(
  hyp_score= ifelse(HSUGBC==5, 0, 1),
)

dat66.balanced$hyp_score<-as.factor(dat66.balanced$hyp_score)

# 
# 
# dat33.balanced<-dat33.balanced[,-4]
# dat33.balanced
# 
# model<-lm(dia_score~., data=dat33.balanced)


set.seed(2021)
training.samples <- dat66.balanced$hyp_score %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat66.balanced[training.samples, ]
test.data <- dat66.balanced[-training.samples, ]

dat66.balanced

#model<-nnet::multinom(cho_score~.-HCHOLBC, data=train.data)
model<-nnet::multinom(hyp_score~.-HYPBC, data=train.data)
predict_class<-predict(model,newdata=test.data)



length(predict_class)
length(test.data$HYPBC)

summary(model)
data.frame(predict_class)

table(predict_class, test.data$hyp_score)

mean(predict_class==test.data$hyp_score)





training.samples <- dat44$HYPBC %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat66[training.samples, ]
test.data <- dat66[-training.samples, ]

model<-nnet::multinom(HYPBC~., data=train.data)


predict_class<-predict(model, newdata=test.data)
table(predict_class, test.data$HYPBC)



mean(predict_class==test.data$HYPBC)

# 
# summary(model)

```

```{r smote balancing dat 7 to dat77 balancing}
dat77<-dat5[,1:26] # do NOT include DIABBC and sores other than dia_score
dat55$HSUGBC<-droplevels(dat77$DIABBC)
dat55<-na.omit(dat77)
table(dat77$DIABBC)
dat77.balanced <-smote(DIABBC~., dat77, perc.over=600, perc.under=2)
table(dat77.balanced$DIABBC)
dat77.balanced<-dat77.balanced %>% mutate(
  hyp_score= ifelse(DIABBC==5, 0, 1),
)

dat77.balanced$sug_score<-as.factor(dat55.balanced$dia_score)



set.seed(2021)
training.samples <- dat77.balanced$dia_score %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat77.balanced[training.samples, ]
test.data <- dat77.balanced[-training.samples, ]

dat77.balanced

#model<-nnet::multinom(cho_score~.-HCHOLBC, data=train.data)
model<-nnet::multinom(sug_score~.-DIABBC, data=train.data)
predict_class<-predict(model,newdata=test.data)



length(predict_class)
length(test.data$DIABBC)

summary(model)
data.frame(predict_class)

table(predict_class, test.data$dia_score)

mean(predict_class==test.data$dia_score)





training.samples <- dat55$HYPBC %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat55[training.samples, ]
test.data <- dat55[-training.samples, ]

model<-nnet::multinom(HSUGBC~., data=train.data)


predict_class<-predict(model, newdata=test.data)
table(predict_class, test.data$DIABBC)



mean(predict_class==test.data$DIABBC)

# 
# summary(model)
```

```{r PCA dat33.balanced}
na_count <-sapply(dat33.balanced, function(y) sum(is.na(y)))
na_percent <- data.frame(na_count)/nrow(dat33.balanced)
training_remove_sparse_records<-dat33.balanced[,na_percent<0.95]

str(training_remove_sparse_records[,1:1])

training_clean<-training_remove_sparse_records[,-c(1:1)]
dim(training_clean)

training_explore<-training_clean
training_explore$cvd_score<-as.numeric(training_explore$cvd_score)
cor_matrix<-abs(cor(training_explore))
diag(cor_matrix)<-0
library(corrplot)
corrplot(cor_matrix, method="square")

prComp<-prcomp(training_clean[,-54],scale. = TRUE)
std_dev <- prComp$sdev
pr_var <- std_dev^2
prop_varex <- pr_var/sum(pr_var)
sum(prop_varex[1:30])

plot(cumsum(prop_varex), xlab = "Principal Component",ylab = "Cumulative Proportion of Variance Explained",type = "b")
abline(h=0.975,col='red',v=12)

train.data<-data.frame(classe = training_clean$OBELV, prComp$x)
train.data <- train.data[,1:12]
metric <- "Accuracy"
control <- trainControl(method="repeatedcv", number=10, repeats=3)
mtry <- sqrt(ncol(train.data))
tunegrid <- expand.grid(.mtry=mtry)
library(doMC)

registerDoMC(cores = 4)
model_rf <- train(classe~.,data=train.data, method="rf",  tuneGrid=tunegrid, trControl=control)

print(model_rf)

prediction_rf<-predict(model_rf, newdata =train.data)
confusionMatrix(prediction_rf,train.data$classe)

```

```{rPCA dat44.balanced}

```


```{r logistic regression data33 balanced}
#data33<-subset(dat33, select=-c(DIABBC,HCHOLBC, HSUGBC, HYPBC,CVDMEDST, cvd_score, sug_score, hyp_score, final_score)) 
dat33<-na.omit(dat33)
library(smbinning)
# segregate continuous and factor variables
factor_vars <- c ("LDL", "EDUCATION", "MARITALSTATUS", "OCCUPATION", "RELATIONSHIP", "RACE", "SEX", "NATIVECOUNTRY")
continuous_vars <- c("AGE", "FNLWGT","EDUCATIONNUM", "HOURSPERWEEK", "CAPITALGAIN", "CAPITALLOSS")

iv_df <- data.frame(VARS=c(factor_vars, continuous_vars), IV=numeric(14))  # init for IV results

# compute IV for categoricals
for(factor_var in factor_vars){
  smb <- smbinning.factor(trainingData, y="ABOVE50K", x=factor_var)  # WOE table
  if(class(smb) != "character"){ # heck if some error occured
    iv_df[iv_df$VARS == factor_var, "IV"] <- smb$iv
  }
}

# compute IV for continuous vars
for(continuous_var in continuous_vars){
  smb <- smbinning(trainingData, y="ABOVE50K", x=continuous_var)  # WOE table
  if(class(smb) != "character"){  # any error while calculating scores.
    iv_df[iv_df$VARS == continuous_var, "IV"] <- smb$iv
  }
}

iv_df <- iv_df[order(-iv_df$IV), ]  # sort
iv_df
```

```{r k-fold cross-valdation for logistic regression dat4}
set.seed(2021)
# Predict cho_score, drop other y and y-related variables
dat4<-subset(dat4, select=-c(DIABBC,HCHOLBC, HSUGBC, HYPBC,CVDMEDST, cvd_score, sug_score, hyp_score, final_score)) 
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation 
k_fold=10
# create k folds
folds<-createFolds(y=dat4$cho_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c() 

#table(dat44.balanced$HYPBC)

# K-fold cross-validation:
for (i in 1:10){
  fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat4[-folds[[i]],] # remaining is training set
  # linear regression using AIC
  M1<-lm(cho_score~., data=fold_train) # full model
  M0<-lm(cho_score~1, data=fold_train) # null model
  lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
  fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
  test_error_RMSE[i] = RMSE(fold_predict, fold_test$cho_score) # calculate and record RMSE
}

summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE

#Predict sug_score
set.seed(2021)
# Predict sug_score, drop other y and y-related variables
dat5<-subset(dat5, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC,CVDMEDST, cho_score,dia_score,cvd_score, hyp_score, final_score)) 
# str(dat5)
# dat55<-dat5[dat5$sug_score>0,]
# str(dat55)
# remove NA values
dat5<-na.omit(dat5)
# set up k value for k-fold cross validation 
k_fold=10
# create k folds
folds<-createFolds(y=dat5$sug_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c() 

#table(dat55.balanced$HYPBC)

# K-fold cross-validation:
for (i in 1:k_fold){
  fold_test<-dat5[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat5[-folds[[i]],] # remaining is training set
  # linear regression using AIC
  M1<-lm(sug_score~., data=fold_train) # full model
  M0<-lm(sug_score~1, data=fold_train) # null model
  lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
  fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
  test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}

summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE

```

```{r k-fold cross-validation for logistic regression dat5}
# dat5 sug_score
set.seed(2021)
# Predict dia_score, drop other y and y-related variables
#dat5<-subset(dat5, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score,CVDMEDST, hyp_score, final_score,cvd_score)) 
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
#dat5<-na.omit(dat5)
# set up k value for k-fold cross validation 
k_fold=10
# create k folds
folds<-createFolds(y=dat5$sug_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c() 

# K-fold cross-validation:
for (i in 1:k_fold){
  fold_test<-dat5[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat5[-folds[[i]],] # remaining is training set
  # linear regression using AIC
  M1<-lm(sug_score~., data=fold_train) # full model
  M0<-lm(sug_score~1, data=fold_train) # null model
  lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
  fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
  test_error_RMSE[i] = RMSE(fold_predict, fold_test$sug_score) # calculate and record RMSE
}

summary(lm_model)
#model<-lm(dia_score~., data=dat5)
#summary(model)

```

```{r logistic regression dat33. balanced }
null=glm(cvd_score~1,data=dat33.balanced,family=binomial)
full=glm(cvd_score~.,data=dat33.balanced,family=binomial)
n=nrow(dat33.balanced)
#step
#stepwise from full model using BIC
stepBICfull<- step(full,k=log(n))
#stepwise from full model using AIC
stepAICfull<-step(full, k=2)
#stepwise from null model using BIC
stepBICnull<-step(nul,scope=list(lower=null, upper=full), k=log(n))
#stepwise from null model using AIC
stepAICnull<-step(null, scope=list(lower=null,upper=full),k=2)
```

```{r smote-k-fold cross-valdation for linear regression }
set.seed(2021)
# Predict cho_score, drop other y and y-related variables
dat4<-subset(dat4, select=-c(DIABBC,HCHOLBC, HSUGBC, HYPBC,CVDMEDST, cvd_score, sug_score, hyp_score, final_score)) 
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation 
k_fold=10
# create k folds
folds<-createFolds(y=dat4$cho_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c() 

#table(dat44.balanced$HYPBC)

# K-fold cross-validation:
for (i in 1:k_fold){
  fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat4[-folds[[i]],] # remaining is training set
  # linear regression using AIC
  M1<-lm(cho_score~., data=fold_train) # full model
  M0<-lm(cho_score~1, data=fold_train) # null model
  lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
  fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
  test_error_RMSE[i] = RMSE(fold_predict, fold_test$cho_score) # calculate and record RMSE
}

summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE

#Predict sug_score
set.seed(2021)
# Predict sug_score, drop other y and y-related variables
dat5<-subset(dat5, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC,CVDMEDST, cho_score,dia_score,cvd_score, hyp_score, final_score)) 
# str(dat5)
# dat55<-dat5[dat5$sug_score>0,]
# str(dat55)
# remove NA values
dat5<-na.omit(dat5)
# set up k value for k-fold cross validation 
k_fold=10
# create k folds
folds<-createFolds(y=dat5$sug_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c() 

#table(dat55.balanced$HYPBC)

# K-fold cross-validation:
for (i in 1:k_fold){
  fold_test<-dat5[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat5[-folds[[i]],] # remaining is training set
  # linear regression using AIC
  M1<-lm(sug_score~., data=fold_train) # full model
  M0<-lm(sug_score~1, data=fold_train) # null model
  lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
  fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
  test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}

summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)

```

```{r k-fold cross-validation for linear regression dat 4 }
set.seed(2021)
# Predict dia_score, drop other y and y-related variables
#dat4<-subset(dat4, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cvd_score, sug_score,CVDMEDST, hyp_score, final_score)) 
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation 
k_fold=10
# create k folds
folds<-createFolds(y=dat4$cho_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c() 

# K-fold cross-validation:
for (i in 1:k_fold){
  fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat4[-folds[[i]],] # remaining is training set
  # linear regression using AIC
  M1<-lm(cho_score~., data=fold_train) # full model
  M0<-lm(cho_score~1, data=fold_train) # null model
  lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
  fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
  test_error_RMSE[i] = RMSE(fold_predict, fold_test$cho_score) # calculate and record RMSE
}

summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE

# dat5 sug_score
set.seed(2021)
# Predict dia_score, drop other y and y-related variables
#dat5<-subset(dat5, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score,CVDMEDST, hyp_score, final_score,cvd_score)) 
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
#dat5<-na.omit(dat5)
# set up k value for k-fold cross validation 
k_fold=10
# create k folds
folds<-createFolds(y=dat5$sug_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c() 

# K-fold cross-validation:
for (i in 1:k_fold){
  fold_test<-dat5[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat5[-folds[[i]],] # remaining is training set
  # linear regression using AIC
  M1<-lm(sug_score~., data=fold_train) # full model
  M0<-lm(sug_score~1, data=fold_train) # null model
  lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
  fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
  test_error_RMSE[i] = RMSE(fold_predict, fold_test$sug_score) # calculate and record RMSE
}

summary(lm_model)
#model<-lm(dia_score~., data=dat5)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
```

```{r k-fold validation package}
cv_penLogistic <- function(y,X,method=c("vanilla",
                                        "fowardAIC",
                                        "forwardBIC",
                                        "stepwiseAIC",
                                        "stepwiseBIC",
                                        "ridge",
                                        "lasso",
                                        "firth"),
                           folds=10,
                           repeats=20,
                           seed=1)
{
  set.seed(seed)
  n <- nrow(X)

  error_mat <- matrix(NA,n,repeats)
  dat <- data.frame(y=y,X=X)
  # repeats是最大的循环, 每个repeat都要执行k-fold CV
  for (r in 1:repeats) 
  {
    sets <- sample(rep(1:folds,n)[1:n],n)
	# 划分 训练集和 测试集
    for(i in 1:folds){
      testSet  <- which(sets==i)
      trainSet <- (1:n)[-testSet] 
      testData    <- dat[testSet, ]
      trainData   <- dat[trainSet, ]
      # 针对各种方法设置相应的训练模型:
      if (method=="vanilla") {
        model       <- glm(y~.,family=binomial,data=trainData)
      }
      
      if (method%in%c("fowardAIC","forwardBIC","stepwiseAIC","stepwiseBIC")) 
      {
        null = glm(y~1,data=trainData,family=binomial)
        full = glm(y~.,data=trainData,family=binomial)
      }
  
      if (method=="fowardAIC") {
        model <- step(null,scope=list(lower=null,upper=full),k=2,trace=0)
      }
      
      if (method=="forwardBIC") {
        model <- step(null,scope=list(lower=null,upper=full),k=log(n),trace=0)
      }     
      
      if (method=="stepwiseAIC") {
        model <- step(full,k=2,trace=0)
      }           
      
      if (method=="stepwiseBIC") {
        model <- step(full,k=log(n),trace=0)
      }
      
      if (method=="ridge") {
        
        model <- cv.glmnet(data.matrix(trainData[,-1]), trainData$y, 
                           alpha = 0, family = "binomial")
      }
      
      if (method=="lasso") {
        
        model <- cv.glmnet(data.matrix(trainData[,-1]), trainData$y, 
                           alpha = 1, family = "binomial")
      }
      
      if (method=="firth") {
        model <- brglm(y~., data=trainData)
      }      
      # 针对各个方法进行test
      if (!(method%in%c("ridge","lasso"))) {
        testProb <- predict(model, testData, type="response")
      }
    
      if (method%in%c("ridge","lasso") ) {
        res <- predict(model, newx=data.matrix(testData[,-1]), s="lambda.1se")
        testProb <- 1/(1 + exp(-res))
      }
      # 记录test的误差
      testPred <- round(testProb)
      errs <- as.numeric(testData$y!=testPred)
      error_mat[testSet,r] <- errs
    }
  }
  
  return(error_mat)
}
```

```{r k-fold validation dat33 balanced}
repeats <-100
res1<-cv_penLogistic(CVD_score, Glu_score, method="ridge", repeats=repeats)
res2<-cv_penLogistic(CVD_score, X, method="lasso", repeats=repeats)

```

```{r random forest dat33 balanced}
library(caTools)
set.seed(123)
sample = sample.split(dat33.balanced$cvd_score, SplitRatio = .75)
train = subset(dat33.balanced, sample == TRUE)
test  = subset(dat33.balanced, sample == FALSE)
dim(train)
dim(test)
dat33brf <- randomForest(
  num ~ .,
  data=train
)
  pred = predict(rf, newdata=test[-14])
  cm = table(test[,14], pred)

```

