---
title: "project 5"
output: html_document
---
# Setup 

```{r library}
source('myfunc.R') # self-defiend functions
library(tidyverse)
#library(here)      # directory referencing
#library(readxl)    # reading Excel files

#library(janitor)   # data cleaning 
#library(stringr)   # string manipulation
library(tidyr)     # new tidy functions
library(knitr) # kable
#library(modi) # ok for multivariate outlier detection
library(caret)# low variance filter
library(glmnet)
library(brglm)
library(modelsummary)
# missing values
#library(naniar)
#library(knitr)
#library(ggpubr) # ggplot arrangement
#ploting 
library(gridExtra)
library(kableExtra)
#outlier
#library(univOutl)
# tree methods
#library(tourr)
#library(RColorBrewer)
#library(plotly)
#library(htmltools)
library(performanceEstimation)# for SMOTE
library(rpart)
library(rpart.plot)
library(rattle) #fancyRpartPlot
library(Rtsne)
library(randomForest)
library(neuralnet)
library(e1071)# SVM regression
library(mltools)
library(data.table)
library(skimr)
library(smotefamily)
library(broom)
library(jtools)
```

```{r load data}
load("tech_data.Rdata") # load cleaned data from John's code, make sure you have the Rdata file within the working directory
```

# Predictors selection by domain knowledge

We focus on adults only with several predictors (i.e., biomarkers and other factors) which are selected based on domain knowledge. These predictors are believed to be useful to predict a person's health status.

```{r data selection by domain knowledge}
#dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64, SMKSTAT==5)  # filter age and smoke status
dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64)  # filter age only
var_list<-c("BMISC","SEX","AGEC","DIABBC","HCHOLBC","HSUGBC","HYPBC","PHDCMWBC","EXLWMBC","EXLWVBC", "SYSTOL","DIASTOL","TRIGRESB","CHOLRESB","LDLRESB","HBA1PREB","GLUCFREB","HDLCHREB","CVDMEDST","APOBRESB") # add/remove variables that are interested
dat2$EXLWMBC<-as.numeric(as.character(dat2$EXLWMBC)) # exerices time should be numeric 
dat2$EXLWVBC<-as.numeric(as.character(dat2$EXLWVBC)) # exerciese time should be numeric
dat2<-dat %>% select (var_list) # select columns that we are interested
str(dat2) # 7238 obs x 20 variables
```
# Dataset clean

## Encode Y:

### Y1 

Y1 is binary and contains only 2 classes: if never had disease, score = 0, else score = 1

```{r}
Y1<-dat2 %>% mutate(
  dia_score= as.factor(ifelse((DIABBC==5), 0, ifelse(DIABBC==3|DIABBC==2|DIABBC==1,1, NA))),
  cho_score= as.factor(ifelse((HCHOLBC==5), 0, ifelse(HCHOLBC==3|HCHOLBC==2|HCHOLBC==1,1, NA))),
  sug_score= as.factor(ifelse((HSUGBC==5),0, ifelse(HSUGBC==3|HSUGBC==2|HSUGBC==1,1,NA))),
  hyp_score= as.factor(ifelse((HYPBC==5), 0, ifelse(HYPBC==3|HYPBC==2|HYPBC==1,1, NA))),
  cvd_score= as.factor(ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),1,ifelse(CVDMEDST==4,0,NA))),
  final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
) %>% select(dia_score, cho_score, sug_score, hyp_score, cvd_score, final_score)

```

## Encode X:

### X2
```{r X2}
# Do not use magic number, 0,1,2,3 should be changed to meaningful levels later
X2<-dat2 %>% mutate(
  Tri_score=as.factor(ifelse((TRIGRESB==1|TRIGRESB==2|TRIGRESB==3),0,ifelse((TRIGRESB==4|TRIGRESB==5),1,ifelse(TRIGRESB==97|TRIGRESB==98,NA,2)))),
  
  Chol_score=as.factor(ifelse((CHOLRESB==1|CHOLRESB==2|CHOLRESB==3),0,ifelse((CHOLRESB==4|CHOLRESB==5|CHOLRESB==6),1,ifelse(CHOLRESB==97|CHOLRESB==98,NA,2)))),
  
  LDL_score=as.factor(ifelse((LDLRESB==1|LDLRESB==2|LDLRESB==3|LDLRESB==4),0,ifelse((LDLRESB==5|LDLRESB==6|LDLRESB==7),1,ifelse(LDLRESB==97|LDLRESB==98,NA,2)))),
  
  Glu_score=as.factor(ifelse((GLUCFREB==4|GLUCFREB==5),0,ifelse((GLUCFREB==6|GLUCFREB==7),1,ifelse(GLUCFREB==97|GLUCFREB==98,NA,2)))),
  
  HDL_score=as.factor(ifelse((HDLCHREB==7|HDLCHREB==8),NA,ifelse((HDLCHREB==5|HDLCHREB==6),0,ifelse(HDLCHREB==1,2,1)))),
  
  
  ApoB_score=as.factor(ifelse((APOBRESB==1|APOBRESB==2|APOBRESB==3|APOBRESB==4),0,ifelse((APOBRESB==5),1,ifelse(APOBRESB==97|APOBRESB==98,NA,2)))),
  
  HbA1c_score=as.factor(ifelse((HBA1PREB==1|HBA1PREB==2),0,ifelse((HBA1PREB==3|HBA1PREB==4),1,ifelse(HBA1PREB==7|HBA1PREB==8,NA,2)))),

) %>% select (SYSTOL, DIASTOL,Tri_score, Chol_score, LDL_score, Glu_score, HDL_score, PHDCMWBC, EXLWMBC, EXLWVBC, ApoB_score, HbA1c_score)

str(X2)
```

### X1

Regroup ALL predictors into different levels such as "normal",  "high", based on domain knowledge. All predictors are categorical variables now.

```{r X1}
# Do not use magic number, 0,1,2,3 should be changed to meaningful levels later
X1<-dat2 %>% mutate(
  Sys_score=as.factor(ifelse(SYSTOL<120,0,ifelse(SYSTOL<130,1,ifelse(SYSTOL<140,2,ifelse(SYSTOL>=998,NA,3))))),
  
  Dis_score=as.factor(ifelse(DIASTOL<80,0,ifelse(DIASTOL<90,1,ifelse(DIASTOL<100,2,ifelse(DIASTOL>=998,NA,3))))),
  
  Tri_score=as.factor(ifelse((TRIGRESB==1|TRIGRESB==2|TRIGRESB==3),0,ifelse((TRIGRESB==4|TRIGRESB==5),1,ifelse(TRIGRESB==97|TRIGRESB==98,NA,2)))),
  
  Chol_score=as.factor(ifelse((CHOLRESB==1|CHOLRESB==2|CHOLRESB==3),0,ifelse((CHOLRESB==4|CHOLRESB==5|CHOLRESB==6),1,ifelse(CHOLRESB==97|CHOLRESB==98,NA,2)))),
  
  LDL_score=as.factor(ifelse((LDLRESB==1|LDLRESB==2|LDLRESB==3|LDLRESB==4),0,ifelse((LDLRESB==5|LDLRESB==6|LDLRESB==7),1,ifelse(LDLRESB==97|LDLRESB==98,NA,2)))),
  
  Glu_score=as.factor(ifelse((GLUCFREB==4|GLUCFREB==5),0,ifelse((GLUCFREB==6|GLUCFREB==7),1,ifelse(GLUCFREB==97|GLUCFREB==98,NA,2)))),
  
  HDL_score=as.factor(ifelse((HDLCHREB==7|HDLCHREB==8),NA,ifelse((HDLCHREB==5|HDLCHREB==6),0,ifelse(HDLCHREB==1,2,1)))),
  
  Waist_score=as.factor(ifelse(SEX==1,ifelse(PHDCMWBC<102,0,ifelse(PHDCMWBC>=998,NA,1)),ifelse(PHDCMWBC<88,0,ifelse(PHDCMWBC>=998,NA,1)))),
  
  MBC_score=ifelse(EXLWMBC<150,1,0),
  
  VBC_score=ifelse(EXLWVBC<75,1,0),
  
  ApoB_score=as.factor(ifelse((APOBRESB==1|APOBRESB==2|APOBRESB==3|APOBRESB==4),0,ifelse((APOBRESB==5),1,ifelse(APOBRESB==97|APOBRESB==98,NA,2)))),
  
  HbA1c_score=as.factor(ifelse((HBA1PREB==1|HBA1PREB==2),0,ifelse((HBA1PREB==3|HBA1PREB==4),1,ifelse(HBA1PREB==7|HBA1PREB==8,NA,2)))),

) %>% select (Sys_score, Dis_score,Tri_score, Chol_score, LDL_score, Glu_score, HDL_score, Waist_score, MBC_score, VBC_score, ApoB_score, HbA1c_score)

X1$MBC_score<-as.factor(X1$MBC_score)
X1$VBC_score<-as.factor(X1$VBC_score)
str(X1)
```

# Data analysis

## Model2 Y1$sug_score~X2

```{r}

temp_dat<-data.frame(cbind(Y1$sug_score, X2))
temp_dat<-na.omit(temp_dat)
y<-temp_dat$Y1.sug_score
#x<-one_hot(as.data.table(temp_dat[, -1])) # first colume is y and the rest will be X
x<-temp_dat[,-1]
```
### Logistic regression

```{r Y1$cvd_score~X1 logsitic regression}
options(warn =-1) # do not show warning
# use one-hot encoding
#xx<-one_hot(as.data.table(temp_dat[, -1]))
xx<-x
# model selection (step/CV) + CV test results
repeats <- 1
res1 <- cv_penLogistic(y,xx,method="vanilla", repeats=repeats)
res2 <- cv_penLogistic(y,xx,method="fowardAIC", repeats=repeats)
res3 <- cv_penLogistic(y,xx,method="forwardBIC", repeats=repeats)
res4 <- cv_penLogistic(y,xx,method="stepwiseAIC", repeats=repeats)
res5 <- cv_penLogistic(y,xx,method="stepwiseBIC", repeats=repeats)
res6 <- cv_penLogistic(y,xx,method="ridge", repeats=repeats)
res7 <- cv_penLogistic(y,xx,method="lasso", repeats=repeats)
res8 <-cv_penLogistic(y,xx,method="firth",repeats=repeats)

save(res1,res2,res3,res4,res5,res6,res7,res8, file="logistic model2 Y1_sug~X2.Rdata")
```

```{r exhbit models and cv performance}
# compare error of above models

load("logistic model2 Y1_sug~X2.Rdata")

tab <- cbind(
  apply(res1[[1]],2,mean),
  apply(res2[[1]],2,mean),
  apply(res3[[1]],2,mean),
  apply(res4[[1]],2,mean),
  apply(res5[[1]],2,mean),
  apply(res6[[1]],2,mean),
  apply(res7[[1]],2,mean),
  apply(res8[[1]],2,mean))

colnames(tab) <- c("logistic",
                   "fwdAIC",
                   "fwdBIC",
                   "bwdAIC",
                   "bwdBIC",
                   "ridge",
                   "lasso",
                   "firth")

boxplot(tab)
datasummary_skim(data=data.frame(100*tab), fmt = "%.2f")

# extract coefficients of models
lcoef <- list(res1[[2]]$coef,
              res2[[2]]$coef,
              res3[[2]]$coef,
              res4[[2]]$coef,
              res5[[2]]$coef,
              drop(coef(res6[[2]], res6[[2]]$lambda.1se )),
              drop(coef(res7[[2]], res7[[2]]$lambda.1se )),
              res8[[2]]$coef
)

varnames <- unique(unlist(map(lcoef,names)))
tab = matrix(0, nrow = length(lcoef), ncol = length(varnames))
colnames(tab) = varnames
for (i in 1:length(lcoef)) 
  tab[i, names(lcoef[[i]])] = lcoef[[i]]

rownames(tab) <- paste("model",1:length(lcoef),sep="")
logistic_coef_table<-kable(t(tab))
logistic_coef_table

# model 6 ridge shows best result
drop(coef(res6[[2]], res6[[2]]$lambda.1se))

```



## Model1 Y1$cvd_score~X1

```{r data }
# remove NA and extract y and x
options(warn =-1) # do not show warning
temp_dat<-data.frame(cbind(Y1$cvd_score, X1))
temp_dat<-na.omit(temp_dat)


y<-temp_dat$Y1.cvd_score
#x<-one_hot(as.data.table(temp_dat[, -1])) # first colume is y and the rest will be X
x<-temp_dat[,-1]
```



### Logistic regression

```{r Y1$cvd_score~X1 logsitic regression}
# use one-hot encoding
xx<-one_hot(as.data.table(temp_dat[, -1]))
# model selection (step/CV) + CV test results
repeats <- 10
res1 <- cv_penLogistic(y,xx,method="vanilla", repeats=repeats)
res2 <- cv_penLogistic(y,xx,method="fowardAIC", repeats=repeats)
res3 <- cv_penLogistic(y,xx,method="forwardBIC", repeats=repeats)
res4 <- cv_penLogistic(y,xx,method="stepwiseAIC", repeats=repeats)
res5 <- cv_penLogistic(y,xx,method="stepwiseBIC", repeats=repeats)
res6 <- cv_penLogistic(y,xx,method="ridge", repeats=repeats)
res7 <- cv_penLogistic(y,xx,method="lasso", repeats=repeats)
res8 <-cv_penLogistic(y,xx,method="firth",repeats=repeats)

save(res1,res2,res3,res4,res5,res6,res7,res8, file="logistic model Y1_cvd~X1.Rdata")
```

```{r exhbit models and cv performance}
# compare error of above models

table(Y1$cho_score)
table(Y1$hyp_score)
table(Y1$sug_score)
table(Y1$dia_score)

load("logistic model Y1_cvd~X1.Rdata")

tab <- cbind(
  apply(res1[[1]],2,mean),
  apply(res2[[1]],2,mean),
  apply(res3[[1]],2,mean),
  apply(res4[[1]],2,mean),
  apply(res5[[1]],2,mean),
  apply(res6[[1]],2,mean),
  apply(res7[[1]],2,mean),
  apply(res8[[1]],2,mean))

colnames(tab) <- c("logistic",
                   "fwdAIC",
                   "fwdBIC",
                   "bwdAIC",
                   "bwdBIC",
                   "ridge",
                   "lasso",
                   "firth")

boxplot(tab)
datasummary_skim(data=data.frame(100*tab), fmt = "%.2f")

# extract coefficients of models
lcoef <- list(res1[[2]]$coef,
              res2[[2]]$coef,
              res3[[2]]$coef,
              res4[[2]]$coef,
              res5[[2]]$coef,
              drop(coef(res6[[2]], res6[[2]]$lambda.1se )),
              drop(coef(res7[[2]], res7[[2]]$lambda.1se )),
              res8[[2]]$coef
)

varnames <- unique(unlist(map(lcoef,names)))
tab = matrix(0, nrow = length(lcoef), ncol = length(varnames))
colnames(tab) = varnames
for (i in 1:length(lcoef)) 
  tab[i, names(lcoef[[i]])] = lcoef[[i]]

rownames(tab) <- paste("model",1:length(lcoef),sep="")
logistic_coef_table<-kable(t(tab))
logistic_coef_table

# model 6 ridge shows best result
drop(coef(res6[[2]], res6[[2]]$lambda.1se))

```

Above plot shows the model "ridge" exhibits the lowest cross-validation test error.

### Random Forest

```{r}

```




