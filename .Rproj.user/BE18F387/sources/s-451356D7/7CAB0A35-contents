---
title: "project 5"
output: html_document
---

# Setup

```{r library}
source('myfunc.R') # self-defiend functions
library(tidyverse)
#library(here)      # directory referencing
#library(readxl)    # reading Excel files

#library(janitor)   # data cleaning 
#library(stringr)   # string manipulation
library(tidyr)     # new tidy functions
library(knitr) # kable
#library(modi) # ok for multivariate outlier detection
library(caret)# low variance filter
library(glmnet)
library(brglm)
library(modelsummary)
# missing values
#library(naniar)
#library(knitr)
#library(ggpubr) # ggplot arrangement
#ploting 
library(gridExtra)
library(kableExtra)
#outlier
#library(univOutl)
# tree methods
#library(tourr)
#library(RColorBrewer)
#library(plotly)
#library(htmltools)
library(performanceEstimation)# for SMOTE
library(rpart)
library(rpart.plot)
library(rattle) #fancyRpartPlot
library(Rtsne)
library(randomForest)
library(neuralnet)
library(e1071)# SVM regression
library(mltools)
library(data.table)
library(skimr)
library(smotefamily)
library(broom)
library(jtools)
```

```{r load data}
load("tech_data.Rdata") # load cleaned data from John's code, make sure you have the Rdata file within the working directory
```

# Dataset

We focus on adults only with several predictors (i.e., biomarkers and other factors) which are selected based on domain knowledge. These predictors are believed to be useful to predict a person's health status.

# Dat2

```{r data selection by domain knowledge}
#dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64, SMKSTAT==5)  # filter age and smoke status
dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64)  # filter age only
var_list<-c("BMISC","SEX","AGEC","DIABBC","HCHOLBC","HSUGBC","HYPBC","PHDCMWBC","EXLWMBC","EXLWVBC", "SYSTOL","DIASTOL","TRIGRESB","CHOLRESB","LDLRESB","HBA1PREB","GLUCFREB","HDLCHREB","CVDMEDST","APOBRESB") # add/remove variables that are interested

dat2<-dat %>% select (var_list) # select columns that we are interested
dat2$EXLWMBC<-as.numeric(as.character(dat$EXLWMBC)) # exerices time should be numeric 
dat2$EXLWVBC<-as.numeric(as.character(dat$EXLWVBC)) # exerciese time should be numeric
str(dat2) # 7238 obs x 20 variables
```

# Dataset clean

## Encode Y:

### Y1

Y1 is binary and contains only 2 classes: if never had disease, score = 0, else score = 1

```{r}
Y1<-dat2 %>% mutate(
  dia_score= as.factor(ifelse((DIABBC==5), 0, ifelse(DIABBC==3|DIABBC==2|DIABBC==1,1, NA))),
  cho_score= as.factor(ifelse((HCHOLBC==5), 0, ifelse(HCHOLBC==3|HCHOLBC==2|HCHOLBC==1,1, NA))),
  sug_score= as.factor(ifelse((HSUGBC==5),0, ifelse(HSUGBC==3|HSUGBC==2|HSUGBC==1,1,NA))),
  hyp_score= as.factor(ifelse((HYPBC==5), 0, ifelse(HYPBC==3|HYPBC==2|HYPBC==1,1, NA))),
  cvd_score= as.factor(ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),1,ifelse(CVDMEDST==4,0,NA))),
  # final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
) %>% select(dia_score, cho_score, sug_score, hyp_score, cvd_score)

```

## Encode X:

### X2

```{r X2}
# Do not use magic number, 0,1,2,3 should be changed to meaningful levels later
X2<-dat2 %>% mutate(
  Tri_score=as.factor(ifelse((TRIGRESB==1|TRIGRESB==2|TRIGRESB==3),0,ifelse((TRIGRESB==4|TRIGRESB==5),1,ifelse(TRIGRESB==97|TRIGRESB==98,NA,2)))),
  
  Chol_score=as.factor(ifelse((CHOLRESB==1|CHOLRESB==2|CHOLRESB==3),0,ifelse((CHOLRESB==4|CHOLRESB==5|CHOLRESB==6),1,ifelse(CHOLRESB==97|CHOLRESB==98,NA,2)))),
  
  LDL_score=as.factor(ifelse((LDLRESB==1|LDLRESB==2|LDLRESB==3|LDLRESB==4),0,ifelse((LDLRESB==5|LDLRESB==6|LDLRESB==7),1,ifelse(LDLRESB==97|LDLRESB==98,NA,2)))),
  
  Glu_score=as.factor(ifelse((GLUCFREB==4|GLUCFREB==5),0,ifelse((GLUCFREB==6|GLUCFREB==7),1,ifelse(GLUCFREB==97|GLUCFREB==98,NA,2)))),
  
  HDL_score=as.factor(ifelse((HDLCHREB==7|HDLCHREB==8),NA,ifelse((HDLCHREB==5|HDLCHREB==6),0,ifelse(HDLCHREB==1,2,1)))),
  
  
  ApoB_score=as.factor(ifelse((APOBRESB==1|APOBRESB==2|APOBRESB==3|APOBRESB==4),0,ifelse((APOBRESB==5),1,ifelse(APOBRESB==97|APOBRESB==98,NA,2)))),
  
  HbA1c_score=as.factor(ifelse((HBA1PREB==1|HBA1PREB==2),0,ifelse((HBA1PREB==3|HBA1PREB==4),1,ifelse(HBA1PREB==7|HBA1PREB==8,NA,2)))),

) %>% select (SYSTOL, DIASTOL,Tri_score, Chol_score, LDL_score, Glu_score, HDL_score, PHDCMWBC, EXLWMBC, EXLWVBC, ApoB_score, HbA1c_score)



str(X2)
```

### X1

Regroup ALL predictors into different levels such as "normal", "high", based on domain knowledge. All predictors are categorical variables now.

    {r X1}
    # Do not use magic number, 0,1,2,3 should be changed to meaningful levels later
    X1<-dat2 %>% mutate(
      Sys_score=as.factor(ifelse(SYSTOL<120,0,ifelse(SYSTOL<130,1,ifelse(SYSTOL<140,2,ifelse(SYSTOL>=998,NA,3))))),
      
      Dis_score=as.factor(ifelse(DIASTOL<80,0,ifelse(DIASTOL<90,1,ifelse(DIASTOL<100,2,ifelse(DIASTOL>=998,NA,3))))),
      
      Tri_score=as.factor(ifelse((TRIGRESB==1|TRIGRESB==2|TRIGRESB==3),0,ifelse((TRIGRESB==4|TRIGRESB==5),1,ifelse(TRIGRESB==97|TRIGRESB==98,NA,2)))),
      
      Chol_score=as.factor(ifelse((CHOLRESB==1|CHOLRESB==2|CHOLRESB==3),0,ifelse((CHOLRESB==4|CHOLRESB==5|CHOLRESB==6),1,ifelse(CHOLRESB==97|CHOLRESB==98,NA,2)))),
      
      LDL_score=as.factor(ifelse((LDLRESB==1|LDLRESB==2|LDLRESB==3|LDLRESB==4),0,ifelse((LDLRESB==5|LDLRESB==6|LDLRESB==7),1,ifelse(LDLRESB==97|LDLRESB==98,NA,2)))),
      
      Glu_score=as.factor(ifelse((GLUCFREB==4|GLUCFREB==5),0,ifelse((GLUCFREB==6|GLUCFREB==7),1,ifelse(GLUCFREB==97|GLUCFREB==98,NA,2)))),
      
      HDL_score=as.factor(ifelse((HDLCHREB==7|HDLCHREB==8),NA,ifelse((HDLCHREB==5|HDLCHREB==6),0,ifelse(HDLCHREB==1,2,1)))),
      
      Waist_score=as.factor(ifelse(SEX==1,ifelse(PHDCMWBC<102,0,ifelse(PHDCMWBC>=998,NA,1)),ifelse(PHDCMWBC<88,0,ifelse(PHDCMWBC>=998,NA,1)))),
      
      MBC_score=ifelse(EXLWMBC<150,1,0),
      
      VBC_score=ifelse(EXLWVBC<75,1,0),
      
      ApoB_score=as.factor(ifelse((APOBRESB==1|APOBRESB==2|APOBRESB==3|APOBRESB==4),0,ifelse((APOBRESB==5),1,ifelse(APOBRESB==97|APOBRESB==98,NA,2)))),
      
      HbA1c_score=as.factor(ifelse((HBA1PREB==1|HBA1PREB==2),0,ifelse((HBA1PREB==3|HBA1PREB==4),1,ifelse(HBA1PREB==7|HBA1PREB==8,NA,2)))),

    ) %>% select (Sys_score, Dis_score,Tri_score, Chol_score, LDL_score, Glu_score, HDL_score, Waist_score, MBC_score, VBC_score, ApoB_score, HbA1c_score)

    X1$MBC_score<-as.factor(X1$MBC_score)
    X1$VBC_score<-as.factor(X1$VBC_score)
    str(X1)

# Data analysis

## Model3 Y1\$cho_score \~ X2

```{r}
mystd<-function(x){
  x<-(x-mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)
}

x<-X2[,-4] %>% mutate_if(is.numeric, list(mystd))
y<-Y1$cho_score

temp_dat<-data.frame(cbind(y,x)) %>% na.omit() # need to drop Chol_score in X2 since Y is calculated based on chol_score
temp_dat
y<-temp_dat$y
temp_dat_bal<-smote(y~., data = temp_dat, k=10)

table(temp_dat_bal$y) # highly unbalanced data: 1882 0 and 280 1

```

### Logistic regression on unbalanced data

```{r}
options(warn =-1) # do not show warning
# use one-hot encoding
xx<-one_hot(as.data.table(temp_dat[, -1]))
#xx<-x
# model selection (step/CV) + CV test results
repeats <- 1
res1 <- cv_penLogistic(y,xx,method="vanilla", repeats=repeats)
res2 <- cv_penLogistic(y,xx,method="fowardAIC", repeats=repeats)
res3 <- cv_penLogistic(y,xx,method="forwardBIC", repeats=repeats)
res4 <- cv_penLogistic(y,xx,method="stepwiseAIC", repeats=repeats)
res5 <- cv_penLogistic(y,xx,method="stepwiseBIC", repeats=repeats)
res6 <- cv_penLogistic(y,xx,method="ridge", repeats=repeats)
res7 <- cv_penLogistic(y,xx,method="lasso", repeats=repeats)
res8 <- cv_penLogistic(y,xx,method="firth",repeats=repeats)

```

```{r}
tab <- cbind(
  apply(res1[[1]],2,mean),
  apply(res2[[1]],2,mean),
  apply(res3[[1]],2,mean),
  apply(res4[[1]],2,mean),
  apply(res5[[1]],2,mean),
  apply(res6[[1]],2,mean),
  apply(res7[[1]],2,mean),
  apply(res8[[1]],2,mean))

colnames(tab) <- c("logistic",
                   "fwdAIC",
                   "fwdBIC",
                   "bwdAIC",
                   "bwdBIC",
                   "ridge",
                   "lasso",
                   "firth")

boxplot(tab)
datasummary_skim(data=data.frame(100*tab), fmt = "%.2f")

# extract coefficients of models
lcoef <- list(res1[[2]]$coef,
              res2[[2]]$coef,
              res3[[2]]$coef,
              res4[[2]]$coef,
              res5[[2]]$coef,
              drop(coef(res6[[2]], res6[[2]]$lambda.1se )),
              drop(coef(res7[[2]], res7[[2]]$lambda.1se )),
              res8[[2]]$coef
)

varnames <- unique(unlist(map(lcoef,names)))
tab = matrix(0, nrow = length(lcoef), ncol = length(varnames))
colnames(tab) = varnames
for (i in 1:length(lcoef)) 
  tab[i, names(lcoef[[i]])] = lcoef[[i]]

rownames(tab) <- paste("model",1:length(lcoef),sep="")
logistic_coef_table<-kable(t(tab))
logistic_coef_table

# model 6 ridge shows best result
drop(coef(res6[[2]], res6[[2]]$lambda.1se))
```

### Logistic regression on balanced data

```{r}
options(warn =-1) # do not show warning
# use one-hot encoding
xx<-one_hot(as.data.table(temp_dat_bal[, -1]))
#xx<-x
y<-temp_dat_bal$y
# model selection (step/CV) + CV test results
repeats <- 1
res1 <- cv_penLogistic(y,xx,method="vanilla", repeats=repeats)
res2 <- cv_penLogistic(y,xx,method="fowardAIC", repeats=repeats)
res3 <- cv_penLogistic(y,xx,method="forwardBIC", repeats=repeats)
res4 <- cv_penLogistic(y,xx,method="stepwiseAIC", repeats=repeats)
res5 <- cv_penLogistic(y,xx,method="stepwiseBIC", repeats=repeats)
res6 <- cv_penLogistic(y,xx,method="ridge", repeats=repeats)
res7 <- cv_penLogistic(y,xx,method="lasso", repeats=repeats)
res8 <- cv_penLogistic(y,xx,method="firth",repeats=repeats)

```

```{r}
tab <- cbind(
  apply(res1[[1]],2,mean),
  apply(res2[[1]],2,mean),
  apply(res3[[1]],2,mean),
  apply(res4[[1]],2,mean),
  apply(res5[[1]],2,mean),
  apply(res6[[1]],2,mean),
  apply(res7[[1]],2,mean),
  apply(res8[[1]],2,mean))

colnames(tab) <- c("logistic",
                   "fwdAIC",
                   "fwdBIC",
                   "bwdAIC",
                   "bwdBIC",
                   "ridge",
                   "lasso",
                   "firth")

boxplot(tab)
datasummary_skim(data=data.frame(100*tab), fmt = "%.2f")

# extract coefficients of models
lcoef <- list(res1[[2]]$coef,
              res2[[2]]$coef,
              res3[[2]]$coef,
              res4[[2]]$coef,
              res5[[2]]$coef,
              drop(coef(res6[[2]], res6[[2]]$lambda.1se )),
              drop(coef(res7[[2]], res7[[2]]$lambda.1se )),
              res8[[2]]$coef
)

varnames <- unique(unlist(map(lcoef,names)))
tab = matrix(0, nrow = length(lcoef), ncol = length(varnames))
colnames(tab) = varnames
for (i in 1:length(lcoef)) 
  tab[i, names(lcoef[[i]])] = lcoef[[i]]

rownames(tab) <- paste("model",1:length(lcoef),sep="")
logistic_coef_table<-kable(t(tab))
logistic_coef_table

# model 6 ridge shows best result
drop(coef(res6[[2]], res6[[2]]$lambda.1se))
```

### Neural network

```{r model 3 NN}
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)
folds

x_onehot<-one_hot(as.data.table(x)) 

temp_dat_onehot<-cbind(y,x_onehot)%>%na.omit()



err_list<-list()

temp_dat_onehot<-data.frame(temp_dat_onehot)

for (i in 1:k_fold){
  fold_test<-temp_dat_onehot[folds[[i]],] # select folds[[i]] as test test
  fold_train<-temp_dat_onehot[-folds[[i]],] # remaining is training set
  model<-neuralnet(y~., data=fold_train, hidden=3)
  fold_predict <- predict(model, fold_test)
  err<-sum(as.numeric((fold_predict!=fold_test$y)))/nrow(fold_test) # error rate for i th cv
  err_list[i]=err
}

err_list[5]

res<-neuralnet(y~., data=fold_train,hidden=3)
print(res)

pred<-predict(res, fold_test[,-1])

pred2<-compute(model, fold_test)

classifcation<-function (x){
  if (x<0.5){
    x=1
  } else{
    x=0
  }
  return (x)
}
res<-apply(pred,1 ,classifcation)
res

model$result.matrix
confusionMatrix(table(res,fold_test$y))

table(res,fold_test$y)
temp_dat_onehot
```

### SVM

```{r model 3 SVM}
library(e1071)
C_val <- 1
coef0 <- 1
gamma <- 1/ncol(X2[,-4])
degree <- 3

# radial SVM

k_fold=10
# create k folds
folds<-createFolds(y=temp_dat_bal[,1], k=k_fold)
# create a new vector to record test results

# K-fold cross-validation for smote balanced data

err_list<-list()
for (i in 1:k_fold){
  fold_test<-temp_dat_bal[folds[[i]],] # select folds[[i]] as test test
  fold_train<-temp_dat_bal[-folds[[i]],] # remaining is training set
  res_svm2 <- svm(y~.,data=temp_dat_bal, cost=C_val,kernel="radial",gamma=gamma)
  fold_predict <- predict(res_svm2, fold_test)
  err<-sum(as.numeric((fold_predict!=fold_test$y)))/nrow(fold_test) # error rate for i th cv
  err_list[i]=err
}

w<-t(res_svm2$coefs)%*%res_svm2$SV
b<--1*res_svm2$rho
data.frame(cbind(t(w),b))

str(temp_dat)

mean(as.numeric(err_list))
confusionMatrix(table(fold_predict, fold_test$y))
```

```{r}

# unbalanced data without smote: not working, predict all 0
folds<-createFolds(y=temp_dat[,1], k=k_fold)
# K-fold cross-validation for smote balanced data
err_list<-list()
for (i in 1:k_fold){
  fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
  fold_train<-temp_dat[-folds[[i]],] # remaining is training set
  res_svm2 <- svm(y~.,data=temp_dat, cost=C_val,kernel="linear",gamma=gamma)
  fold_predict <- predict(res_svm2, fold_test)
  err<-sum(as.numeric((fold_predict!=fold_test$y)))/nrow(fold_test) # error rate for i th cv
  err_list[i]=err
}
mean(as.numeric(err_list))



confusionMatrix(table(fold_predict, fold_test$y))

```

## Model2 Y1\$sug_score\~X2

```{r}

temp_dat<-data.frame(cbind(Y1$sug_score, X2))
temp_dat<-na.omit(temp_dat)
y<-temp_dat$Y1.sug_score
#x<-one_hot(as.data.table(temp_dat[, -1])) # first colume is y and the rest will be X
x<-temp_dat[,-1]
```

### Logistic regression

```{r Y1$cvd_score~X1 logsitic regression}
options(warn =-1) # do not show warning
# use one-hot encoding
#xx<-one_hot(as.data.table(temp_dat[, -1]))
xx<-x
# model selection (step/CV) + CV test results
repeats <- 1
res1 <- cv_penLogistic(y,xx,method="vanilla", repeats=repeats)
res2 <- cv_penLogistic(y,xx,method="fowardAIC", repeats=repeats)
res3 <- cv_penLogistic(y,xx,method="forwardBIC", repeats=repeats)
res4 <- cv_penLogistic(y,xx,method="stepwiseAIC", repeats=repeats)
res5 <- cv_penLogistic(y,xx,method="stepwiseBIC", repeats=repeats)
res6 <- cv_penLogistic(y,xx,method="ridge", repeats=repeats)
res7 <- cv_penLogistic(y,xx,method="lasso", repeats=repeats)
res8 <-cv_penLogistic(y,xx,method="firth",repeats=repeats)

save(res1,res2,res3,res4,res5,res6,res7,res8, file="logistic model2 Y1_sug~X2.Rdata")
```

```{r exhbit models and cv performance}
# compare error of above models

load("logistic model2 Y1_sug~X2.Rdata")

tab <- cbind(
  apply(res1[[1]],2,mean),
  apply(res2[[1]],2,mean),
  apply(res3[[1]],2,mean),
  apply(res4[[1]],2,mean),
  apply(res5[[1]],2,mean),
  apply(res6[[1]],2,mean),
  apply(res7[[1]],2,mean),
  apply(res8[[1]],2,mean))

colnames(tab) <- c("logistic",
                   "fwdAIC",
                   "fwdBIC",
                   "bwdAIC",
                   "bwdBIC",
                   "ridge",
                   "lasso",
                   "firth")

boxplot(tab)
datasummary_skim(data=data.frame(100*tab), fmt = "%.2f")

# extract coefficients of models
lcoef <- list(res1[[2]]$coef,
              res2[[2]]$coef,
              res3[[2]]$coef,
              res4[[2]]$coef,
              res5[[2]]$coef,
              drop(coef(res6[[2]], res6[[2]]$lambda.1se )),
              drop(coef(res7[[2]], res7[[2]]$lambda.1se )),
              res8[[2]]$coef
)

varnames <- unique(unlist(map(lcoef,names)))
tab = matrix(0, nrow = length(lcoef), ncol = length(varnames))
colnames(tab) = varnames
for (i in 1:length(lcoef)) 
  tab[i, names(lcoef[[i]])] = lcoef[[i]]

rownames(tab) <- paste("model",1:length(lcoef),sep="")
logistic_coef_table<-kable(t(tab))
logistic_coef_table

# model 6 ridge shows best result
drop(coef(res6[[2]], res6[[2]]$lambda.1se))

```

## Model1 Y1\$cvd_score\~X1

```{r data }
# remove NA and extract y and x
options(warn =-1) # do not show warning
temp_dat<-data.frame(cbind(Y1$cvd_score, X1))
temp_dat<-na.omit(temp_dat)


y<-temp_dat$Y1.cvd_score
#x<-one_hot(as.data.table(temp_dat[, -1])) # first colume is y and the rest will be X
x<-temp_dat[,-1]
```

### Logistic regression

```{r Y1$cvd_score~X1 logsitic regression}
# use one-hot encoding
xx<-one_hot(as.data.table(temp_dat[, -1]))
# model selection (step/CV) + CV test results
repeats <- 10
res1 <- cv_penLogistic(y,xx,method="vanilla", repeats=repeats)
res2 <- cv_penLogistic(y,xx,method="fowardAIC", repeats=repeats)
res3 <- cv_penLogistic(y,xx,method="forwardBIC", repeats=repeats)
res4 <- cv_penLogistic(y,xx,method="stepwiseAIC", repeats=repeats)
res5 <- cv_penLogistic(y,xx,method="stepwiseBIC", repeats=repeats)
res6 <- cv_penLogistic(y,xx,method="ridge", repeats=repeats)
res7 <- cv_penLogistic(y,xx,method="lasso", repeats=repeats)
res8 <-cv_penLogistic(y,xx,method="firth",repeats=repeats)

save(res1,res2,res3,res4,res5,res6,res7,res8, file="logistic model Y1_cvd~X1.Rdata")
```

```{r exhbit models and cv performance}
# compare error of above models

table(Y1$cho_score)
table(Y1$hyp_score)
table(Y1$sug_score)
table(Y1$dia_score)

load("logistic model Y1_cvd~X1.Rdata")

tab <- cbind(
  apply(res1[[1]],2,mean),
  apply(res2[[1]],2,mean),
  apply(res3[[1]],2,mean),
  apply(res4[[1]],2,mean),
  apply(res5[[1]],2,mean),
  apply(res6[[1]],2,mean),
  apply(res7[[1]],2,mean),
  apply(res8[[1]],2,mean))

colnames(tab) <- c("logistic",
                   "fwdAIC",
                   "fwdBIC",
                   "bwdAIC",
                   "bwdBIC",
                   "ridge",
                   "lasso",
                   "firth")

boxplot(tab)
datasummary_skim(data=data.frame(100*tab), fmt = "%.2f")

# extract coefficients of models
lcoef <- list(res1[[2]]$coef,
              res2[[2]]$coef,
              res3[[2]]$coef,
              res4[[2]]$coef,
              res5[[2]]$coef,
              drop(coef(res6[[2]], res6[[2]]$lambda.1se )),
              drop(coef(res7[[2]], res7[[2]]$lambda.1se )),
              res8[[2]]$coef
)

varnames <- unique(unlist(map(lcoef,names)))
tab = matrix(0, nrow = length(lcoef), ncol = length(varnames))
colnames(tab) = varnames
for (i in 1:length(lcoef)) 
  tab[i, names(lcoef[[i]])] = lcoef[[i]]

rownames(tab) <- paste("model",1:length(lcoef),sep="")
logistic_coef_table<-kable(t(tab))
logistic_coef_table

# model 6 ridge shows best result
drop(coef(res6[[2]], res6[[2]]$lambda.1se))

```

Above plot shows the model "ridge" exhibits the lowest cross-validation test error.

### Random Forest

```{r}

```
