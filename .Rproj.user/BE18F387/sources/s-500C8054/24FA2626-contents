---
title: "Case Studies - Dimension Reduction"
author: "John Ormerod"
date: "28/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      tidy.opts=list(width.cutoff=120),
                      tidy=TRUE)

options(warn=-1) 
options(digits=2)
options(width = 120)

library <- function(...) {
  suppressPackageStartupMessages(base::library(...))
}

library(tidyverse)
library(here)
library(ggplot2)
library(rmarkdown)
library(knitr)
library(pdftools)
library(GGally)
library(kableExtra)
library(knitr)
library(ggrepel)
library(caret)
library(Rtsne)

set.seed(1)
```


### Principal Component Analysis

We will again look at the USArrests dataset. Here we are going to perform a PCA and see whether
principal components preserve the different regions of the United States in principal component
space. First we will load the data, note that I scraped the regions for each state from the internet
manually.

```{r}
dat <- read.csv(file=here("data","USArrests.csv"),header=TRUE)

# North eastern US states
NE <- c("Connecticut","Maine","Massachusetts","New Hampshire","Rhode Island","Vermont","New Jersey",
        "New York","Pennsylvania")

# Midwest US states
MW <- c("Illinois","Indiana","Michigan","Ohio","Wisconsin","Iowa", "Kansas","Minnesota","Missouri",
        "Nebraska","North Dakota","South Dakota")

# Southern US states
SO <- c("Delaware","Florida","Georgia","Maryland","North Carolina","South Carolina","Virginia",
        "District of Columbia","West Virginia","Alabama","Kentucky","Mississippi","Tennessee",
        "Arkansas","Louisiana","Oklahoma","Texas")

# Western US states
WE <- c("Arizona", "Colorado", "Idaho", "Montana", "Nevada","New Mexico", "Utah",  "Wyoming", "Alaska", 
        "California","Hawaii", "Oregon", "Washington")

# Rename the "states" variable
colnames(dat)[1] <- "states"

# Create a new variable corresponding to the different regions of the US
areas <- rep(NA,length(dat$states))
areas[dat$states%in%NE] <- "NE"
areas[dat$states%in%MW] <- "MW"
areas[dat$states%in%SO] <- "SO"
areas[dat$states%in%WE] <- "WE"
areas <- factor(areas)
dat <- cbind(dat,areas=areas)

n <- nrow(dat)
p <- 4
```

### Exploratory data anslysis

Let's have a quick look at the data:
```{r}
glimpse(dat)
```

Let's look at a scatterplot, marginal density plot and calculate correlations
at the same time:

```{r}
library(GGally)
ggscatmat(dat[,2:5])
```

There appears to be a high correlation (0.8) between assult and murder,
and a high correlation (0.6) between burglary  and urban population.
These numbers suggest that we might be able to perform some dimension
reduction.

Let's perform a principal component analysis using the `prcomp` function.
From the above `glimpse` command we see that the variables are in quite
different scales. Hence, we will both center and scale so that all variables
have unit variance and no one variable dominates the analysis simply because it
is on a different scale.

### PCA via the `prcomp` function

```{r echo=TRUE}
options(digits=2)
pca_res <- prcomp(dat[,2:5], center=TRUE, scale=TRUE)
pca_res
```

From the above we see that PC1 is of the form

$$Y_1 = -0.54 \times Murder - 0.58 \times Assault -0.28 \times UrbanPop -0.54 \times Burglary$$

where the variables `Murder`, `Assault`, `UrbanPop` and `Burglary` have been standardized.
From the abolve PC1 is most negatively correlated with 
`Murder`, `Assault`, and `Burglary`.

Similarly, PC2 is 

$$Y_1 = 0.42 \times Murder - 0.19 \times Assault -0.87 \times UrbanPop -0.17 \times Burglary$$

which is most negatively correlated with `UrbanPop`.

```{r}
lambda     <- pca_res$sdev^2
proportion <- lambda/sum(lambda)
cum_prop   <- cumsum(proportion)

df_variance <- data.frame(
  variance=lambda,
  proportion=proportion,
  cum_prop=cum_prop) %>%
  round(digits = 2)
```

### Table of variance explained

```{r}
lambda     <- pca_res$sdev^2
proportion <- lambda/sum(lambda)
cum_prop   <- cumsum(proportion)

df_variance <- data.frame(
  variance=lambda,
  proportion=proportion,
  cum_prop=cum_prop) %>%
  round(digits = 2)

pca_smry <- t(df_variance) %>%
  as.data.frame()

colnames(pca_smry) <- colnames(pca_res$rotation)
rownames(pca_smry) <- c("Variance", "Proportion", "Cum. prop")
 
library(flextable)
ft_cumprop <- flextable(
  pca_smry %>% 
    rownames_to_column("statistic")) %>% 
  autofit()
ft_cumprop
```




```{r}
ggscreeplot <- df_variance %>%
  ggplot(aes(x=1:p, y=variance)) + 
  geom_line() +
  xlab("Number of PCs") + ylab("Eigenvalue") +
  theme_bw() +
  ggtitle("scree plot") +
  theme(
    plot.title = element_text(color="red", size=14, face="bold.italic"),
    axis.title.x = element_text(color="blue", size=14, face="bold"),
    axis.title.y = element_text(color="#993333", size=14, face="bold")
  )

ggscreeplot
```


```{r}
loadings <- round(pca_res$rotation,2)
items <- rownames(pca_res$rotation)

df_loadings <- data.frame(item=items, 
                          PC1=loadings[,1], 
                          PC2=loadings[,2])


myft1 <- flextable(df_loadings, 
                  col_keys = c("item", "PC1","PC2"))
myft1
```


```{r}
pca_pcs <- tibble(
  PC1=pca_res$x[,1],
  PC2=pca_res$x[,2],
  targets=factor(dat$states),
  areas=factor(dat$areas))
```

```{r}
pca_evc <- df_loadings %>% 
  add_column(
    origin=rep(0, p), 
    variable=items,
    PC1s = df_loadings$PC1*(df_variance$variance[1]*2.5), 
    PC2s = df_loadings$PC2*(df_variance$variance[2]*2.5)
  )
```

```{r}
library(ggrepel)
g <- ggplot() +
  # Add line segments corresponding to PC directions
  geom_segment(data=pca_evc, 
               aes(x=origin, 
                   xend=PC1s, 
                   y=origin, 
                   yend=PC2s), 
               colour="blue",
               size=1) +
  # Add labels to the end of line segments
  geom_text_repel(data=pca_evc, 
                  aes(x=PC1s, 
                      y=PC2s, 
                      label=variable), 
                  colour="purple") +
  # Add principal component scores
  geom_point(data=pca_pcs, 
             aes(x=PC1, y=PC2,colour=areas), 
             size=4) +
  # Add labels for targets
  geom_text_repel(data=pca_pcs, 
                  aes(x=PC1, y=PC2, label=targets), 
                  size=3) +
  xlab("PC1") + ylab("PC2") +
  theme(aspect.ratio=1) +
  theme_bw() +
  guides(color = guide_legend(override.aes = list(size=5))) 
g
```


### PCA via the `eigen` function

Let's do it first on the covariance matrix rather than the correlation matrix
```{r}
X = data.matrix(dat[,2:5])
S = cov(X)
S
```

Here we see that the variances of `Assault` and `UrbanPop` are very large.
This will mean that these two variables will dominate the PCA.

```{r}
res = eigen(S)
lambda = res$values
U = res$vectors
lambda
```
The first two eigenvalues are massive (as expected, since we have not
scaled `Assault` and `UrbanPop`.

```{r}
U[,1:2]
```

The PC1 is 

$$Y_1 = -0.042 \times Murder -0.995 \times Assault -0.045 \times UrbanPop -0.075 \times Burglary$$
so that PC1 us dominated by Assault.  Similarly PC2 is dominated by `UrbanPop`.

Now lets do the same analysis using the correlation matrix.

```{r}
X = scale(X)
R = cor(X)
res = eigen(R)
lambda = res$values
lambda
```
The first eigenvalue is much smaller so this looks much more sensible now.

```{r}
U = res$vectors
U[,1:2]
```

These are the same loadings vectors as from the `prcomp` command.
 
 

### t-SNE in R


```{r, eval=TRUE}
# It is good practice to set the seed at the top of the script
# (and nowhere else)
set.seed(9)

# It is good practie to set any "magic" numbers 
# Or tuning values at the top of the script
n_sub <- 10000
```

```{r, eval=TRUE, cache=TRUE}
# This step will take a little while so we cache this line
mnist_raw <- read.csv(here("data","mnist_train.csv"), header = FALSE)
```

This dataset contains one row for each of the $60000$ samples, 
and one column for each of the $784$ pixels in a $28\times 28$ image. 
The data as downloaded doesnâ€™t have column labels, but are arranged as 
"row 1 column 1, row 1 column 2, row 1 column 3..." and so on). 
This is a useful enough representation for machine learning.  

```{r}
digit_num <- 3
labels <- mnist_raw[,1]
digit_img <- matrix(as.numeric(mnist_raw[digit_num,-1]), 28,28)
rotate <- function(x) t(apply(x, 2, rev))
image(rotate(t(digit_img)))
print(labels[digit_num])
```

We will take a subset of `r {n_sub}` to play with because otherwise
t-SNE needs to store at least two $60000 \times 60000$ matrices
which is too much for most laptops in 2020

```{r}
mnist_sub  <- mnist_raw[1:n_sub,]
labels_sub <- labels[1:n_sub]
```

Next we will use t-SNE to reduce the dimension of the data to two dimensions.

```{r}
## Rtsne function may take some minutes to complete...
tsne_model_1 = Rtsne(mnist_sub,    # The data 
                     check_duplicates=FALSE,  # Don't remove duplicates
                     pca=TRUE,                # Perform PCA before t-SNE
                     perplexity=10,           
                     theta=0.5, 
                     dims=2)

## getting the two dimension matrix
d_tsne_1 = as.data.frame(tsne_model_1$Y)
```
```{r}
mnist_sub
```


Finally, we will plot the data in the two dimensional space with labels added.

```{r}
# Add the labels to the data frame
d_tsne_2 <- cbind( d_tsne_1, labels=factor(labels_sub))

## plotting the results without clustering
ggplot(d_tsne_2, aes(x=V1, y=V2, color=labels)) +
  geom_point(size=1) +
  xlab("") + ylab("") +
  ggtitle("t-SNE") +
  theme_light(base_size=20) +
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank()) + 
  guides(color = guide_legend(override.aes = list(size=5))) +
  scale_fill_hue(l=40)

```

Notice that the clusters for the digits 4 and 9 are close to one another in t-SNE space,
and that the labellings have largely been preserved by t-SNE in the lower dimensional space.


