dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation
k_fold=10
# create k folds
folds<-createFolds(y=dat4$dia_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c()
# K-fold cross-validation:
for (i in 1:k_fold){
fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat4[-folds[[i]],] # remaining is training set
# linear regression using AIC
M1<-lm(dia_score~., data=fold_train) # full model
M0<-lm(dia_score~1, data=fold_train) # null model
lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
summary(lm_model)
dat3<-dat2 %>% mutate(
dia_score= ifelse(DIABBC==5, 0, ifelse(DIABBC==3, 100, ifelse(DIABBC==2, 200, ifelse(DIABBC==1, 300, NA)))),
cho_score= ifelse(HCHOLBC==5, 0, ifelse(HCHOLBC==3, 1, ifelse(HCHOLBC==2, 2, ifelse(HCHOLBC==1, 3, NA)))),
sug_score= ifelse(HSUGBC==5, 0, ifelse(HSUGBC==3, 1, ifelse(HSUGBC==2, 2, ifelse(HSUGBC==1, 3, NA)))),
hyp_score= ifelse(HYPBC==5, 0, ifelse(HYPBC==3, 1, ifelse(HYPBC==2, 2, ifelse(HYPBC==1, 3, NA)))),
final_score = dia_score+cho_score+sug_score+hyp_score
)
set.seed(2021)
# Predict dia_score, drop other y and y-related variables
dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation
k_fold=10
# create k folds
folds<-createFolds(y=dat4$dia_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c()
# K-fold cross-validation:
for (i in 1:k_fold){
fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat4[-folds[[i]],] # remaining is training set
# linear regression using AIC
M1<-lm(dia_score~., data=fold_train) # full model
M0<-lm(dia_score~1, data=fold_train) # null model
lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
summary(lm_model)
dat3<-dat2 %>% mutate(
dia_score= ifelse(DIABBC==5, 0, ifelse(DIABBC==3, 1, ifelse(DIABBC==2, 2, ifelse(DIABBC==1, 3, NA)))),
cho_score= ifelse(HCHOLBC==5, 0, ifelse(HCHOLBC==3, 1, ifelse(HCHOLBC==2, 2, ifelse(HCHOLBC==1, 3, NA)))),
sug_score= ifelse(HSUGBC==5, 0, ifelse(HSUGBC==3, 1, ifelse(HSUGBC==2, 2, ifelse(HSUGBC==1, 3, NA)))),
hyp_score= ifelse(HYPBC==5, 0, ifelse(HYPBC==3, 1, ifelse(HYPBC==2, 2, ifelse(HYPBC==1, 3, NA)))),
final_score = dia_score+cho_score+sug_score+hyp_score
)
set.seed(2021)
# Predict dia_score, drop other y and y-related variables
dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation
k_fold=10
# create k folds
folds<-createFolds(y=dat4$dia_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c()
# K-fold cross-validation:
for (i in 1:k_fold){
fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat4[-folds[[i]],] # remaining is training set
# linear regression using AIC
M1<-lm(dia_score~., data=fold_train) # full model
M0<-lm(dia_score~1, data=fold_train) # null model
lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
summary(lm_model)
install.packages('DMwR')
library(DMwR)
install.packages('DMwR')
install.packages('smotefamily')
library(smotefamily)
library(tidyverse)
#library(here)      # directory referencing
#library(readxl)    # reading Excel files
#library(janitor)   # data cleaning
#library(stringr)   # string manipulation
library(tidyr)     # new tidy functions
library(knitr) # kable
#library(modi) # ok for multivariate outlier detection
library(caret)# low variance filter
install.packages('caret')
library(tidyverse)
#library(here)      # directory referencing
#library(readxl)    # reading Excel files
#library(janitor)   # data cleaning
#library(stringr)   # string manipulation
library(tidyr)     # new tidy functions
library(knitr) # kable
#library(modi) # ok for multivariate outlier detection
library(caret)# low variance filter
library(tidyverse)
#library(here)      # directory referencing
#library(readxl)    # reading Excel files
#library(janitor)   # data cleaning
#library(stringr)   # string manipulation
library(tidyr)     # new tidy functions
library(knitr) # kable
#library(modi) # ok for multivariate outlier detection
library(caret)# low variance filter
# missing values
#library(naniar)
#library(knitr)
#library(ggpubr) # ggplot arrangement
#ploting
library(gridExtra)
library(kableExtra)
#outlier
#library(univOutl)
# tree methods
#library(tourr)
#library(RColorBrewer)
#library(plotly)
#library(htmltools)
library(performanceEstimation)# for SMOTE
library(rpart)
library(rpart.plot)
library(rattle) #fancyRpartPlot
library(Rtsne)
library(randomForest)
library(neuralnet)
library(e1071)# SVM regression
library(mltools)
library(data.table)
library(skimr)
library(smotefamily)
library(broom)
library(jtools)
load("tech_data.Rdata") # load cleaned data from John's code, make sure you have the Rdata file within the working directory
#dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64, SMKSTAT==5)  # filter age and smoke status
dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64)  # filter age and smoke status
var_list<-c("BMISC","SEX","AGEC","DIABBC","HCHOLBC","HSUGBC","HYPBC", "SYSTOL","DIASTOL","TRIGRESB","FOLATREB","CHOLRESB","LDLRESB","HBA1PREB","GLUCFREB","ALTRESB","HDLCHREB") # add/remove variables that are interested
dat2<-dat %>% select (var_list) # select columns that we are interested
str(dat2) # 3488 obs x 13 variables
skim(dat2$DIABBC)
#dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64, SMKSTAT==5)  # filter age and smoke status
dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64)  # filter age and smoke status
var_list<-c("BMISC","SEX","AGEC","DIABBC","HCHOLBC","HSUGBC","HYPBC", "SYSTOL","DIASTOL","TRIGRESB","FOLATREB","CHOLRESB","LDLRESB","HBA1PREB","GLUCFREB","ALTRESB","HDLCHREB","CVDMEDST") # add/remove variables that are interested
dat2<-dat %>% select (var_list) # select columns that we are interested
str(dat2) # 3488 obs x 13 variables
skim(dat2$DIABBC)
# dat3 contains obesity scores that are manually created for 4 diseases
# the higher score the more likely to be obesity
dat3<-dat2 %>% mutate(
dia_score= ifelse(DIABBC==5, 0, ifelse(DIABBC==3, 1, ifelse(DIABBC==2, 2, ifelse(DIABBC==1, 3, NA)))),
cho_score= ifelse(HCHOLBC==5, 0, ifelse(HCHOLBC==3, 1, ifelse(HCHOLBC==2, 2, ifelse(HCHOLBC==1, 3, NA)))),
sug_score= ifelse(HSUGBC==5, 0, ifelse(HSUGBC==3, 1, ifelse(HSUGBC==2, 2, ifelse(HSUGBC==1, 3, NA)))),
hyp_score= ifelse(HYPBC==5, 0, ifelse(HYPBC==3, 1, ifelse(HYPBC==2, 2, ifelse(HYPBC==1, 3, NA)))),
cvd_score= ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),1,ifelse(CVDMEDST==4,0,NA)),
final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
)
hist(dat3$dia_score) # the final score is highly unbalanced
nrow(dat3[which(dat3$DIABBC==5),]) # the final score of 2790 observations are 0
skim(dat3)
# dat3 contains obesity scores that are manually created for 4 diseases
# the higher score the more likely to be obesity
dat3<-dat2 %>% mutate(
dia_score= ifelse(DIABBC==5, 0, ifelse(DIABBC==3, 1, ifelse(DIABBC==2, 2, ifelse(DIABBC==1, 3, NA)))),
cho_score= ifelse(HCHOLBC==5, 0, ifelse(HCHOLBC==3, 1, ifelse(HCHOLBC==2, 2, ifelse(HCHOLBC==1, 3, NA)))),
sug_score= ifelse(HSUGBC==5, 0, ifelse(HSUGBC==3, 1, ifelse(HSUGBC==2, 2, ifelse(HSUGBC==1, 3, NA)))),
hyp_score= ifelse(HYPBC==5, 0, ifelse(HYPBC==3, 1, ifelse(HYPBC==2, 2, ifelse(HYPBC==1, 3, NA)))),
cvd_score= ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),1,ifelse(CVDMEDST==4,0,NA)),
final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
)
hist(dat3$cvd_score) # the final score is highly unbalanced
nrow(dat3[which(dat3$CVDMEDST==5),]) # the final score of 2790 observations are 0
skim(dat3)
SMOTE <- function(form,data,
perc.over=200,k=5,
perc.under=200,
learner=NULL,...
)
# INPUTS:
# form a model formula
# data the original training set (with the unbalanced distribution)
# minCl  the minority class label
# per.over/100 is the number of new cases (smoted cases) generated
#              for each rare case. If perc.over < 100 a single case
#              is generated uniquely for a randomly selected perc.over
#              of the rare cases
# k is the number of neighbours to consider as the pool from where
#   the new examples are generated
# perc.under/100 is the number of "normal" cases that are randomly
#                selected for each smoted case
# learner the learning system to use.
# ...  any learning parameters to pass to learner
{
# the column where the target variable is
tgt <- which(names(data) == as.character(form[[2]]))
minCl <- levels(data[,tgt])[which.min(table(data[,tgt]))]
# get the cases of the minority class
minExs <- which(data[,tgt] == minCl)
# generate synthetic cases from these minExs
if (tgt < ncol(data)) {
cols <- 1:ncol(data)
cols[c(tgt,ncol(data))] <- cols[c(ncol(data),tgt)]
data <-  data[,cols]
}
newExs <- smote.exs(data[minExs,],ncol(data),perc.over,k)
if (tgt < ncol(data)) {
newExs <- newExs[,cols]
data <- data[,cols]
}
# get the undersample of the "majority class" examples
selMaj <- sample((1:NROW(data))[-minExs],
as.integer((perc.under/100)*nrow(newExs)),
replace=T)
# the final data set (the undersample+the rare cases+the smoted exs)
newdataset <- rbind(data[selMaj,],data[minExs,],newExs)
# learn a model if required
if (is.null(learner)) return(newdataset)
else do.call(learner,list(form,newdataset,...))
}
# ===================================================
# Obtain a set of smoted examples for a set of rare cases.
# L. Torgo, Feb 2010
# ---------------------------------------------------
smote.exs <- function(data,tgt,N,k)
# INPUTS:
# data are the rare cases (the minority "class" cases)
# tgt is the name of the target variable
# N is the percentage of over-sampling to carry out;
# and k is the number of nearest neighours to use for the generation
# OUTPUTS:
# The result of the function is a (N/100)*T set of generated
# examples with rare values on the target
{
nomatr <- c()
T <- matrix(nrow=dim(data)[1],ncol=dim(data)[2]-1)
for(col in seq.int(dim(T)[2]))
if (class(data[,col]) %in% c('factor','character')) {
T[,col] <- as.integer(data[,col])
nomatr <- c(nomatr,col)
} else T[,col] <- data[,col]
if (N < 100) { # only a percentage of the T cases will be SMOTEd
nT <- NROW(T)
idx <- sample(1:nT,as.integer((N/100)*nT))
T <- T[idx,]
N <- 100
}
p <- dim(T)[2]
nT <- dim(T)[1]
ranges <- apply(T,2,max)-apply(T,2,min)
nexs <-  as.integer(N/100) # this is the number of artificial exs generated
# for each member of T
new <- matrix(nrow=nexs*nT,ncol=p)    # the new cases
for(i in 1:nT) {
# the k NNs of case T[i,]
xd <- scale(T,T[i,],ranges)
for(a in nomatr) xd[,a] <- xd[,a]==0
dd <- drop(xd^2 %*% rep(1, ncol(xd)))
kNNs <- order(dd)[2:(k+1)]
for(n in 1:nexs) {
# select randomly one of the k NNs
neig <- sample(1:k,1)
ex <- vector(length=ncol(T))
# the attribute values of the generated case
difs <- T[kNNs[neig],]-T[i,]
new[(i-1)*nexs+n,] <- T[i,]+runif(1)*difs
for(a in nomatr)
new[(i-1)*nexs+n,a] <- c(T[kNNs[neig],a],T[i,a])[1+round(runif(1),0)]
}
}
newCases <- data.frame(new)
for(a in nomatr)
newCases[,a] <- factor(newCases[,a],levels=1:nlevels(data[,a]),labels=levels(data[,a]))
newCases[,tgt] <- factor(rep(data[1,tgt],nrow(newCases)),levels=levels(data[,tgt]))
colnames(newCases) <- colnames(data)
newCases
}
```{r smote}
dat33<-dat3[,1:18] # do NOT include DIABBC and sores other than dia_score
dat33<-na.omit(dat33)
table(dat33$CVDMEDST)
dat33.balanced <-smote(DIABBC~., dat33, perc.over=600, perc.under=2)
table(dat33.balanced$DIABBC)
dat33.balanced<-dat33.balanced %>% mutate(
dia_score= ifelse(DIABBC==5, 0, 1),
)
dat33.balanced$dia_score<-as.factor(dat33.balanced$dia_score)
#
#
# dat33.balanced<-dat33.balanced[,-4]
# dat33.balanced
#
# model<-lm(dia_score~., data=dat33.balanced)
set.seed(2021)
training.samples <- dat33.balanced$dia_score %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat33.balanced[training.samples, ]
test.data <- dat33.balanced[-training.samples, ]
dat33.balanced
#model<-nnet::multinom(dia_score~.-DIABBC, data=train.data)
model<-nnet::multinom(dia_score~.-DIABBC, family=binomial, data=train.data)
predict_class<-predict(model,newdata=test.data)
length(predict_class)
length(test.data$DIABBC)
summ(model)
dat33<-dat3[,1:18] # do NOT include DIABBC and sores other than dia_score
dat33<-na.omit(dat33)
table(dat33$CVDMEDST)
dat33.balanced <-smote(CVDMEDST~., dat33, perc.over=600, perc.under=2)
dat33<-dat3[,1:18] # do NOT include DIABBC and sores other than dia_score
dat33$CVDMEDST<-droplevels(dat33$CVDMEDST)
dat33<-na.omit(dat33)
table(dat33$CVDMEDST)
dat33.balanced <-smote(CVDMEDST~., dat33, perc.over=600, perc.under=2)
table(dat33.balanced$CVDMEDST)
dat33.balanced<-dat33.balanced %>% mutate(
dia_score= ifelse(CVDMEDST==5, 0, 1),
)
dat33.balanced$dia_score<-as.factor(dat33.balanced$dia_score)
#
#
# dat33.balanced<-dat33.balanced[,-4]
# dat33.balanced
#
# model<-lm(dia_score~., data=dat33.balanced)
set.seed(2021)
training.samples <- dat33.balanced$dia_score %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat33.balanced[training.samples, ]
test.data <- dat33.balanced[-training.samples, ]
dat33.balanced
#model<-nnet::multinom(dia_score~.-DIABBC, data=train.data)
model<-nnet::multinom(dia_score~.-DIABBC, family=binomial, data=train.data)
dat33<-dat3[,1:18] # do NOT include DIABBC and sores other than dia_score
dat33$CVDMEDST<-droplevels(dat33$CVDMEDST)
dat33<-na.omit(dat33)
table(dat33$CVDMEDST)
dat33.balanced <-smote(CVDMEDST~., dat33, perc.over=600, perc.under=2)
table(dat33.balanced$CVDMEDST)
dat33.balanced<-dat33.balanced %>% mutate(
dia_score= ifelse(CVDMEDST==5, 0, 1),
)
dat33.balanced$dia_score<-as.factor(dat33.balanced$dia_score)
#
#
# dat33.balanced<-dat33.balanced[,-4]
# dat33.balanced
#
# model<-lm(dia_score~., data=dat33.balanced)
set.seed(2021)
training.samples <- dat33.balanced$dia_score %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat33.balanced[training.samples, ]
test.data <- dat33.balanced[-training.samples, ]
dat33.balanced
#model<-nnet::multinom(dia_score~.-DIABBC, data=train.data)
model<-nnet::multinom(cvd_score~.-CVDMEDST, family=binomial, data=train.data)
set.seed(2021)
# Predict dia_score, drop other y and y-related variables
dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC,CVDMEDST, cho_score, sug_score, hyp_score, final_score))
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation
k_fold=10
# create k folds
folds<-createFolds(y=dat4$dia_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c()
#table(dat33.balanced$HYPBC)
# K-fold cross-validation:
for (i in 1:k_fold){
fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat4[-folds[[i]],] # remaining is training set
# linear regression using AIC
M1<-lm(cvd_score~., data=fold_train) # full model
M0<-lm(cvd_score~1, data=fold_train) # null model
lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
set.seed(2021)
# Predict dia_score, drop other y and y-related variables
dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score,CVDMEDST, hyp_score, final_score))
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation
k_fold=10
# create k folds
folds<-createFolds(y=dat4$cvd_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c()
# K-fold cross-validation:
for (i in 1:k_fold){
fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat4[-folds[[i]],] # remaining is training set
# linear regression using AIC
M1<-lm(cvd_score~., data=fold_train) # full model
M0<-lm(cvd_score~1, data=fold_train) # null model
lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
test_error_RMSE[i] = RMSE(fold_predict, fold_test$cvd_score) # calculate and record RMSE
}
summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
# we use dat3 as a starting point for neural network analysis
dat5=dat3
# normalize all numeric variables
mystd <-function (x){
x<- (x-mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)
}
# remove NA
dat5<-na.omit(dat5)
# normalize numeric values in dat5 such that they have mean value = 0 and std = 1
dat5_std<-dat5%>% mutate_if(is.numeric, list(mystd))
# create dataset with one hot encoding
dat5_one_hot <- one_hot(as.data.table(dat5_std))
# set up k value for k-fold cross validation
k_fold=10
# set number of hiddne layers
hid_layer=5
# create k folds
folds<-createFolds(y=dat5_one_hot$dia_score, k=k_fold)
test_error_RMSE<-c()
for (i in 1:k_fold){
fold_test<-dat5_one_hot[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat5_one_hot[-folds[[i]],] # remaining is training set
network<-neuralnet(dia_score~.-final_score-cho_score-sug_score-hyp_score, data=fold_train, hidden =hid_layer)
fold_predict<-predict(network, fold_test)
test_error_RMSE[i]<-RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
plot(network)
# we use dat3 as a starting point for neural network analysis
dat5=dat3
# normalize all numeric variables
mystd <-function (x){
x<- (x-mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)
}
# remove NA
dat5<-na.omit(dat5)
# normalize numeric values in dat5 such that they have mean value = 0 and std = 1
dat5_std<-dat5%>% mutate_if(is.numeric, list(mystd))
# create dataset with one hot encoding
dat5_one_hot <- one_hot(as.data.table(dat5_std))
# set up k value for k-fold cross validation
k_fold=10
# set number of hiddne layers
hid_layer=5
# create k folds
folds<-createFolds(y=dat5_one_hot$dia_score, k=k_fold)
test_error_RMSE<-c()
for (i in 1:k_fold){
fold_test<-dat5_one_hot[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat5_one_hot[-folds[[i]],] # remaining is training set
network<-neuralnet(cvd_score~.-final_score-cho_score-sug_score-hyp_score, data=fold_train, hidden =hid_layer)
fold_predict<-predict(network, fold_test)
test_error_RMSE[i]<-RMSE(fold_predict, fold_test$cvd_score) # calculate and record RMSE
}
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
plot(network)
