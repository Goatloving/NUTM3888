mean(as.numeric(err_list))
# unbalanced data without smote: not working, predict all 0
folds<-createFolds(y=temp_dat[,1], k=k_fold)
# K-fold cross-validation for smote balanced data
err_list<-list()
for (i in 1:k_fold){
fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
fold_train<-temp_dat[-folds[[i]],] # remaining is training set
res_svm2 <- svm(y~.,data=temp_dat, cost=C_val,kernel="linear",gamma=gamma)
fold_predict <- predict(res_svm2, fold_test)
err<-sum(as.numeric((fold_predict!=fold_test$y)))/nrow(fold_test) # error rate for i th cv
err_list[i]=err
}
mean(as.numeric(err_list))
coef(res_svm2)
confusionMatrix(table(fold_predict, fold_test$y))
library(e1071)
C_val <- 1
coef0 <- 1
gamma <- 1/ncol(X2[,-4])
degree <- 3
# radial SVM
k_fold=10
# create k folds
folds<-createFolds(y=temp_dat_bal[,1], k=k_fold)
# create a new vector to record test results
# K-fold cross-validation for smote balanced data
err_list<-list()
for (i in 1:k_fold){
fold_test<-temp_dat_bal[folds[[i]],] # select folds[[i]] as test test
fold_train<-temp_dat_bal[-folds[[i]],] # remaining is training set
res_svm2 <- svm(y~.,data=temp_dat_bal, cost=C_val,kernel="radial",gamma=gamma)
fold_predict <- predict(res_svm2, fold_test)
err<-sum(as.numeric((fold_predict!=fold_test$y)))/nrow(fold_test) # error rate for i th cv
err_list[i]=err
}
w<-t(res_svm2$coefs)%*%res_svm2$SV
b<--1*res_svm2$rho
data.frame(cbind(t(w),b))
coef(res_svm2)
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)
err_list<-list()
for (i in 1:k_fold){
fold_test<-temp_dat_bal[folds[[i]],] # select folds[[i]] as test test
fold_train<-temp_dat_bal[-folds[[i]],] # remaining is training set
model<-neuralnet(y~., data=fold_train, hidden=3)
fold_predict <- predict(model, fold_test)
err<-sum(as.numeric((fold_predict!=fold_test$y)))/nrow(fold_test) # error rate for i th cv
err_list[i]=err
}
model<-neuralnet(y~., data=fold_train, hidden=3)
temp_dat
temp_dat$y<-as.numeric(temp_dat$y)
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)
temp_dat$y<-as.numeric(temp_dat$y)
err_list<-list()
for (i in 1:k_fold){
fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
fold_train<-temp_dat[-folds[[i]],] # remaining is training set
model<-neuralnet(y~., data=fold_train, hidden=3)
fold_predict <- predict(model, fold_test)
err<-sum(as.numeric((fold_predict!=fold_test$y)))/nrow(fold_test) # error rate for i th cv
err_list[i]=err
}
temp_dat<-one_hot(temp_dat)
temp_dat<-one_hot(as.data.table(temp_dat))
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)
temp_dat<-one_hot(as.data.table(temp_dat))
err_list<-list()
for (i in 1:k_fold){
fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
fold_train<-temp_dat[-folds[[i]],] # remaining is training set
model<-neuralnet(y~., data=fold_train, hidden=3)
fold_predict <- predict(model, fold_test)
err<-sum(as.numeric((fold_predict!=fold_test$y)))/nrow(fold_test) # error rate for i th cv
err_list[i]=err
}
fold_train
hist(fold_train$y)
mystd<-function(x){
x<-(x-mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)
}
x<-X2[,-4] %>% mutate_if(is.numeric, list(mystd))
y<-Y1$cho_score
temp_dat<-data.frame(cbind(y,x)) %>% na.omit() # need to drop Chol_score in X2 since Y is calculated based on chol_score
temp_dat
temp_dat_bal<-smote(y~., data = temp_dat, k=10)
table(temp_dat_bal$y) # highly unbalanced data: 1882 0 and 280 1
x_onehot<-one_hot(as.data.table(x))
temp_dat_onehot<-cbind(y,x_onehot)
fold_test<-temp_dat_onehot[folds[[i]],] # select folds[[i]] as test test
fold_train<-temp_dat_onehot[-folds[[i]],] # remaining is training set
temp_dat_onehot
temp_dat_onehot<-cbind(y,x_onehot)%>%na.omit()
fold_test<-temp_dat_onehot[folds[[i]],] # select folds[[i]] as test test
folds<-createFolds(y=temp_dat[,1],k=k_fold)
folds
temp_dat_onehot[folds[1],]
temp_dat_onehot<-data.frame(temp_dat_onehot)
fold_test<-temp_dat_onehot[folds[[i]],] # select folds[[i]] as test test
fold_train<-temp_dat_onehot[-folds[[i]],] # remaining is training set
for (i in 1:k_fold){
fold_test<-temp_dat_onehot[folds[[i]],] # select folds[[i]] as test test
fold_train<-temp_dat_onehot[-folds[[i]],] # remaining is training set
model<-neuralnet(y~., data=fold_train, hidden=3)
fold_predict <- predict(model, fold_test)
err<-sum(as.numeric((fold_predict!=fold_test$y)))/nrow(fold_test) # error rate for i th cv
err_list[i]=err
}
err_list
fold_predict
model$pred
model$pred<-fold_predict[,1]<0.5
table(fold_test$y, model$pred)
res<-nerualnet(y~., data=fold_train,hidden=3)
res<-neuralnet(y~., data=fold_train,hidden=3)
pred<-predict(res, fold_test)
res$pred<-pred[,1]<0.5
table(fold_test$y, res$pred)
table(res$pred,fold_test$y)
confusionMatrix(table(res$pred,fold_test$y))
pred[,1]
fold_test
fold_test$y
pred<- pred %>% mutate(
pred_class = ifelse(pred[,1]<0.5, 0, 1)
)
pred<- pred %>% mutate(
pred_class = ifelse(pred[1]<0.5, 0, 1)
)
pred<-data.frame(pred)
pred
pred<- pred %>% mutate(
pred_class = ifelse(X1<0.5, 0, 1)
)
fold_test$y
res$pred<-pred[,1]<0.5
confusionMatrix(table(res$pred,fold_test$y))
confusionMatrix(table(pred$pred_class,fold_test$y))
model<-neuralnet(y~., data=fold_train, hidden=3)
err_list[5]
res<-neuralnet(y~., data=fold_train,hidden=3)
res<-neuralnet(y~., data=fold_train,hidden=3)
pred<-pred
print(res)
plot(model)
res$weights
temp_dat
temp_dat_onehot
res$weights
compute(model, fold_test)
pred<-predict(res, fold_test)
pred
pred<-predict(res, fold_test[,-1])
pred
x_onehot<-mystd(x_onehot)
res<-neuralnet(y~., data=fold_train,hidden=3)
res<-neuralnet(y~., data=fold_train,hidden=3)
pred<-predict(res, fold_test[,-1])
pred<-predict(res, fold_test[,-1])
pred
model$result.matrix
pred2<-compute(model, fold_test)
pred
pred[1]
pred[,1]
return x
classifcation<-function (x){
if (x>0.5){
x=1
} else{
x=0
}
return (x)
}
res<-apply(pred,1 ,classifcation)
res
confusionMatrix(table(res,fold_test$y))
if (x<0.5){
classifcation<-function (x){
if (x<0.5){
x=1
} else{
x=0
}
return (x)
}
res<-apply(pred,1 ,classifcation)
res
confusionMatrix(table(res,fold_test$y))
confusionMatrix(table(res,fold_test$y))
table(res,fold_test$y)
table(res,fold_test$y)
res
install.packages(installr)
install.packages('installr'')
install.packages('installr')
require('installr')
updateR()
library(xfun)
install.packages("xfun")
install.packages('rtools')
source('myfunc.R') # self-defiend functions
library(tidyverse)
source('myfunc.R') # self-defiend functions
library(tidyverse)
install.packages('tidyverse')
source('myfunc.R') # self-defiend functions
library(tidyverse)
library(tidyr)     # new tidy functions
library(knitr) # kable
source('myfunc.R') # self-defiend functions
library(tidyverse)
library(tidyr)     # new tidy functions
library(knitr) # kable
library(caret)# low variance filter
library(caret)# low variance filter
install.packages('caret')
library(caret)# low variance filter
library(knitr) # kable
library(caret)# low variance filter
source('myfunc.R') # self-defiend functions
library(tidyverse)
library(tidyr)     # new tidy functions
library(knitr) # kable
library(caret)# low variance filter
library(glmnet)
install.packages('glmnet')
library(glmnet)
library(brglm)
install.packages('brglm')
library(brglm)
library(modelsummary)
install.packages('modelsummary')
library(brglm)
library(gridExtra)
library(modelsummary)
library(rpart)
library(rpart.plot)
library(rattle) #fancyRpartPlot
library(Rtsne)
library(randomForest)
library(neuralnet)
library(e1071)# SVM regression
library(mltools)
library(data.table)
library(skimr)
library(smotefamily)
library(broom)
library(jtools)
library(ranger)
library(ROCR)
library(ROCR)
library(ranger)
install.packages('ranger')
library(ranger)
source('myfunc.R') # self-defiend functions
library(tidyverse)
library(tidyr)     # new tidy functions
library(knitr) # kable
library(caret)# low variance filter
library(glmnet)
library(brglm)
library(modelsummary)
library(gridExtra)
#dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64, SMKSTAT==5)  # filter age and smoke status
dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64)  # filter age only
# dat<-dat%>% mutate(
#  rfm = ifelse(SEX==2, 76-(20*(PHDCMHBC/PHDCMWBC)),64-(20*(PHDCMHBC/PHDCMWBC)) )
# )
#
var_list<-c("BMISC","PHDKGWBC","PHDCMHBC","SLPTIME","SEX","AGEC","DIABBC","HCHOLBC","HSUGBC","HYPBC","PHDCMWBC","EXLWMBC","EXLWVBC", "SYSTOL","DIASTOL","CVDMEDST","SMKSTAT") # add/remove variables that are interested
dat3<-dat %>% select (var_list) # select columns that we are interested
dat3$EXLWMBC<-as.numeric(as.character(dat$EXLWMBC)) # exerices time should be numeric
dat3$EXLWVBC<-as.numeric(as.character(dat$EXLWVBC)) # exerciese time should be numeric
str(dat3)
dat3c<-dat3%>%na.omit()
str(dat3c)
#table(dat3c$CVDMEDST)
hist(dat3$SYSTOL)
dat3c
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")
Y<-dat3c%>% select (response)
X<-dat3c%>% select (!response)
y<-Y %>% mutate(
CVDMEDST = ifelse((CVDMEDST==4), 0, ifelse((CVDMEDST==3|CVDMEDST==2|CVDMEDST==1),1, NA))
)
y<-y$CVDMEDST # y is binary (0,1)
x<-X
str(x)
temp_dat
temp_dat<-cbind(y,x)
temp_dat<-na.omit(temp_dat)
str(temp_dat)
# K-fold CV
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)
# create some matrix to record results during k-fold cv
result_full<- matrix(NA, k_fold, 2) # record performance indicators
model_full=list() # record model
result_rfm<- matrix(NA, k_fold, 2)
model_rfm=list()
result_nobmi<- matrix(NA, k_fold, 2)
model_nobmi=list()
for (i in 1:k_fold){
fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
fold_train<-temp_dat[-folds[[i]],] # remaining is training set
n<-nrow(fold_train)
# glm
full_model <-glm(y~., data=temp_dat, family=binomial)
null_model <-glm(y~1, data=temp_dat, family=binomial)
model <- step(full_model,k=log(n),trace=0)
pred<-predict(model, fold_test, type="response") # numeric predicted values
fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
matrix<-table(fold_predict, fold_test$y)# confusion matrix
if (nrow(matrix<2)){
matrix<-rbind(matrix,c(0,0))
}
matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
f1<-calculate_f1(matrix) # calculate f1 score
auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC
model_full[i]<-model
result_full[i,1]<-f1
result_full[i,2]<-auc
# # glm
# full_model <-glm(y~., data=temp_dat, family=binomial)
# null_model <-glm(y~1, data=temp_dat, family=binomial)
# model<-glm(y~., data=temp_dat, family=binomial)
# pred<-predict(model, fold_test, type="response") # numeric predicted values
# fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
# matrix<-table(fold_predict, fold_test$y)# confusion matrix
# if (nrow(matrix<2)){
#   matrix<-rbind(matrix,c(0,0))
# }
#
#
# matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
# f1<-calculate_f1(matrix) # calculate f1 score
# auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC
# model_full[i]<-model
# result_full[i,1]<-f1
# result_full[i,2]<-auc
# rfm
# model<-glm(y~rfm, data=temp_dat, family=binomial)
# pred<-predict(model, fold_test, type="response") # numeric predicted values
# fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
# matrix<-table(fold_predict, fold_test$y)# confusion matrix
# if (nrow(matrix<2)){
#   matrix<-rbind(matrix,c(0,0))
# }
#
#
# matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
# f1<-calculate_f1(matrix) # calculate f1 score
# auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC
# model_rfm[i]<-model
# result_rfm[i,1]<-f1
# result_rfm[i,2]<-auc
}
install.packages(PROC)
install.packages(RROC)
str(temp_dat)
temp_dat<-cbind(y,x)
temp_dat<-na.omit(temp_dat)
str(temp_dat)
temp_dat<-cbind(as.factor(y),x)
temp_dat<-na.omit(temp_dat)
str(temp_dat)
y<-as.factor(y)
temp_dat<-cbind(y,x)
temp_dat<-na.omit(temp_dat)
str(temp_dat)
model<-ranger(
y~.,
data=temp_dat,
mtry=floor((ncol(temp_dat)-1)/3),
seed=2021,
respect.unordered.factors = "order",
)
model$prediction.error
model
model<-ranger(
y~.,
data=temp_dat,
# mtry=floor((ncol(temp_dat)-1)/3),
mrty=12,
seed=2021,
respect.unordered.factors = "order",
)
model
model<-ranger(
y~.,
data=temp_dat,
# mtry=floor((ncol(temp_dat)-1)/3),
mrty=22,
seed=2021,
respect.unordered.factors = "order",
)
model
model<-ranger(
y~.-BMISC,
data=temp_dat,
# mtry=floor((ncol(temp_dat)-1)/3),
mrty=22,
seed=2021,
respect.unordered.factors = "order",
)
model<-ranger(
y~.-BMISC,
data=temp_dat,
mtry=floor((ncol(temp_dat)-1)/3),
#mrty=22,
seed=2021,
respect.unordered.factors = "order",
)
model
model<-ranger(
y~.-BMISC-AGE,
data=temp_dat,
mtry=floor((ncol(temp_dat)-1)/3),
#mrty=22,
seed=2021,
respect.unordered.factors = "order",
)
model
model<-ranger(
y~.,
data=temp_dat,
mtry=floor((ncol(temp_dat)-1)/3),
#mrty=22,
seed=2021,
respect.unordered.factors = "order",
)
model
model<-ranger(
y~.,
data=temp_dat,
num.trees = 2000,
mtry=floor((ncol(temp_dat)-1)/3),
#mrty=22,
seed=2021,
respect.unordered.factors = "order",
)
model
model<-ranger(
y~.,
data=temp_dat,
num.trees = 2000,
min.node.size = 5,
importance = 'impurity',
mtry=floor((ncol(temp_dat)-1)/3),
#mrty=22,
seed=2021,
respect.unordered.factors = "order",
)
model
install.packages('vip')
library(vip)
library(vip)
p1<-vip::vip(model,num_features=10,bar=FALSE)
p1
partial(model, pred.var = "AGEC", prob=TRUE, plot=TRUE)
partial(model, pred.var = "AGEC", prob=TRUE, plot=TRUE)
plotPartial(model)
install.packages('pdp')
lirary(pbp)
lirary(pdp)
library(pdp)
plotPartial(model)
model<-randomForest(
y~.,
data=temp_dat,
num.trees = 2000,
min.node.size = 5,
importance = 'impurity',
mtry=floor((ncol(temp_dat)-1)/3),
#mrty=22,
seed=2021,
respect.unordered.factors = "order",
)
install.packages('randomForest')
library(randomForest)
model<-randomForest(
y~.,
data=temp_dat,
num.trees = 2000,
min.node.size = 5,
importance = 'impurity',
mtry=floor((ncol(temp_dat)-1)/3),
#mrty=22,
seed=2021,
respect.unordered.factors = "order",
)
model<-randomForest(
y~.,
data=temp_dat,
num.trees = 2000,
min.node.size = 5,
mtry=floor((ncol(temp_dat)-1)/3),
#mrty=22,
seed=2021,
respect.unordered.factors = "order",
)
model
mystd(temp_dat)
install.packages('pdp')
install.packages("pdp")
library(Rtools)
install.packages("Rtools")
