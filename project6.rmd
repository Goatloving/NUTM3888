---
title: "Project6"
output: 
  html_document: 
    toc: yes
    highlight: pygments
    theme: journal
    number_sections: yes
---

# Setup

```{r setup, include=FALSE}
source('myfunc.R') # self-defiend functions
library(tidyverse)
library(tidyr)     # new tidy functions
library(knitr) # kable
library(caret)# low variance filter
library(glmnet)
library(brglm)
library(modelsummary)
library(gridExtra)
library(kableExtra)
library(performanceEstimation)# for SMOTE
library(rpart)
library(rpart.plot)
library(rattle) #fancyRpartPlot
library(Rtsne)
library(randomForest)
library(neuralnet)
library(e1071)# SVM regression
library(mltools)
library(data.table)
library(skimr)
library(smotefamily)
library(broom)
library(jtools)
library(ranger)
library(ROCR)
library(pROC)
library(flextable)
library(nnet)
library(VGAM) # vglm multiple class logisitic regression

```

# Todo

-   Random forest on Model 31 (y=binary) still problematic
-   

# Dataset

```{r}
load("tech_data.Rdata")
```

## dat5 (biom+nutr datasets combined)

contains CVDMEDST (the only diease) and other predictors

```{r}
dat<-cbind(tech_biom, tech_nutr)
predictor_list<-c("BMR","FATT1","FATT2","MOISTT1","MOISTT2","PROPER1","PROPER2","SUGPER1","SUGPER2","ALCT1","ALCT2","B1T1","B1T2","B2T1","B2T2", "B3T1","B3T2","B6T1","B6T2","B12T1","B12T2","CAFFT1","CAFFT2","FATPER1","FATPER2","TRANPER1","TRANPER2","MONOPER1","MONOPER2","POLYPER1","POLYPER2","WATERG1N","WATERG2N","PHDKGWBC","PHDCMHBC","SLPTIME","DIETRDI","DIETQ5","DIETQ8","SEX","AGEC","PHDCMWBC","EXLWMBC","EXLWVBC", "SYSTOL","DIASTOL","CVDMEDST","SMKSTAT",)

#response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")

dat4<-dat%>% select(predictor_list) %>% filter(AGEC>=19, AGEC<=64)

t_list<-colnames(dat4%>% select(contains("T1")))
r_list<-colnames(dat4%>% select(contains("R1")))
n_list<-colnames(dat4%>% select(contains("1N")))

len<-ncol(dat4)

for (i in 1: len){
  if (colnames(dat4[i])%in% t_list){
    
    new_col = (dat4[,i]+dat4[,i+1])/2
    
    new_names = paste0("AVG_", colnames(dat4[i]))
    
    
    dat4<-cbind(dat4,new_col)
    names(dat4)[names(dat4)=="new_col"]<-new_names
    
  }
   if (colnames(dat4[i])%in% r_list){
    
    new_col = (dat4[,i]+dat4[,i+1])/2
    
    new_names = paste0("AVG_", colnames(dat4[i]))
    
    dat4<-cbind(dat4,new_col)
    names(dat4)[names(dat4)=="new_col"]<-new_names
    
   }
   if (colnames(dat4[i])%in% n_list){
    
    new_col = (dat4[,i]+dat4[,i+1])/2
    
    new_names = paste0("AVG_", colnames(dat4[i]))

    dat4<-cbind(dat4,new_col)
    names(dat4)[names(dat4)=="new_col"]<-new_names
    
  }
  
}

predictor_list2<-c("BMR","PHDKGWBC","PHDCMHBC","SLPTIME","DIETRDI","DIETQ5","DIETQ8","SEX","AGEC","PHDCMWBC","EXLWMBC","EXLWVBC", "SYSTOL","DIASTOL","CVDMEDST","SMKSTAT")

avg<-dat4%>%select(contains("AVG_"))
predict2<-dat4%>%select(predictor_list2)
predict2
dat5<-cbind(predict2,avg)
dat5$EXLWMBC<-as.numeric(as.character(dat5$EXLWMBC)) # exerices time should be numeric 
dat5$EXLWVBC<-as.numeric(as.character(dat5$EXLWVBC)) # exerciese time should be numeric
str(dat5)

dat5c<-na.omit(dat5)
str(dat5c)
```



## dat3 (sleep, height, weight, -BMI, -biomarkers)

```{r}
#dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64, SMKSTAT==5)  # filter age and smoke status
dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64)  # filter age only

# dat<-dat%>% mutate(
#  rfm = ifelse(SEX==2, 76-(20*(PHDCMHBC/PHDCMWBC)),64-(20*(PHDCMHBC/PHDCMWBC)) )
# )
# 


var_list<-c("BMISC","PHDKGWBC","PHDCMHBC","SLPTIME","SEX","AGEC","DIABBC","HCHOLBC","HSUGBC","HYPBC","PHDCMWBC","EXLWMBC","EXLWVBC", "SYSTOL","DIASTOL","CVDMEDST","SMKSTAT") # add/remove variables that are interested
dat3<-dat %>% select (var_list) # select columns that we are interested
dat3$EXLWMBC<-as.numeric(as.character(dat$EXLWMBC)) # exerices time should be numeric 
dat3$EXLWVBC<-as.numeric(as.character(dat$EXLWVBC)) # exerciese time should be numeric
str(dat3) 
dat3c<-dat3%>%na.omit()
str(dat3c)
#table(dat3c$CVDMEDST)


# check AGE vs. CVDMEDST

a<-as.numeric(as.character(dat3$CVDMEDST))
b<-dat3$AGEC

plot(b,a)


hist(dat3$SYSTOL)

```

## dat2

```{r}
#dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64, SMKSTAT==5)  # filter age and smoke status
dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64)  # filter age only
var_list<-c("BMISC","SEX","AGEC","DIABBC","HCHOLBC","HSUGBC","HYPBC","PHDCMWBC","EXLWMBC","EXLWVBC", "SYSTOL","DIASTOL","TRIGRESB","CHOLRESB","LDLRESB","HBA1PREB","GLUCFREB","HDLCHREB","CVDMEDST","APOBRESB","SMKSTAT") # add/remove variables that are interested
dat2<-dat %>% select (var_list) # select columns that we are interested
dat2$EXLWMBC<-as.numeric(as.character(dat$EXLWMBC)) # exerices time should be numeric 
dat2$EXLWVBC<-as.numeric(as.character(dat$EXLWVBC)) # exerciese time should be numeric
str(dat2) # 7238 obs x 20 variables

```

### dat2c (NA removed)

```{r}
dat2c<-na.omit(dat2)
dat2c

```

# Models

## Model 51(CVDMEDST)

### [511] y=numerical CVD

#### [5111] Step selection logistic regression

```{r}
##########################
response<-c("CVDMEDST")
Y<-dat5 %>%mutate(
  CVD_cat = ifelse((CVDMEDST==4), 0, ifelse((CVDMEDST==3|CVDMEDST==2|CVDMEDST==1),1, NA)),
  CVD_num = as.numeric(as.character(CVDMEDST)),
  CVD_cat_num = as.numeric(ifelse((CVDMEDST==4), 0, ifelse((CVDMEDST==3|CVDMEDST==2|CVDMEDST==1),1, NA))),
)

X<-dat5 %>% select (!response)
y<-Y$CVD_cat_num # y is numerical

temp_dat<-cbind(y,X) # construct temp dataset for analysis
temp_dat<-na.omit(temp_dat)
str(temp_dat)
hist(temp_dat$y)
##########################

repeats<-10
y<-temp_dat$y
x<-temp_dat[,-1]
x

hist(y)


res1 <- cv_penLogistic(y,x,method="vanilla", repeats=repeats)
res2 <- cv_penLogistic(y,x,method="fowardAIC", repeats=repeats)
res3 <- cv_penLogistic(y,x,method="forwardBIC", repeats=repeats)
res4 <- cv_penLogistic(y,x,method="stepwiseAIC", repeats=repeats)
res5 <- cv_penLogistic(y,x,method="stepwiseBIC", repeats=repeats)
res8 <- cv_penLogistic(y,x,method="firth",repeats=repeats)

save(res1,res2,res3,res4,res5,res8, file="5111 results")



tab<-cbind(
  apply(res1[[1]],2,mean),
  apply(res2[[1]],2,mean),
  apply(res3[[1]],2,mean),
  apply(res4[[1]],2,mean),
  apply(res5[[1]],2,mean),
  apply(res8[[1]],2,mean)
)



colnames(tab) <- c("logistic",
                   "fwdAIC",
                   "fwdBIC",
                   "bwdAIC",
                   "bwdBIC",
                   #"ridge",
                   #"lasso",
                   "firth")


tab
boxplot(tab)

# extract coefficients of models
lcoef <- list(res1[[2]]$coef,
              res2[[2]]$coef,
              res3[[2]]$coef,
              res4[[2]]$coef,
              res5[[2]]$coef,
              #drop(coef(res6[[2]], res6[[2]]$lambda.1se )),
              #drop(coef(res7[[2]], res7[[2]]$lambda.1se )),
              res8[[2]]$coef
)

lcoef


```

#### [5112] Random forests

```{r}
###### dataset construction ######
response<-c("CVDMEDST")
Y<-dat5c %>% select (response) %>% mutate(
  CVD_cat = as.numeric(ifelse((CVDMEDST==4), 0, ifelse((CVDMEDST==3|CVDMEDST==2|CVDMEDST==1),1, NA))),
  CVD_num = as.numeric(as.character(CVDMEDST)),
  
)

str(Y)
X<-dat5c %>% select (!response)
y<-Y$CVD_cat # y is binary class
temp_dat<-cbind(y,X) # construct temp dataset for analysis
table(y)
str(temp_dat)
###### dataset construction ######


# feature selection 1 using rfe from caret package:
control<-rfeControl(
  functions = rfFuncs, # random forest
  method = 'cv',
  number = 10
)

temp_dat
results<-rfe(temp_dat[,-1], # x, predictors
             temp_dat[,1], # y
             sizes = c(1:5), # how many features should be kept
             rfeControl = control
        
)


results
(floor(ncol(temp_dat)-1)/3)

model_rfe <-randomForest(
  y~AGEC+DIASTOL+PHDCMWBC+PHDKGWBC+BMR,
  data=temp_dat,
  num.trees=500,
  min.node.size=5,
  mtry = 10,
  seed=2021,
  respect.unordered.factors='order',
)

table(as.numeric(model_rfe$predicted>0.5), temp_dat$y)
p1.5112.rfe<-vip::vip(model_rfe,num_features=5,bar=FALSE)

p1.5112.rfe
p2.5112.rfe<-partial(model_rfe, pred.var = c("PHDCMWBC","AGEC"), prob=TRUE, plot=TRUE)

p2.5112.rfe


## partial dependency for each predictor:
pics.5112<-list()

pics.5112[[1]]<-partial(model_rfe, pred.var = c("PHDCMWBC"), prob=TRUE, plot=TRUE)
pics.5112[[2]]<-partial(model_rfe, pred.var = c("AGEC"), prob=TRUE, plot=TRUE)
pics.5112[[3]]<-partial(model_rfe, pred.var = c("BMR"), prob=TRUE, plot=TRUE)
pics.5112[[4]]<-partial(model_rfe, pred.var = c("PHDKGWBC"), prob=TRUE, plot=TRUE)
pics.5112[[5]]<-partial(model_rfe, pred.var = c("DIASTOL"), prob=TRUE, plot=TRUE)

for (i in 1:5){
  print(pics.5112[[i]])  
}


# feature selection 2 using rfe from caret package:
control.2<-rfeControl(
  functions = treebagFuncs, # random forest
  method = 'cv',
  number = 10
)

temp_dat
results.2<-rfe(temp_dat[,-1], # x, predictors
             temp_dat[,1], # y
             sizes = c(1:5), # how many features should be kept
             rfeControl = control.2
        
)


results
(floor(ncol(temp_dat)-1)/3)

model_rfe <-randomForest(
  y~AGEC+DIASTOL+PHDCMWBC+PHDKGWBC+BMR,
  data=temp_dat,
  num.trees=500,
  min.node.size=5,
  mtry = 10,
  seed=2021,
  respect.unordered.factors='order',
)

table(as.numeric(model_rfe$predicted>0.5), temp_dat$y)
p1.5112.rfe<-vip::vip(model_rfe,num_features=5,bar=FALSE)

p1.5112.rfe
p2.5112.rfe<-partial(model_rfe, pred.var = c("PHDCMWBC","AGEC"), prob=TRUE, plot=TRUE)

p2.5112.rfe


## partial dependency for each predictor:
pics.5112<-list()

pics.5112[[1]]<-partial(model_rfe, pred.var = c("PHDCMWBC"), prob=TRUE, plot=TRUE)
pics.5112[[2]]<-partial(model_rfe, pred.var = c("AGEC"), prob=TRUE, plot=TRUE)
pics.5112[[3]]<-partial(model_rfe, pred.var = c("BMR"), prob=TRUE, plot=TRUE)
pics.5112[[4]]<-partial(model_rfe, pred.var = c("PHDKGWBC"), prob=TRUE, plot=TRUE)
pics.5112[[5]]<-partial(model_rfe, pred.var = c("DIASTOL"), prob=TRUE, plot=TRUE)

for (i in 1:5){
  print(pics.5112[[i]])  
}

```



## Model 35 (HYPBC)

Y: HYPBC (undersamping-\> \~300 class = 1)

```{r}
str(dat3c)
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")

Y<-dat3c%>% select (response) 
X<-dat3c%>% select (!response)


y<-Y %>% mutate(
  HYPBC = as.factor(ifelse((HYPBC==5), 0, ifelse((HYPBC==3|HYPBC==2|HYPBC==1),1, NA)))
)
Y
y<-y$HYPBC # y is binary (0,1)
x<-X
table(y)
str(x)
```

```{r}
temp_dat<-cbind(y,x)
temp_dat<-na.omit(temp_dat)
table(temp_dat$y)
a<-temp_dat[which(temp_dat$y==0),]
aa<-a[1:300,]
aa
b<-temp_dat[which(temp_dat$y==1),]
b
temp_dat<-rbind(aa,b)

str(dat3c)
#temp_dat <-smote(y ~., temp_dat, perc.over=600, perc.under=2)

str(temp_dat)
# K-fold CV
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)

# create some matrix to record results during k-fold cv
result_full<- matrix(NA, k_fold, 2) # record performance indicators
model_full=list() # record model
result_bmi<- matrix(NA, k_fold, 2)
model_bmi=list()
result_nobmi<- matrix(NA, k_fold, 2)
model_nobmi=list()


for (i in 1:k_fold){
  fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
  fold_train<-temp_dat[-folds[[i]],] # remaining is training set
  
  # glm
  model<-glm(y~., data=temp_dat, family=binomial)
  pred<-predict(model, fold_test, type="response") # numeric predicted values
  fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  matrix<-table(fold_predict, fold_test$y)# confusion matrix
  if (nrow(matrix<2)){
    matrix<-rbind(matrix,c(0,0))
  }
  
  
  matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
  
  matrix
  
  f1<-calculate_f1(matrix) # calculate f1 score
  auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  model_full[i]<-model
  result_full[i,1]<-f1
  result_full[i,2]<-auc
    
}

# make a table to exhibit all results from different models
k_th<-seq(1:(k_fold+1))
result<-data.frame(cbind(result_full))
colnames(result)<-c("Full F1","Full AUC")
avg_result=apply(result[,],2,mean, na.rm=TRUE) # avearge value of each 
final_result<-rbind(result,avg_result)
final_result<-cbind(k_th, final_result)
final_result[(k_fold+1),1]="Average"
final_result


summary(model)

confusionMatrix(matrix)
```

## Model 34 (HSUGBC)

-   Y HSUGBC, under sampling (only contains \~100 class 1)

```{r}
str(dat3c)
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")

Y<-dat3c%>% select (response) 
X<-dat3c%>% select (!response)


y<-Y %>% mutate(
  HSUGBC = as.factor(ifelse((HSUGBC==5), 0, ifelse((HSUGBC==3|HSUGBC==2|HSUGBC==1),1, NA)))
)
Y
y<-y$HSUGBC # y is binary (0,1)
x<-X
table(y)
str(x)
```

```{r}
temp_dat<-cbind(y,x)
temp_dat<-na.omit(temp_dat)
table(temp_dat$y)
a<-temp_dat[which(temp_dat$y==0),]
aa<-a[1:100,]
aa
b<-temp_dat[which(temp_dat$y==1),]
b
temp_dat<-rbind(aa,b)

str(dat3c)
#temp_dat <-smote(y ~., temp_dat, perc.over=600, perc.under=2)

str(temp_dat)
# K-fold CV
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)

# create some matrix to record results during k-fold cv
result_full<- matrix(NA, k_fold, 2) # record performance indicators
model_full=list() # record model
result_bmi<- matrix(NA, k_fold, 2)
model_bmi=list()
result_nobmi<- matrix(NA, k_fold, 2)
model_nobmi=list()


for (i in 1:k_fold){
  fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
  fold_train<-temp_dat[-folds[[i]],] # remaining is training set
  
  # glm
  model<-glm(y~., data=temp_dat, family=binomial)
  pred<-predict(model, fold_test, type="response") # numeric predicted values
  fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  matrix<-table(fold_predict, fold_test$y)# confusion matrix
  if (nrow(matrix<2)){
    matrix<-rbind(matrix,c(0,0))
  }
  
  
  matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
  
  matrix
  
  f1<-calculate_f1(matrix) # calculate f1 score
  auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  model_full[i]<-model
  result_full[i,1]<-f1
  result_full[i,2]<-auc
    
}

# make a table to exhibit all results from different models
k_th<-seq(1:(k_fold+1))
result<-data.frame(cbind(result_full))
colnames(result)<-c("Full F1","Full AUC")
avg_result=apply(result[,],2,mean, na.rm=TRUE) # avearge value of each 
final_result<-rbind(result,avg_result)
final_result<-cbind(k_th, final_result)
final_result[(k_fold+1),1]="Average"
final_result


summary(model)

confusionMatrix(matrix)
```

## Model 33 (HCHOLBC)

Y: HCHOLBC, binary X:

undersampling y=0 y=0 -\> \~300 y=1 -\> \~280

```{r}
str(dat3c)
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")

Y<-dat3c%>% select (response) 
X<-dat3c%>% select (!response)


y<-Y %>% mutate(
  HCHOLBC = as.factor(ifelse((HCHOLBC==5), 0, ifelse((HCHOLBC==3|HCHOLBC==2|HCHOLBC==1),1, NA)))
)
Y
y<-y$HCHOLBC # y is binary (0,1)
table(y)
x<-X

str(x)
```

### Logistic regression

```{r}
temp_dat<-cbind(y,x)
temp_dat<-na.omit(temp_dat)
table(temp_dat$y)
a<-temp_dat[which(temp_dat$y==0),]
aa<-a[1:300,]
aa
b<-temp_dat[which(temp_dat$y==1),]
b
temp_dat<-rbind(aa,b)

str(dat3c)
#temp_dat <-smote(y ~., temp_dat, perc.over=600, perc.under=2)

str(temp_dat)
# K-fold CV
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)

# create some matrix to record results during k-fold cv
result_full<- matrix(NA, k_fold, 2) # record performance indicators
model_full=list() # record model
result_bmi<- matrix(NA, k_fold, 2)
model_bmi=list()
result_nobmi<- matrix(NA, k_fold, 2)
model_nobmi=list()


for (i in 1:k_fold){
  fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
  fold_train<-temp_dat[-folds[[i]],] # remaining is training set
  
  # glm
  model<-glm(y~., data=temp_dat, family=binomial)
  pred<-predict(model, fold_test, type="response") # numeric predicted values
  fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  matrix<-table(fold_predict, fold_test$y)# confusion matrix
  if (nrow(matrix<2)){
    matrix<-rbind(matrix,c(0,0))
  }
  
  
  matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
  
  matrix
  
  f1<-calculate_f1(matrix) # calculate f1 score
  auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  model_full[i]<-model
  result_full[i,1]<-f1
  result_full[i,2]<-auc
    
}

# make a table to exhibit all results from different models
k_th<-seq(1:(k_fold+1))
result<-data.frame(cbind(result_full))
colnames(result)<-c("Full F1","Full AUC")
avg_result=apply(result[,],2,mean, na.rm=TRUE) # avearge value of each 
final_result<-rbind(result,avg_result)
final_result<-cbind(k_th, final_result)
final_result[(k_fold+1),1]="Average"
final_result


summary(model)

confusionMatrix(matrix)
```

## Model 32 (HCHOLBC)

-   Y:HCHOLBC
-   X: new predictors

```{r}
str(dat3c)
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")

Y<-dat3c%>% select (response) 
X<-dat3c%>% select (!response)


y<-Y %>% mutate(
  HCHOLBC = ifelse((HCHOLBC==5), 0, ifelse((HCHOLBC==3|HCHOLBC==2|HCHOLBC==1),1, NA))
)
Y
y<-y$HCHOLBC # y is binary (0,1)
x<-X

str(x)
```

```{r}
temp_dat<-cbind(y,x)
temp_dat<-na.omit(temp_dat)
temp_dat

str(temp_dat)
# K-fold CV
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)

# create some matrix to record results during k-fold cv
result_full<- matrix(NA, k_fold, 2) # record performance indicators
model_full=list() # record model
result_bmi<- matrix(NA, k_fold, 2)
model_bmi=list()
result_nobmi<- matrix(NA, k_fold, 2)
model_nobmi=list()


for (i in 1:k_fold){
  fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
  fold_train<-temp_dat[-folds[[i]],] # remaining is training set
  
  # glm
  model<-glm(y~., data=temp_dat, family=binomial)
  pred<-predict(model, fold_test, type="response") # numeric predicted values
  fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  matrix<-table(fold_predict, fold_test$y)# confusion matrix
  if (nrow(matrix<2)){
    matrix<-rbind(matrix,c(0,0))
  }
  
  
  matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
  
  matrix
  
  f1<-calculate_f1(matrix) # calculate f1 score
  auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  model_full[i]<-model
  result_full[i,1]<-f1
  result_full[i,2]<-auc
    
}

# make a table to exhibit all results from different models
k_th<-seq(1:(k_fold+1))
result<-data.frame(cbind(result_full))
colnames(result)<-c("Full F1","Full AUC")
avg_result=apply(result[,],2,mean, na.rm=TRUE) # avearge value of each 
final_result<-rbind(result,avg_result)
final_result<-cbind(k_th, final_result)
final_result[(k_fold+1),1]="Average"
final_result


summary(model)

confusionMatrix(matrix)
```

## Model 31 (CVDMEDST)

-   Y: CVDMEDST
-   x: mixture of numerical and categorical values

### [311] y=CVD_cat, treat y as binary categorical

#### [3111] Logistic regression

```{r}
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")
Y<-dat3c %>% select (response) %>% mutate(
  CVD_cat = ifelse((CVDMEDST==4), 0, ifelse((CVDMEDST==3|CVDMEDST==2|CVDMEDST==1),1, NA)),
  CVD_num = as.numeric(as.character(CVDMEDST)),
  
)

X<-dat3c %>% select (!response)
y<-Y$CVD_cat # y is binary class
temp_dat<-cbind(y,X) # construct temp dataset for analysis
table(y)
str(temp_dat)

model<-glm(y~., data=temp_dat)
summary(model)

```

#### [3112] Classification Tree

```{r}
###### dataset construction ######
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")
Y<-dat3c %>% select (response) %>% mutate(
  CVD_cat = as.factor(as.character(ifelse((CVDMEDST==4), 0, ifelse((CVDMEDST==3|CVDMEDST==2|CVDMEDST==1),1, NA)))),
  CVD_num = as.numeric(as.character(CVDMEDST)),
  
)

X<-dat3c %>% select (!response)
y<-Y$CVD_cat # y is binary class
temp_dat<-cbind(y,X) # construct temp dataset for analysis
table(y)
str(temp_dat)
###### dataset construction ######

# classification tree
model<-rpart(y~., data=temp_dat,parms = list(split="information"))

model
# set pc
rpart_cont <- rpart.control(cp=0)
# model
model <- rpart(y~.,data=temp_dat, 
              control = rpart_cont,
              parms = list(split="information"))
# make a table for cp comparison
df <- data.frame(model$cptable)
ft <- flextable(df) %>%
  autofit()
ft
cpvals <- model$cptable[,1]
nsplit <- model$cptable[,2]
xerror <- model$cptable[,4]
# find the index of the min of xerror in cp table
minpos <- min(seq_along(xerror)[xerror == min(xerror)])     
minpos
# prune the tree 
model_prune<-prune.rpart(model, cp =0.009753)
model_prune
rpart.plot(model_prune)
```

#### X [3113] Random Forest (y={0,1} factor)

The following code gives opposite prediction, probably due to the coding of Y

```{r}
###### dataset construction ######
response <- c("DIABBC", "HCHOLBC", "HSUGBC", "HYPBC", "CVDMEDST")
Y <- dat3c %>% select (response) %>% mutate(
  CVD_cat = as.factor(as.character(ifelse((CVDMEDST ==4), 0, ifelse((CVDMEDST == 3 | CVDMEDST == 2 | CVDMEDST == 1), 1, NA)))),
  CVD_num = as.numeric(as.character(CVDMEDST)),
)

X<-dat3c %>% select (!response)
y<-Y$CVD_cat # y is binary class
temp_dat<-cbind(y,X) # construct temp dataset for analysis
table(y)
str(temp_dat)
###### dataset construction ######


# Random forest
model<-randomForest(
  y~.,
  data=temp_dat,
  num.trees = 500,
  min.node.size = 5,
  mtry=floor((ncol(temp_dat)-1)/3),
  seed=2021,
  respect.unordered.factors = "order",
)

model
table(model$y)

library(vip)
p1.3113<-vip::vip(model,num_features=10,bar=FALSE) # variable importance
p1.3113
library(pdp)
p2.3113<-partial(model, pred.var = c("PHDCMHBC","AGEC"), prob=TRUE, plot=TRUE)
p2.3113
```

#### [3114] Random forest (y={0,1} but numeric)

fixed issues in [3113] by treating y as numerical {0,1} values rather than factor


```{r}
###### dataset construction ######
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")
Y<-dat3c %>% select (response) %>% mutate(
  CVD_cat = as.numeric(ifelse((CVDMEDST==4), 0, ifelse((CVDMEDST==3|CVDMEDST==2|CVDMEDST==1),1, NA))),
  CVD_num = as.numeric(as.character(CVDMEDST)),
  
)

str(Y)
X<-dat3c %>% select (!response)
y<-Y$CVD_cat # y is binary class
temp_dat<-cbind(y,X) # construct temp dataset for analysis
table(y)
str(temp_dat)
###### dataset construction ######


# feature selection using rfe from caret package:
control<-rfeControl(
  functions = rfFuncs, # random forest
  method = 'cv',
  number = 10
)

temp_dat
results<-rfe(temp_dat[,-1], # x, predictors
             temp_dat[,1], # y
             sizes = c(1:5), # how many features should be kept
             rfeControl = control
        
)


results


model_rfe <-randomForest(
  y~AGEC+BMISC+PHDCMWBC+PHDKGWBC+SEX,
  data=temp_dat,
  num.trees=500,
  min.node.size=5,
  mtry=2,
  seed=2021,
  respect.unordered.factors='order',
)

table(as.numeric(model_rfe$predicted>0.5), temp_dat$y)
p1.3114.rfe<-vip::vip(model_rfe,num_features=5,bar=FALSE)

p1.3114.rfe
p2.3114.rfe<-partial(model_rfe, pred.var = c("PHDCMWBC","AGEC"), prob=TRUE, plot=TRUE)

p2.3114.rfe


# Random forest
model<-randomForest(
  y~.,
  data=temp_dat,
  num.trees = 500,
  min.node.size = 5,
  mtry=floor((ncol(temp_dat)-1)/3),
  seed=2021,
  respect.unordered.factors = "order",
)

table(as.numeric(model$predicted>0.5), temp_dat$y)
table(model$y)


library(vip)
p1.3114<-vip::vip(model,num_features=10,bar=FALSE) # variable importance
p1.3114
library(pdp)
p2.3114<-partial(model, pred.var = c("PHDCMHBC","AGEC"), prob=TRUE, plot=TRUE)
p2.3114

pics<-list()

x<-temp_dat[,-1]
x
names<-colnames(x)
names
for (i in 1:(ncol(x))){
  p<-partial(model, pred.var = names[i], prob=TRUE, plot=TRUE)
  pics[[i]]  <-p
}


for (i in 1:ncol(x)){
  print(pics[[i]])  
}



```

#### [3115] Nerual network (y is numeric)

```{r}
###### dataset construction ######
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")
Y<-dat3c %>% select (response) %>% mutate(
  CVD_cat = as.numeric(ifelse((CVDMEDST==4), 0, ifelse((CVDMEDST==3|CVDMEDST==2|CVDMEDST==1),1, NA))),
  CVD_num = as.numeric(as.character(CVDMEDST)),
  
)
str(Y)
X<-dat3c %>% select (!response)
y<-Y$CVD_cat # y is binary class

# normalization
X_std<-X%>% mutate_if(
  is.numeric, unlist(mystd)
)


temp_dat_std<-cbind(y,X_std) # construct temp dataset for analysis
temp_dat_std<-droplevels(temp_dat_std)

temp_dat_std_1hot<-one_hot(as.data.table(temp_dat_std))






# set up k value for k-fold cross validation 
k_fold=2
# set number of hidden layers
hid_layer=3
# create k folds
folds<-createFolds(y=temp_dat_std_1hot$y, k=k_fold)

test_error_RMSE<-c() 

for (i in 1:k_fold){
  fold_test<-temp_dat_std_1hot[folds[[i]],] # select folds[[i]] as test test
  fold_train<-temp_dat_std_1hot[-folds[[i]],] # remaining is training set
  network<-neuralnet(y~., data=fold_train, hidden =hid_layer)
  fold_predict<-predict(network, fold_test, type="response")
  test_error_RMSE[i]<-RMSE(fold_predict, fold_test$y) # calculate and record RMSE
}


network$result.matrix

avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE


net_list=list()

net_list[[1]]=network

net_list[[1]]

plot(network)

nn.results <- compute(network, fold_test)
results <- data.frame(actual = fold_test$y,compute=nn.results$net.result, prediction = as.numeric(nn.results$net.result>0.5))

results
table(results$prediction, results$actual)

hist(fold_predict)

###### dataset construction ######

```




### y=CVD_numeric, treat original level as continuous numbers

#### Linear regression

```{r}
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")
Y<-dat3c %>% select (response) %>% mutate(
  CVD_cat = as.factor(ifelse((CVDMEDST==4), "negative", ifelse((CVDMEDST==3|CVDMEDST==2|CVDMEDST==1),"positive", NA))),
  CVD_num = as.numeric(as.character(CVDMEDST)),
  
)

X<-dat3c %>% select (!response)
y<-Y$CVD_num # y is multiple class
temp_dat<-cbind(y,X) # construct temp dataset for analysis
table(y)
str(temp_dat)
model<-lm(y~., data=temp_dat)
summary(model)
```

#### Random Forest

```{r}
###### dataset construction ######
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")
Y<-dat3c %>% select (response) %>% mutate(
  CVD_cat = as.factor(as.character(ifelse((CVDMEDST==4), 0, ifelse((CVDMEDST==3|CVDMEDST==2|CVDMEDST==1),1, NA)))),
  CVD_num = as.numeric(as.character(CVDMEDST)),
  
)

table(Y$CVD_num, Y$CVDMEDST)

X<-dat3c %>% select (!response)
y<-Y$CVD_num # y is binary class
temp_dat<-cbind(y,X) # construct temp dataset for analysis
table(y)
str(temp_dat)
###### dataset construction ######


# Random forest
model<-randomForest(
  y~.,
  data=temp_dat,
  num.trees = 500,
  min.node.size = 5,
  mtry=floor((ncol(temp_dat)-1)/3),
  seed=2021,
  respect.unordered.factors = "order",
)

model
table(model$y)

library(vip)
p1<-vip::vip(model,num_features=10,bar=FALSE) # variable importance
p1
library(pdp)
p2<-partial(model, pred.var = c("PHDCMHBC","AGEC"), prob=TRUE, plot=TRUE)
p2
```

Above partial dependency plot shows that:

-   higher ages -\> lower y scores (y=1 is the worst, y=4 never have the diease)
-   higher waist circumference -\> higher score

### y=CVDMEDST, multiple classes

#### Logsitic regression

```{r}
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")
Y<-dat3c %>% select (response) %>% mutate(
  CVD_cat = as.factor(ifelse((CVDMEDST==4), "negative", ifelse((CVDMEDST==3|CVDMEDST==2|CVDMEDST==1),"positive", NA))),
  CVD_num = as.numeric(ifelse((CVDMEDST==4), 1, ifelse((CVDMEDST==3|CVDMEDST==2|CVDMEDST==1),0, NA))),
  
)
X<-dat3c %>% select (!response)
y<-Y$CVDMEDST # y is multiple class
temp_dat<-cbind(y,X) # construct temp dataset for analysis
# the model use CVDMEDST=1 as reference level (basic)
# e.g., log(P(CVD=2)/P(CVD=1)) = b0+b1x1+b2x2+...
# e.g., log(P(CVD=3)/P(CVD=1)) = b0+b1x1+b2x2+...
model<-multinom(y~., data=temp_dat)
summary(model)$coefficients # model coefficients
# two tailed z test:
z<-summary(model)$coefficients/summary(model)$standard.errors
p<-(1-pnorm(abs(z),0,1))*2 # p value of coefficients
p # p < 0.05 is significant 
```

```{r logisitic regression}


# 1. Y=CVDMEDST, multiple classes
table(temp_dat$CVDMEDST)
pred<-predict(model, temp_dat)
table(pred)

temp_dat<-cbind(y1,y2,y3,x)
temp_dat<-na.omit(temp_dat)

str(temp_dat)

# compare different methods
glm(y1~.-y2-y3, data=temp_dat, family = multinomial )

model1<-multinom(y1~.-y2-y3, data=temp_dat)

summary(model1)
# 2-tail z test



```

### Logistic regression (y= binary)

```{r logisitic regression}
y<-Y$CVD_cat
temp_dat<-cbind(y,x)
temp_dat<-na.omit(temp_dat)

str(temp_dat)
# K-fold CV
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)

# create some matrix to record results during k-fold cv
result_full<- matrix(NA, k_fold, 2) # record performance indicators
model_full=list() # record model
result_rfm<- matrix(NA, k_fold, 2)
model_rfm=list()
result_nobmi<- matrix(NA, k_fold, 2)
model_nobmi=list()



for (i in 1:k_fold){
  fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
  fold_train<-temp_dat[-folds[[i]],] # remaining is training set
  
  n<-nrow(fold_train)  
  # glm setep
  full_model <-glm(y~., data=temp_dat, family=binomial)
  null_model <-glm(y~1, data=temp_dat, family=binomial)
  model <- step(full_model,k=log(n),trace=0)
  pred<-predict(model, fold_test, type="response") # numeric predicted values
  fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  matrix<-table(fold_predict, fold_test$y)# confusion matrix
  if (nrow(matrix<2)){
    matrix<-rbind(matrix,c(0,0))
  }
  
  
  matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
  f1<-calculate_f1(matrix) # calculate f1 score
  auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  model_full[i]<-model
  result_full[i,1]<-f1
  result_full[i,2]<-auc
  
  
  # # glm
  # full_model <-glm(y~., data=temp_dat, family=binomial)
  # null_model <-glm(y~1, data=temp_dat, family=binomial)
  # model<-glm(y~., data=temp_dat, family=binomial)
  # pred<-predict(model, fold_test, type="response") # numeric predicted values
  # fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  # matrix<-table(fold_predict, fold_test$y)# confusion matrix
  # if (nrow(matrix<2)){
  #   matrix<-rbind(matrix,c(0,0))
  # }
  # 
  # 
  # matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
  # f1<-calculate_f1(matrix) # calculate f1 score
  # auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  # model_full[i]<-model
  # result_full[i,1]<-f1
  # result_full[i,2]<-auc
  
  # rfm
  # model<-glm(y~rfm, data=temp_dat, family=binomial)
  # pred<-predict(model, fold_test, type="response") # numeric predicted values
  # fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  # matrix<-table(fold_predict, fold_test$y)# confusion matrix
  # if (nrow(matrix<2)){
  #   matrix<-rbind(matrix,c(0,0))
  # }
  # 
  # 
  # matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
  # f1<-calculate_f1(matrix) # calculate f1 score
  # auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  # model_rfm[i]<-model
  # result_rfm[i,1]<-f1
  # result_rfm[i,2]<-auc
  
}

# make a table to exhibit all results from different models
k_th<-seq(1:(k_fold+1))
result<-data.frame(cbind(result_full, result_rfm))
colnames(result)<-c("Full F1","Full AUC","Full F1","Full AUC")
avg_result=apply(result[,],2,mean, na.rm=TRUE) # avearge value of each 
final_result<-rbind(result,avg_result)
final_result<-cbind(k_th, final_result)
final_result[(k_fold+1),1]="Average"
final_result

summary(model)
confusionMatrix(matrix)


```

### Classfication Tree

```{r}
y<-as.factor(y)
temp_dat<-cbind(y,x)
# temp_dat<-na.omit(temp_dat)
str(temp_dat)

# classification tree
model<-rpart(y~., data=temp_dat,parms = list(split="information"))

model
# set pc
rpart_cont <- rpart.control(cp=0)
# model
model <- rpart(y~.,data=temp_dat, 
              control = rpart_cont,
              parms = list(split="information"))

df <- data.frame(model$cptable)
ft <- flextable(df) %>%
  autofit()
ft
cpvals <- model$cptable[,1]
nsplit <- model$cptable[,2]
xerror <- model$cptable[,4]
# find the index of the min of xerror in cp table
minpos <- min(seq_along(xerror)[xerror == min(xerror)])     
minpos

model_prune<-prune.rpart(model, cp =0.0017)
model_prune
model
rpart.plot(model_prune)
```

### Random Forest

```{r}
# Random forest
# CVDMEDST

model<-randomForest(
  y~.,
  data=temp_dat,
  num.trees = 500,
  min.node.size = 5,
  mtry=floor((ncol(temp_dat)-1)/3),
  seed=2021,
  respect.unordered.factors = "order",
)

model
table(model$y)

library(vip)


p1<-vip::vip(model,num_features=10,bar=FALSE)

p1

library(pdp)
partial(model, pred.var = c("PHDCMHBC","AGEC"), prob=TRUE, plot=TRUE)
partial(model, pred.var = c("AGEC"), prob=TRUE, plot=TRUE)
#partial(model, pred.var = c("BMISC"), prob=TRUE, plot=TRUE)

#%>% plotPartial()

#plotPartial(model)

```

```{r}
# Random forest
model<-randomForest(
  y~BMISC,
  data=temp_dat,
  num.trees = 500,
  min.node.size = 5,
  mtry=floor((ncol(temp_dat)-1)/3),
  seed=2021,
  respect.unordered.factors = "order",
)

model
table(model$y)

library(vip)


p1<-vip::vip(model,num_features=10,bar=FALSE)

p1

library(pdp)
partial(model, pred.var = c("BMISC"), prob=TRUE, plot=TRUE)
partial(model, pred.var = c("AGEC"), prob=TRUE, plot=TRUE)
#partial(model, pred.var = c("BMISC"), prob=TRUE, plot=TRUE)

#%>% plotPartial()

#plotPartial(model)

```

## Model 21

-   Y: HCHOLBC = 1,2,3 -\> Y=1; HCHOLBC = 5 -\>Y=0
-   X: mixture of numerical and recoded categorical

```{r construct x and y}
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")

Y<-dat2c%>% select (response) 
X<-dat2c%>% select (!response)

str(Y)
str(X)

y<-Y %>% mutate(
  HCHOLBC = as.factor(ifelse((HCHOLBC==5), 0, ifelse(HCHOLBC==3|HCHOLBC==2|HCHOLBC==1,1, NA)))
)
y<-y$HCHOLBC # y is binary (0,1)

x<-X%>% mutate(
  TRIGRESB=as.factor(ifelse((TRIGRESB==1|TRIGRESB==2|TRIGRESB==3),0,ifelse((TRIGRESB==4|TRIGRESB==5),1,ifelse(TRIGRESB==97|TRIGRESB==98,NA,2)))),
  
  CHOLRESB=as.factor(ifelse((CHOLRESB==1|CHOLRESB==2|CHOLRESB==3),0,ifelse((CHOLRESB==4|CHOLRESB==5|CHOLRESB==6),1,ifelse(CHOLRESB==97|CHOLRESB==98,NA,2)))),
  
  LDLRESB=as.factor(ifelse((LDLRESB==1|LDLRESB==2|LDLRESB==3|LDLRESB==4),0,ifelse((LDLRESB==5|LDLRESB==6|LDLRESB==7),1,ifelse(LDLRESB==97|LDLRESB==98,NA,2)))),
  
  GLUCFREB=as.factor(ifelse((GLUCFREB==4|GLUCFREB==5),0,ifelse((GLUCFREB==6|GLUCFREB==7),1,ifelse(GLUCFREB==97|GLUCFREB==98,NA,2)))),
  
  HDLCHREB=as.factor(ifelse((HDLCHREB==7|HDLCHREB==8),NA,ifelse((HDLCHREB==5|HDLCHREB==6),0,ifelse(HDLCHREB==1,2,1)))),
  
  APOBRESB=as.factor(ifelse((APOBRESB==1|APOBRESB==2|APOBRESB==3|APOBRESB==4),0,ifelse((APOBRESB==5),1,ifelse(APOBRESB==97|APOBRESB==98,NA,2)))),
  
  HBA1PREB=as.factor(ifelse((HBA1PREB==1|HBA1PREB==2),0,ifelse((HBA1PREB==3|HBA1PREB==4),1,ifelse(HBA1PREB==7|HBA1PREB==8,NA,2)))),
)

x<-x[,-10] # drop cholersb, do not use cholresb to predict high cholersterol 
str(x)

```

### Logistic regression

```{r}
repeats<-1
res1 <- cv_penLogistic(y,x,method="vanilla", repeats=repeats)
res2 <- cv_penLogistic(y,x,method="fowardAIC", repeats=repeats)
res3 <- cv_penLogistic(y,x,method="forwardBIC", repeats=repeats)
res4 <- cv_penLogistic(y,x,method="stepwiseAIC", repeats=repeats)
res5 <- cv_penLogistic(y,x,method="stepwiseBIC", repeats=repeats)
#res6 <- cv_penLogistic(y,xx,method="ridge", repeats=repeats)
#res7 <- cv_penLogistic(y,xx,method="lasso", repeats=repeats)
res8 <- cv_penLogistic(y,x,method="firth",repeats=repeats)
```

```{r}
tab <- cbind(
  apply(res1[[1]],2,mean),
  apply(res2[[1]],2,mean),
  apply(res3[[1]],2,mean),
  apply(res4[[1]],2,mean),
  apply(res5[[1]],2,mean),
  #apply(res6[[1]],2,mean),
  #apply(res7[[1]],2,mean),
  apply(res8[[1]],2,mean))

colnames(tab) <- c("logistic",
                   "fwdAIC",
                   "fwdBIC",
                   "bwdAIC",
                   "bwdBIC",
                   #"ridge",
                   #"lasso",
                   "firth")

boxplot(tab)

# extract coefficients of models
lcoef <- list(res1[[2]]$coef,
              res2[[2]]$coef,
              res3[[2]]$coef,
              res4[[2]]$coef,
              res5[[2]]$coef,
              #drop(coef(res6[[2]], res6[[2]]$lambda.1se )),
              #drop(coef(res7[[2]], res7[[2]]$lambda.1se )),
              res8[[2]]$coef
)

varnames <- unique(unlist(map(lcoef,names)))
tab = matrix(0, nrow = length(lcoef), ncol = length(varnames))
colnames(tab) = varnames
for (i in 1:length(lcoef)) 
  tab[i, names(lcoef[[i]])] = lcoef[[i]]

rownames(tab) <- paste("model",1:length(lcoef),sep="")
kable(t(tab))

```

Above codes show the coefficients of BMISC of many models are close to 0, suggesting BMI may not be a good indicator for predicting high cholesterol. On the other hand, SEX, AGE, PHDCMWBC, DIASTOL, TRIGRESB, LDLRESB, HBA1PREB, GLUCFREB, HDLCHREB, APOBRESB are important predictors for predicting high cholesterol.

## Model 22

-   Y: 0-1 HCHOLOBC
-   X: numerical and non-recoded categorical variables, do not drop anything further

```{r}
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")

Y<-dat2c%>% select (response) 
X<-dat2c%>% select (!response)

str(Y)
str(X)
Y
y<-Y %>% mutate(
  HCHOLBC = ifelse((HCHOLBC==5), 0, ifelse(HCHOLBC==3|HCHOLBC==2|HCHOLBC==1,1, NA))
)
y<-y$HCHOLBC # y is binary (0,1)
x<-X
str(x)
```

### Logistic regression of various models

```{r logisitic regression}
temp_dat<-cbind(y,x)
temp_dat
# K-fold CV
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)

# create some matrix to record results during k-fold cv
result_full<- matrix(NA, k_fold, 2) # record performance indicators
model_full=list() # record model
result_bmi<- matrix(NA, k_fold, 2)
model_bmi=list()
result_nobmi<- matrix(NA, k_fold, 2)
model_nobmi=list()


for (i in 1:k_fold){
  fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
  fold_train<-temp_dat[-folds[[i]],] # remaining is training set
  
  # full model
  model<-glm(y~., data=temp_dat, family=binomial)
  pred<-predict(model, fold_test, type="response") # numeric predicted values
  fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  matrix<-table(fold_predict, fold_test$y)# confusion matrix
  if (nrow(matrix<2)){
    matrix<-rbind(matrix,c(0,0))
  }
  matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
  f1<-calculate_f1(matrix) # calculate f1 score
  auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  model_full[i]<-model
  result_full[i,1]<-f1
  result_full[i,2]<-auc
  
  # bmi only model
  model<-glm(y~BMISC, data=temp_dat, family=binomial)
  pred<-predict(model, fold_test, type="response") # numeric predicted values
  fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  matrix<-table(fold_predict, fold_test$y) # confusion matrix, note the consequence is reversed such that class 1 is positive
  if (nrow(matrix<2)){
    matrix<-rbind(matrix,c(0,0))
  }
  matrix<-matrix[2:1,2:1]
  f1<-calculate_f1(matrix) # calculate f1 score
  auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  
  model_bmi[i]<-model
  result_bmi[i,1]<-f1
  result_bmi[i,2]<-auc
  
  # no bmi model
  model<-glm(y~.-BMISC, data=temp_dat, family=binomial)
  pred<-predict(model, fold_test, type="response") # numeric predicted values
  fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  matrix<-table(fold_predict, fold_test$y) # confusion matrix, note the consequence is reversed such that class 1 is positive
  if (nrow(matrix<2)){
    matrix<-rbind(matrix,c(0,0))
  }
  matrix<-matrix[2:1,2:1]
  f1<-calculate_f1(matrix) # calculate f1 score
  auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  model_nobmi[i]<-model
  result_nobmi[i,1]<-f1
  result_nobmi[i,2]<-auc
    
}

# make a table to exhibit all results from different models
k_th<-seq(1:(k_fold+1))
result<-data.frame(cbind(result_full, result_bmi, result_nobmi))
colnames(result)<-c("Full F1","Full AUC", "BMI F1", "BMI AUC","NoBMI F1","NoBMI AUC")
avg_result=apply(result[,],2,mean, na.rm=TRUE) # avearge value of each 
final_result<-rbind(result,avg_result)
final_result<-cbind(k_th, final_result)
final_result[(k_fold+1),1]="Average"
final_result
```

### Random Forest

```{r}
temp_dat<-cbind(y,x)
temp_dat$y<-as.factor(as.character(temp_dat$y))
str(temp_dat)
dat2c$CV

model<-ranger(HCHOLBC~.-HSUGBC-HYPBC-DIABBC-CVDMEDST, 
       data = dat2c,
       mtry = (floor(ncol(temp_dat)-1)/3),
       respect.unordered.factors = "order",
       seed = 2021,
)


summary(model)
```

# Justify

## BMI vs. High cholesterol

```{r}
Y # whether has high cholesterol 
X # a set of predictors
temp_dat<-cbind(Y,X)
temp_dat<-temp_dat %>% mutate(
  HCHOLBC = ifelse(HCHOLBC==5, 0, 1) # encode Y as binary variable
)

# example 
model<-glm(HCHOLBC~., data = temp_dat)
y_pred<-predict(model, temp_dat, type="response")
hist(y_pred)
result<-cbind(y_pred,temp_dat)

# why this is better than BMI: 
hc <- result[which(result$HCHOLBC==1),] # people who has high cholesterol 
nhc <- result[which(result$HCHOLBC==0),]
  
hist(hc$BMISC)
hist(nhc$BMISC)

mean(result[which(result$HCHOLBC==1),"BMISC"]) # mean BMI = 29.4844 for people who has high cholesterol 
mean(result[which(result$HCHOLBC==0),"BMISC"]) # mean BMI = 27.1008 for people who does not have high cholesterol
d_BMI <- result[which(result$HCHOLBC==1),"BMISC"]-result[which(result$HCHOLBC==0),"BMISC"]

t.test(d_BMI)

mean(result[which(result$HCHOLBC==1),"y_pred"]) # mean y_pred = 0.4379, for people who has high cholesterol 
mean(result[which(result$HCHOLBC==0),"y_pred"]) # mean y_pred =0.0845, for people who does not have high cholesterol

# two sample t-test or other tests and show p value of (y_pred) < p value of (BMISC):
wilcox.test(result[which(result$HCHOLBC==1),"y_pred"],result[which(result$HCHOLBC==0),"y_pred"])
wilcox.test(result[which(result$HCHOLBC==1),"BMISC"], result[which(result$HCHOLBC==0),"BMISC"])
t.test(result[which(result$HCHOLBC==1),"BMISC"], result[which(result$HCHOLBC==0),"BMISC"])
t.test(result[which(result$HCHOLBC==1),"y_pred"],result[which(result$HCHOLBC==0),"y_pred"])

# ROC plot

library(pROC)
result
p1<-roc(result$HCHOLBC, result$BMISC) # BMI ROC
p2<-roc(result$HCHOLBC, result$y_pred) # Y-pred ROC
plot(p1, col='blue')
plot.roc(p2, add=TRUE, col='red')
auc(p1) # 0.6274
auc(p2) # 0.8765

```

```{r bmi cho}
bmi_high <- dat2%>% filter(dat2$BMISC >= 30)
nrow(bmi_high) # 1658 people high bmi
bmi_low <- dat2%>% filter(dat2$BMISC < 30)
nrow(bmi_low) # 4503 people low bmi
table(bmi_high$HCHOLBC)
table(bmi_low$HCHOLBC)
```

The claim is: BMI along is not a good indicator to diagnose high cholesterol

Based on BMI, we can see that

-   8.57% of low BMI people have high cholesterol (ideally, the majority of them should NOT have high cholesterol, since they are not in obesity according to BMI)
-   15.86% of high BMI people have high cholesterol (ideally, the majority of them should have, since they ARE in obesity according to bmi)

To beat BMI,

option 1

```{r}

str(dat2)

x_regroup <- dat2 %>% mutate(
   

  
   Tri_score=as.factor(ifelse((TRIGRESB==1|TRIGRESB==2|TRIGRESB==3),0,ifelse((TRIGRESB==4|TRIGRESB==5),1,ifelse(TRIGRESB==97|TRIGRESB==98,NA,2)))),
  
   Chol_score=as.factor(ifelse((CHOLRESB==1|CHOLRESB==2|CHOLRESB==3),0,ifelse((CHOLRESB==4|CHOLRESB==5|CHOLRESB==6),1,ifelse(CHOLRESB==97|CHOLRESB==98,NA,2)))),
  
  LDL_score=as.factor(ifelse((LDLRESB==1|LDLRESB==2|LDLRESB==3|LDLRESB==4),0,ifelse((LDLRESB==5|LDLRESB==6|LDLRESB==7),1,ifelse(LDLRESB==97|LDLRESB==98,NA,2)))),
  
  Glu_score=as.factor(ifelse((GLUCFREB==4|GLUCFREB==5),0,ifelse((GLUCFREB==6|GLUCFREB==7),1,ifelse(GLUCFREB==97|GLUCFREB==98,NA,2)))),
  
  HDL_score=as.factor(ifelse((HDLCHREB==7|HDLCHREB==8),NA,ifelse((HDLCHREB==5|HDLCHREB==6),0,ifelse(HDLCHREB==1,2,1)))),
  
  
  ApoB_score=as.factor(ifelse((APOBRESB==1|APOBRESB==2|APOBRESB==3|APOBRESB==4),0,ifelse((APOBRESB==5),1,ifelse(APOBRESB==97|APOBRESB==98,NA,2)))),
  
  HbA1c_score=as.factor(ifelse((HBA1PREB==1|HBA1PREB==2),0,ifelse((HBA1PREB==3|HBA1PREB==4),1,ifelse(HBA1PREB==7|HBA1PREB==8,NA,2)))),
)

x_predictors <- x %>% select (BMISC, SEX, AGEC, PHDCMWBC, EXLWMBC, EXLWVBC, SYSTOL, DIASTOL, TRIGRESB, LDLRESB, HBA1PREB, GLUCFREB, HDLCHREB, APOBRESB)
}


