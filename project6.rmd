---
title: "Project6"
output: html_document
---
# Setup
```{r setup, include=FALSE}
source('myfunc.R') # self-defiend functions
library(tidyverse)
library(tidyr)     # new tidy functions
library(knitr) # kable
library(caret)# low variance filter
library(glmnet)
library(brglm)
library(modelsummary)
library(gridExtra)
library(kableExtra)
library(performanceEstimation)# for SMOTE
library(rpart)
library(rpart.plot)
library(rattle) #fancyRpartPlot
library(Rtsne)
library(randomForest)
library(neuralnet)
library(e1071)# SVM regression
library(mltools)
library(data.table)
library(skimr)
library(smotefamily)
library(broom)
library(jtools)
library(ranger)
library(ROCR)

library(ranger)
```

# Todo

- Random forest


# Dataset

## dat3 (sleep,  height, weight, -BMI, -biomarkers)

```{r}
#dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64, SMKSTAT==5)  # filter age and smoke status
dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64)  # filter age only

# dat<-dat%>% mutate(
#  rfm = ifelse(SEX==2, 76-(20*(PHDCMHBC/PHDCMWBC)),64-(20*(PHDCMHBC/PHDCMWBC)) )
# )
# 


var_list<-c("BMISC","PHDKGWBC","PHDCMHBC","SLPTIME","SEX","AGEC","DIABBC","HCHOLBC","HSUGBC","HYPBC","PHDCMWBC","EXLWMBC","EXLWVBC", "SYSTOL","DIASTOL","CVDMEDST","SMKSTAT") # add/remove variables that are interested
dat3<-dat %>% select (var_list) # select columns that we are interested
dat3$EXLWMBC<-as.numeric(as.character(dat$EXLWMBC)) # exerices time should be numeric 
dat3$EXLWVBC<-as.numeric(as.character(dat$EXLWVBC)) # exerciese time should be numeric
str(dat3) 
dat3c<-dat3%>%na.omit()
str(dat3c)
#table(dat3c$CVDMEDST)

hist(dat3$SYSTOL)

```

## dat2 

```{r}
#dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64, SMKSTAT==5)  # filter age and smoke status
dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64)  # filter age only
var_list<-c("BMISC","SEX","AGEC","DIABBC","HCHOLBC","HSUGBC","HYPBC","PHDCMWBC","EXLWMBC","EXLWVBC", "SYSTOL","DIASTOL","TRIGRESB","CHOLRESB","LDLRESB","HBA1PREB","GLUCFREB","HDLCHREB","CVDMEDST","APOBRESB","SMKSTAT") # add/remove variables that are interested
dat2<-dat %>% select (var_list) # select columns that we are interested
dat2$EXLWMBC<-as.numeric(as.character(dat$EXLWMBC)) # exerices time should be numeric 
dat2$EXLWVBC<-as.numeric(as.character(dat$EXLWVBC)) # exerciese time should be numeric
str(dat2) # 7238 obs x 20 variables

```
### dat2c (NA removed)

```{r}
dat2c<-na.omit(dat2)
dat2c

```


# Models

## Model 35 (HYPBC)

Y: HYPBC (undersamping-> ~300 class = 1)

```{r}
str(dat3c)
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")

Y<-dat3c%>% select (response) 
X<-dat3c%>% select (!response)


y<-Y %>% mutate(
  HYPBC = as.factor(ifelse((HYPBC==5), 0, ifelse((HYPBC==3|HYPBC==2|HYPBC==1),1, NA)))
)
Y
y<-y$HYPBC # y is binary (0,1)
x<-X
table(y)
str(x)
```
```{r}
temp_dat<-cbind(y,x)
temp_dat<-na.omit(temp_dat)
table(temp_dat$y)
a<-temp_dat[which(temp_dat$y==0),]
aa<-a[1:300,]
aa
b<-temp_dat[which(temp_dat$y==1),]
b
temp_dat<-rbind(aa,b)

str(dat3c)
#temp_dat <-smote(y ~., temp_dat, perc.over=600, perc.under=2)

str(temp_dat)
# K-fold CV
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)

# create some matrix to record results during k-fold cv
result_full<- matrix(NA, k_fold, 2) # record performance indicators
model_full=list() # record model
result_bmi<- matrix(NA, k_fold, 2)
model_bmi=list()
result_nobmi<- matrix(NA, k_fold, 2)
model_nobmi=list()


for (i in 1:k_fold){
  fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
  fold_train<-temp_dat[-folds[[i]],] # remaining is training set
  
  # glm
  model<-glm(y~., data=temp_dat, family=binomial)
  pred<-predict(model, fold_test, type="response") # numeric predicted values
  fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  matrix<-table(fold_predict, fold_test$y)# confusion matrix
  if (nrow(matrix<2)){
    matrix<-rbind(matrix,c(0,0))
  }
  
  
  matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
  
  matrix
  
  f1<-calculate_f1(matrix) # calculate f1 score
  auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  model_full[i]<-model
  result_full[i,1]<-f1
  result_full[i,2]<-auc
    
}

# make a table to exhibit all results from different models
k_th<-seq(1:(k_fold+1))
result<-data.frame(cbind(result_full))
colnames(result)<-c("Full F1","Full AUC")
avg_result=apply(result[,],2,mean, na.rm=TRUE) # avearge value of each 
final_result<-rbind(result,avg_result)
final_result<-cbind(k_th, final_result)
final_result[(k_fold+1),1]="Average"
final_result


summary(model)

confusionMatrix(matrix)
```

## Model 34 (HSUGBC)

- Y HSUGBC, under sampling (only contains ~100 class 1)

```{r}
str(dat3c)
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")

Y<-dat3c%>% select (response) 
X<-dat3c%>% select (!response)


y<-Y %>% mutate(
  HSUGBC = as.factor(ifelse((HSUGBC==5), 0, ifelse((HSUGBC==3|HSUGBC==2|HSUGBC==1),1, NA)))
)
Y
y<-y$HSUGBC # y is binary (0,1)
x<-X
table(y)
str(x)
```
```{r}
temp_dat<-cbind(y,x)
temp_dat<-na.omit(temp_dat)
table(temp_dat$y)
a<-temp_dat[which(temp_dat$y==0),]
aa<-a[1:100,]
aa
b<-temp_dat[which(temp_dat$y==1),]
b
temp_dat<-rbind(aa,b)

str(dat3c)
#temp_dat <-smote(y ~., temp_dat, perc.over=600, perc.under=2)

str(temp_dat)
# K-fold CV
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)

# create some matrix to record results during k-fold cv
result_full<- matrix(NA, k_fold, 2) # record performance indicators
model_full=list() # record model
result_bmi<- matrix(NA, k_fold, 2)
model_bmi=list()
result_nobmi<- matrix(NA, k_fold, 2)
model_nobmi=list()


for (i in 1:k_fold){
  fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
  fold_train<-temp_dat[-folds[[i]],] # remaining is training set
  
  # glm
  model<-glm(y~., data=temp_dat, family=binomial)
  pred<-predict(model, fold_test, type="response") # numeric predicted values
  fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  matrix<-table(fold_predict, fold_test$y)# confusion matrix
  if (nrow(matrix<2)){
    matrix<-rbind(matrix,c(0,0))
  }
  
  
  matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
  
  matrix
  
  f1<-calculate_f1(matrix) # calculate f1 score
  auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  model_full[i]<-model
  result_full[i,1]<-f1
  result_full[i,2]<-auc
    
}

# make a table to exhibit all results from different models
k_th<-seq(1:(k_fold+1))
result<-data.frame(cbind(result_full))
colnames(result)<-c("Full F1","Full AUC")
avg_result=apply(result[,],2,mean, na.rm=TRUE) # avearge value of each 
final_result<-rbind(result,avg_result)
final_result<-cbind(k_th, final_result)
final_result[(k_fold+1),1]="Average"
final_result


summary(model)

confusionMatrix(matrix)
```


## Model 33 (HCHOLBC)

Y: HCHOLBC, binary
X: 

undersampling y=0
y=0 -> ~300
y=1 -> ~280

```{r}
str(dat3c)
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")

Y<-dat3c%>% select (response) 
X<-dat3c%>% select (!response)


y<-Y %>% mutate(
  HCHOLBC = as.factor(ifelse((HCHOLBC==5), 0, ifelse((HCHOLBC==3|HCHOLBC==2|HCHOLBC==1),1, NA)))
)
Y
y<-y$HCHOLBC # y is binary (0,1)
table(y)
x<-X

str(x)
```

```{r}
temp_dat<-cbind(y,x)
temp_dat<-na.omit(temp_dat)
table(temp_dat$y)
a<-temp_dat[which(temp_dat$y==0),]
aa<-a[1:300,]
aa
b<-temp_dat[which(temp_dat$y==1),]
b
temp_dat<-rbind(aa,b)

str(dat3c)
#temp_dat <-smote(y ~., temp_dat, perc.over=600, perc.under=2)

str(temp_dat)
# K-fold CV
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)

# create some matrix to record results during k-fold cv
result_full<- matrix(NA, k_fold, 2) # record performance indicators
model_full=list() # record model
result_bmi<- matrix(NA, k_fold, 2)
model_bmi=list()
result_nobmi<- matrix(NA, k_fold, 2)
model_nobmi=list()


for (i in 1:k_fold){
  fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
  fold_train<-temp_dat[-folds[[i]],] # remaining is training set
  
  # glm
  model<-glm(y~., data=temp_dat, family=binomial)
  pred<-predict(model, fold_test, type="response") # numeric predicted values
  fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  matrix<-table(fold_predict, fold_test$y)# confusion matrix
  if (nrow(matrix<2)){
    matrix<-rbind(matrix,c(0,0))
  }
  
  
  matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
  
  matrix
  
  f1<-calculate_f1(matrix) # calculate f1 score
  auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  model_full[i]<-model
  result_full[i,1]<-f1
  result_full[i,2]<-auc
    
}

# make a table to exhibit all results from different models
k_th<-seq(1:(k_fold+1))
result<-data.frame(cbind(result_full))
colnames(result)<-c("Full F1","Full AUC")
avg_result=apply(result[,],2,mean, na.rm=TRUE) # avearge value of each 
final_result<-rbind(result,avg_result)
final_result<-cbind(k_th, final_result)
final_result[(k_fold+1),1]="Average"
final_result


summary(model)

confusionMatrix(matrix)
```


## Model 32 (HCHOLBC)

- Y:HCHOLBC
- X: new predictors 


```{r}
str(dat3c)
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")

Y<-dat3c%>% select (response) 
X<-dat3c%>% select (!response)


y<-Y %>% mutate(
  HCHOLBC = ifelse((HCHOLBC==5), 0, ifelse((HCHOLBC==3|HCHOLBC==2|HCHOLBC==1),1, NA))
)
Y
y<-y$HCHOLBC # y is binary (0,1)
x<-X

str(x)
```
```{r}
temp_dat<-cbind(y,x)
temp_dat<-na.omit(temp_dat)
temp_dat

str(temp_dat)
# K-fold CV
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)

# create some matrix to record results during k-fold cv
result_full<- matrix(NA, k_fold, 2) # record performance indicators
model_full=list() # record model
result_bmi<- matrix(NA, k_fold, 2)
model_bmi=list()
result_nobmi<- matrix(NA, k_fold, 2)
model_nobmi=list()


for (i in 1:k_fold){
  fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
  fold_train<-temp_dat[-folds[[i]],] # remaining is training set
  
  # glm
  model<-glm(y~., data=temp_dat, family=binomial)
  pred<-predict(model, fold_test, type="response") # numeric predicted values
  fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  matrix<-table(fold_predict, fold_test$y)# confusion matrix
  if (nrow(matrix<2)){
    matrix<-rbind(matrix,c(0,0))
  }
  
  
  matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
  
  matrix
  
  f1<-calculate_f1(matrix) # calculate f1 score
  auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  model_full[i]<-model
  result_full[i,1]<-f1
  result_full[i,2]<-auc
    
}

# make a table to exhibit all results from different models
k_th<-seq(1:(k_fold+1))
result<-data.frame(cbind(result_full))
colnames(result)<-c("Full F1","Full AUC")
avg_result=apply(result[,],2,mean, na.rm=TRUE) # avearge value of each 
final_result<-rbind(result,avg_result)
final_result<-cbind(k_th, final_result)
final_result[(k_fold+1),1]="Average"
final_result


summary(model)

confusionMatrix(matrix)
```


## Model 31 (CVDMEDST)

- Y: CVDMEDST
- x: mixture of numerical and categorical values

```{r}

dat3c
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")

Y<-dat3c%>% select (response) 
X<-dat3c%>% select (!response)

y<-Y %>% mutate(
  CVDMEDST = ifelse((CVDMEDST==4), 0, ifelse((CVDMEDST==3|CVDMEDST==2|CVDMEDST==1),1, NA))
)
y<-y$CVDMEDST # y is binary (0,1)
x<-X

str(x)
```

### Logistic regression

```{r logisitic regression}
temp_dat<-cbind(y,x)
temp_dat<-na.omit(temp_dat)

str(temp_dat)
# K-fold CV
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)

# create some matrix to record results during k-fold cv
result_full<- matrix(NA, k_fold, 2) # record performance indicators
model_full=list() # record model
result_rfm<- matrix(NA, k_fold, 2)
model_rfm=list()
result_nobmi<- matrix(NA, k_fold, 2)
model_nobmi=list()



for (i in 1:k_fold){
  fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
  fold_train<-temp_dat[-folds[[i]],] # remaining is training set
  
  n<-nrow(fold_train)  
  # glm
  full_model <-glm(y~., data=temp_dat, family=binomial)
  null_model <-glm(y~1, data=temp_dat, family=binomial)
  model <- step(full_model,k=log(n),trace=0)
  pred<-predict(model, fold_test, type="response") # numeric predicted values
  fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  matrix<-table(fold_predict, fold_test$y)# confusion matrix
  if (nrow(matrix<2)){
    matrix<-rbind(matrix,c(0,0))
  }
  
  
  matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
  f1<-calculate_f1(matrix) # calculate f1 score
  auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  model_full[i]<-model
  result_full[i,1]<-f1
  result_full[i,2]<-auc
  
  
  # # glm
  # full_model <-glm(y~., data=temp_dat, family=binomial)
  # null_model <-glm(y~1, data=temp_dat, family=binomial)
  # model<-glm(y~., data=temp_dat, family=binomial)
  # pred<-predict(model, fold_test, type="response") # numeric predicted values
  # fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  # matrix<-table(fold_predict, fold_test$y)# confusion matrix
  # if (nrow(matrix<2)){
  #   matrix<-rbind(matrix,c(0,0))
  # }
  # 
  # 
  # matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
  # f1<-calculate_f1(matrix) # calculate f1 score
  # auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  # model_full[i]<-model
  # result_full[i,1]<-f1
  # result_full[i,2]<-auc
  
  # rfm
  # model<-glm(y~rfm, data=temp_dat, family=binomial)
  # pred<-predict(model, fold_test, type="response") # numeric predicted values
  # fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  # matrix<-table(fold_predict, fold_test$y)# confusion matrix
  # if (nrow(matrix<2)){
  #   matrix<-rbind(matrix,c(0,0))
  # }
  # 
  # 
  # matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
  # f1<-calculate_f1(matrix) # calculate f1 score
  # auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  # model_rfm[i]<-model
  # result_rfm[i,1]<-f1
  # result_rfm[i,2]<-auc
  
}

# make a table to exhibit all results from different models
k_th<-seq(1:(k_fold+1))
result<-data.frame(cbind(result_full, result_rfm))
colnames(result)<-c("Full F1","Full AUC","Full F1","Full AUC")
avg_result=apply(result[,],2,mean, na.rm=TRUE) # avearge value of each 
final_result<-rbind(result,avg_result)
final_result<-cbind(k_th, final_result)
final_result[(k_fold+1),1]="Average"
final_result

summary(model)

confusionMatrix(matrix)
```

### Random Forest

```{r}



y<-as.factor(y)
temp_dat<-cbind(y,x)
temp_dat<-na.omit(temp_dat)
str(temp_dat)

# classification tree
model<-rpart(y~., data=temp_dat,parms = list(split="information"))


# 设置cp值
rpart_cont <- rpart.control(cp=0)
# 对树的模型设置CP值
model <- rpart(y~.,data=temp_dat, 
              control = rpart_cont,
              parms = list(split="information"))

library(flextable)
df <- data.frame(model$cptable)
ft <- flextable(df) %>%
  autofit()
ft
cpvals <- model$cptable[,1]
nsplit <- model$cptable[,2]
xerror <- model$cptable[,4]
# 下面这句话是找到cp table中 xerror最小的一行所对应的行号(index)
minpos <- min(seq_along(xerror)[xerror == min(xerror)])     
rpart.plot(model)


# Random forest
model<-randomForest(
  y~.,
  data=temp_dat,
  num.trees = 500,
  min.node.size = 5,
  mtry=floor((ncol(temp_dat)-1)/3),
  #mrty=22,
  seed=2021,
  respect.unordered.factors = "order",
)

model
table(model$y)

library(vip)


p1<-vip::vip(model,num_features=10,bar=FALSE)

p1

library(pdp)
partial(model, pred.var = c("BMISC","AGEC"), prob=TRUE, plot=TRUE)

#%>% plotPartial()

#plotPartial(model)

```


## Model 21

- Y: HCHOLBC = 1,2,3 -> Y=1; HCHOLBC = 5 ->Y=0
- X: mixture of numerical and recoded categorical

```{r construct x and y}
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")

Y<-dat2c%>% select (response) 
X<-dat2c%>% select (!response)

str(Y)
str(X)

y<-Y %>% mutate(
  HCHOLBC = as.factor(ifelse((HCHOLBC==5), 0, ifelse(HCHOLBC==3|HCHOLBC==2|HCHOLBC==1,1, NA)))
)
y<-y$HCHOLBC # y is binary (0,1)

x<-X%>% mutate(
  TRIGRESB=as.factor(ifelse((TRIGRESB==1|TRIGRESB==2|TRIGRESB==3),0,ifelse((TRIGRESB==4|TRIGRESB==5),1,ifelse(TRIGRESB==97|TRIGRESB==98,NA,2)))),
  
  CHOLRESB=as.factor(ifelse((CHOLRESB==1|CHOLRESB==2|CHOLRESB==3),0,ifelse((CHOLRESB==4|CHOLRESB==5|CHOLRESB==6),1,ifelse(CHOLRESB==97|CHOLRESB==98,NA,2)))),
  
  LDLRESB=as.factor(ifelse((LDLRESB==1|LDLRESB==2|LDLRESB==3|LDLRESB==4),0,ifelse((LDLRESB==5|LDLRESB==6|LDLRESB==7),1,ifelse(LDLRESB==97|LDLRESB==98,NA,2)))),
  
  GLUCFREB=as.factor(ifelse((GLUCFREB==4|GLUCFREB==5),0,ifelse((GLUCFREB==6|GLUCFREB==7),1,ifelse(GLUCFREB==97|GLUCFREB==98,NA,2)))),
  
  HDLCHREB=as.factor(ifelse((HDLCHREB==7|HDLCHREB==8),NA,ifelse((HDLCHREB==5|HDLCHREB==6),0,ifelse(HDLCHREB==1,2,1)))),
  
  APOBRESB=as.factor(ifelse((APOBRESB==1|APOBRESB==2|APOBRESB==3|APOBRESB==4),0,ifelse((APOBRESB==5),1,ifelse(APOBRESB==97|APOBRESB==98,NA,2)))),
  
  HBA1PREB=as.factor(ifelse((HBA1PREB==1|HBA1PREB==2),0,ifelse((HBA1PREB==3|HBA1PREB==4),1,ifelse(HBA1PREB==7|HBA1PREB==8,NA,2)))),
)

x<-x[,-10] # drop cholersb, do not use cholresb to predict high cholersterol 
str(x)

```



### Logistic regression
```{r}
repeats<-1
res1 <- cv_penLogistic(y,x,method="vanilla", repeats=repeats)
res2 <- cv_penLogistic(y,x,method="fowardAIC", repeats=repeats)
res3 <- cv_penLogistic(y,x,method="forwardBIC", repeats=repeats)
res4 <- cv_penLogistic(y,x,method="stepwiseAIC", repeats=repeats)
res5 <- cv_penLogistic(y,x,method="stepwiseBIC", repeats=repeats)
#res6 <- cv_penLogistic(y,xx,method="ridge", repeats=repeats)
#res7 <- cv_penLogistic(y,xx,method="lasso", repeats=repeats)
res8 <- cv_penLogistic(y,x,method="firth",repeats=repeats)
```

```{r}
tab <- cbind(
  apply(res1[[1]],2,mean),
  apply(res2[[1]],2,mean),
  apply(res3[[1]],2,mean),
  apply(res4[[1]],2,mean),
  apply(res5[[1]],2,mean),
  #apply(res6[[1]],2,mean),
  #apply(res7[[1]],2,mean),
  apply(res8[[1]],2,mean))

colnames(tab) <- c("logistic",
                   "fwdAIC",
                   "fwdBIC",
                   "bwdAIC",
                   "bwdBIC",
                   #"ridge",
                   #"lasso",
                   "firth")

boxplot(tab)

# extract coefficients of models
lcoef <- list(res1[[2]]$coef,
              res2[[2]]$coef,
              res3[[2]]$coef,
              res4[[2]]$coef,
              res5[[2]]$coef,
              #drop(coef(res6[[2]], res6[[2]]$lambda.1se )),
              #drop(coef(res7[[2]], res7[[2]]$lambda.1se )),
              res8[[2]]$coef
)

varnames <- unique(unlist(map(lcoef,names)))
tab = matrix(0, nrow = length(lcoef), ncol = length(varnames))
colnames(tab) = varnames
for (i in 1:length(lcoef)) 
  tab[i, names(lcoef[[i]])] = lcoef[[i]]

rownames(tab) <- paste("model",1:length(lcoef),sep="")
kable(t(tab))

```

Above codes show the coefficients of BMISC of many models are close to 0, suggesting BMI may not be a good indicator for predicting high cholesterol. 
On the other hand, SEX, AGE, PHDCMWBC, DIASTOL, TRIGRESB, LDLRESB, HBA1PREB, GLUCFREB, HDLCHREB, APOBRESB are important predictors for predicting high cholesterol. 



## Model 22

- Y: 0-1 HCHOLOBC
- X: numerical and non-recoded categorical variables, do not drop anything further

```{r}
response<-c("DIABBC","HCHOLBC","HSUGBC","HYPBC","CVDMEDST")

Y<-dat2c%>% select (response) 
X<-dat2c%>% select (!response)

str(Y)
str(X)
Y
y<-Y %>% mutate(
  HCHOLBC = ifelse((HCHOLBC==5), 0, ifelse(HCHOLBC==3|HCHOLBC==2|HCHOLBC==1,1, NA))
)
y<-y$HCHOLBC # y is binary (0,1)
x<-X
str(x)
```

### Logistic regression of various models

```{r logisitic regression}
temp_dat<-cbind(y,x)
temp_dat
# K-fold CV
k_fold = 10
folds<-createFolds(y=temp_dat[,1],k=k_fold)

# create some matrix to record results during k-fold cv
result_full<- matrix(NA, k_fold, 2) # record performance indicators
model_full=list() # record model
result_bmi<- matrix(NA, k_fold, 2)
model_bmi=list()
result_nobmi<- matrix(NA, k_fold, 2)
model_nobmi=list()


for (i in 1:k_fold){
  fold_test<-temp_dat[folds[[i]],] # select folds[[i]] as test test
  fold_train<-temp_dat[-folds[[i]],] # remaining is training set
  
  # full model
  model<-glm(y~., data=temp_dat, family=binomial)
  pred<-predict(model, fold_test, type="response") # numeric predicted values
  fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  matrix<-table(fold_predict, fold_test$y)# confusion matrix
  if (nrow(matrix<2)){
    matrix<-rbind(matrix,c(0,0))
  }
  matrix<-matrix[2:1,2:1] # note the consequence is reversed such that class 1 is positive
  f1<-calculate_f1(matrix) # calculate f1 score
  auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  model_full[i]<-model
  result_full[i,1]<-f1
  result_full[i,2]<-auc
  
  # bmi only model
  model<-glm(y~BMISC, data=temp_dat, family=binomial)
  pred<-predict(model, fold_test, type="response") # numeric predicted values
  fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  matrix<-table(fold_predict, fold_test$y) # confusion matrix, note the consequence is reversed such that class 1 is positive
  if (nrow(matrix<2)){
    matrix<-rbind(matrix,c(0,0))
  }
  matrix<-matrix[2:1,2:1]
  f1<-calculate_f1(matrix) # calculate f1 score
  auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  
  model_bmi[i]<-model
  result_bmi[i,1]<-f1
  result_bmi[i,2]<-auc
  
  # no bmi model
  model<-glm(y~.-BMISC, data=temp_dat, family=binomial)
  pred<-predict(model, fold_test, type="response") # numeric predicted values
  fold_predict<-round(pred) # only 0 and 1 (if >=0.5 then 1 else 0)
  matrix<-table(fold_predict, fold_test$y) # confusion matrix, note the consequence is reversed such that class 1 is positive
  if (nrow(matrix<2)){
    matrix<-rbind(matrix,c(0,0))
  }
  matrix<-matrix[2:1,2:1]
  f1<-calculate_f1(matrix) # calculate f1 score
  auc<-roc(fold_test$y, fold_predict)$auc # calculate AUC 
  model_nobmi[i]<-model
  result_nobmi[i,1]<-f1
  result_nobmi[i,2]<-auc
    
}

# make a table to exhibit all results from different models
k_th<-seq(1:(k_fold+1))
result<-data.frame(cbind(result_full, result_bmi, result_nobmi))
colnames(result)<-c("Full F1","Full AUC", "BMI F1", "BMI AUC","NoBMI F1","NoBMI AUC")
avg_result=apply(result[,],2,mean, na.rm=TRUE) # avearge value of each 
final_result<-rbind(result,avg_result)
final_result<-cbind(k_th, final_result)
final_result[(k_fold+1),1]="Average"
final_result
```

### Random Forest 
```{r}
temp_dat<-cbind(y,x)
temp_dat$y<-as.factor(as.character(temp_dat$y))
str(temp_dat)
dat2c$CV

model<-ranger(HCHOLBC~.-HSUGBC-HYPBC-DIABBC-CVDMEDST, 
       data = dat2c,
       mtry = (floor(ncol(temp_dat)-1)/3),
       respect.unordered.factors = "order",
       seed = 2021,
)


summary(model)
```




# Justify


## BMI vs. High cholesterol

```{r}
Y # whether has high cholesterol 
X # a set of predictors
temp_dat<-cbind(Y,X)
temp_dat<-temp_dat %>% mutate(
  HCHOLBC = ifelse(HCHOLBC==5, 0, 1) # encode Y as binary variable
)

# example 
model<-glm(HCHOLBC~., data = temp_dat)
y_pred<-predict(model, temp_dat, type="response")
hist(y_pred)
result<-cbind(y_pred,temp_dat)

# why this is better than BMI: 
hc <- result[which(result$HCHOLBC==1),] # people who has high cholesterol 
nhc <- result[which(result$HCHOLBC==0),]
  
hist(hc$BMISC)
hist(nhc$BMISC)

mean(result[which(result$HCHOLBC==1),"BMISC"]) # mean BMI = 29.4844 for people who has high cholesterol 
mean(result[which(result$HCHOLBC==0),"BMISC"]) # mean BMI = 27.1008 for people who does not have high cholesterol
d_BMI <- result[which(result$HCHOLBC==1),"BMISC"]-result[which(result$HCHOLBC==0),"BMISC"]

t.test(d_BMI)

mean(result[which(result$HCHOLBC==1),"y_pred"]) # mean y_pred = 0.4379, for people who has high cholesterol 
mean(result[which(result$HCHOLBC==0),"y_pred"]) # mean y_pred =0.0845, for people who does not have high cholesterol

# two sample t-test or other tests and show p value of (y_pred) < p value of (BMISC):
wilcox.test(result[which(result$HCHOLBC==1),"y_pred"],result[which(result$HCHOLBC==0),"y_pred"])
wilcox.test(result[which(result$HCHOLBC==1),"BMISC"], result[which(result$HCHOLBC==0),"BMISC"])
t.test(result[which(result$HCHOLBC==1),"BMISC"], result[which(result$HCHOLBC==0),"BMISC"])
t.test(result[which(result$HCHOLBC==1),"y_pred"],result[which(result$HCHOLBC==0),"y_pred"])

# ROC plot

library(pROC)
result
p1<-roc(result$HCHOLBC, result$BMISC) # BMI ROC
p2<-roc(result$HCHOLBC, result$y_pred) # Y-pred ROC
plot(p1, col='blue')
plot.roc(p2, add=TRUE, col='red')
auc(p1) # 0.6274
auc(p2) # 0.8765

```


```{r bmi cho}
bmi_high <- dat2%>% filter(dat2$BMISC >= 30)
nrow(bmi_high) # 1658 people high bmi
bmi_low <- dat2%>% filter(dat2$BMISC < 30)
nrow(bmi_low) # 4503 people low bmi
table(bmi_high$HCHOLBC)
table(bmi_low$HCHOLBC)
```
The claim is: BMI along is not a good indicator to diagnose high cholesterol

Based on BMI, we can see that

- 8.57% of low BMI people have high cholesterol (ideally, the majority of them should NOT have high cholesterol, since they are not in obesity according to BMI)
- 15.86% of high BMI people have high cholesterol (ideally, the majority of them should have, since they ARE in obesity according to bmi)

To beat BMI, 


方案3:
```{r}

```


方案1:
```{r}

# 重新group, 把每个predictor分成不同level, 比如 normal, bad, very bad ...
# 
str(dat2)

x_regroup <- dat2 %>% mutate(
   
   age_score<-
   bmi_score<-
   对每个 predictor 设置level..
   .....
  
   Tri_score=as.factor(ifelse((TRIGRESB==1|TRIGRESB==2|TRIGRESB==3),0,ifelse((TRIGRESB==4|TRIGRESB==5),1,ifelse(TRIGRESB==97|TRIGRESB==98,NA,2)))),
  
   Chol_score=as.factor(ifelse((CHOLRESB==1|CHOLRESB==2|CHOLRESB==3),0,ifelse((CHOLRESB==4|CHOLRESB==5|CHOLRESB==6),1,ifelse(CHOLRESB==97|CHOLRESB==98,NA,2)))),
  
  LDL_score=as.factor(ifelse((LDLRESB==1|LDLRESB==2|LDLRESB==3|LDLRESB==4),0,ifelse((LDLRESB==5|LDLRESB==6|LDLRESB==7),1,ifelse(LDLRESB==97|LDLRESB==98,NA,2)))),
  
  Glu_score=as.factor(ifelse((GLUCFREB==4|GLUCFREB==5),0,ifelse((GLUCFREB==6|GLUCFREB==7),1,ifelse(GLUCFREB==97|GLUCFREB==98,NA,2)))),
  
  HDL_score=as.factor(ifelse((HDLCHREB==7|HDLCHREB==8),NA,ifelse((HDLCHREB==5|HDLCHREB==6),0,ifelse(HDLCHREB==1,2,1)))),
  
  
  ApoB_score=as.factor(ifelse((APOBRESB==1|APOBRESB==2|APOBRESB==3|APOBRESB==4),0,ifelse((APOBRESB==5),1,ifelse(APOBRESB==97|APOBRESB==98,NA,2)))),
  
  HbA1c_score=as.factor(ifelse((HBA1PREB==1|HBA1PREB==2),0,ifelse((HBA1PREB==3|HBA1PREB==4),1,ifelse(HBA1PREB==7|HBA1PREB==8,NA,2)))),
)

x_predictors <- x %>% select (BMISC, SEX, AGEC, PHDCMWBC, EXLWMBC, EXLWVBC, SYSTOL, DIASTOL, TRIGRESB, LDLRESB, HBA1PREB, GLUCFREB, HDLCHREB, APOBRESB)
}

# 根据X regroup, 对于每个人, SUMMATION[每个predictor的分数 × predictor在回归模型的系数]-> 给每个人算出一个obesity score
....
# 再给obesity score 找一个threshold 
# 目标是 对于>threshold (肥胖人群), 高胆固醇发病率>15.86% 
# <threshold 人群, 高胆固醇发病率 < 8.57%

```
方案2:
直接使用 回归模型的Cross validation结果. 如logistic regression的model 1的错误率是15%, 则说明有15%的人被误判, 和BMI结果对比发现, 高BMI人群 有100%-15.86%的人没有得病: 即 若根据 BMI>30 -> high cholesterol = 1 来判断, 则相当于有接近85%的人会被误判


